1. 신경망 module의 최전선? fractal
  1.1 image classification을 하기 위한 신경망 model이 계속 improve되어 왔고 AlexNet 에서 VGG로 그리고 GoogLeNet 그리고 마지막으로 ResNet이 되었다.
  1.2 Inception model에서 부터 Residual connection을 통해서 deep하고 sparse한 신경망을 학습 할 수 있게 되었다.
  1.3 regularizer로 drop-path, dropou이 제안되었고 그리고 stochastic depth나 student-teacher training method들이 제안되었다.
  1.4 추가적으로 data argumentation을 통해서도 성능이 향상되는 것을 확인 할 수 있었다.
  1.5 학습을 개선하기 위해서 Sigmoid나 tanh같은 gradient가 vanishing 되는것을 막기위해서 ReLU가 제안되었고 다양한 개량형 ReLU가 제안되었다.
  1.6 good initialization또한 이런 문제를 잘 개선시킬 수 있다.
  1.7 학습 속도를 개선시키기 위해 main loss가 아닌 auxiliary loss를 통해서 학습 속도를 개선 시킬 수 있지만 training과 실제 값 사이의 차이가 발생하게 된다.
  1.8 Fractal 도형처럼 같은 모양의 그림이 작아지면서 반복되게 등장하는 module
  1.9 각 module 간에는 pooling layer가 있고 하나의 module안에는 가장 바깥쪽에 가장 긴 것 과 평행하게 그것의 절반이되는 것 2개 또 절반 4개 인 식으로 module이 구성되어 있다.
  1.10 F_1 = conv(z), F_c+1 = [F_c dot F_c)] join conv(z) 인 식으로 되어 있다.
  1.11 모든 signal, input들이 conv layer를 거치기 때문에, 더 중요한 데이터가 있지 않다.
  1.12 drop path를 적용할때, local 방식과 global 방식 2가지로 적용한다.
    1.12.1 local은 각 각 join position에서 하나의 fractal 조각이 drop된다.
    1.12.2 global은 join 간 하나의 path가 사라지게 된다.
  1.13 이를 통해서 performance를 계속해서 확인해서 성능에 영향을 많이주는 path를 찾아서 model을 compression 시킬 수 있다.
  1.14 이 논문에서는 fractal의 기본 단위로 conv layer를 사용했다. 다른 fire module이나 inception moduel을 사용하면 성능이 개선 되는지 궁금하다.
  1.15 drop-path와 batch normalization을 같이 적용 할 수 있다는 것인지, 두가지를 따로따로 적용했는지 잘 모르겠다.
  1.16 어쨋든 drop-path 방식역시 fractalnet과 같이 여러 path로 나누어져 있는경우 깊은 신경망 학습에 이용하면 아주 좋다.
  1.17 https://arxiv.org/abs/1605.07648

2. 증식 LSTM
  2.1 sequence data를 입력으로 받아서 sequence data를 출력으로 내놓는 model에 대해서 RNN은 좋은 성능을 보여주고 있다.
  2.2 RNN의 한계점인 vanishing gradient문제는 LSTM과 같은 추가적인 model로 개선을 시킬 수 있다.
  2.3 하지만 이런 RNN은 입력 데이터가 잘 정제되지 않았을때, 실제 예측을 진행할 때, 잘못된 입력이 들어왔을때, hidden cell data가 고쳐지는데 시간이 오래 걸리게 되는 문제점이 발생 한다.
  2.4 이런 문제점을 해결 하기 위해서 Multiplicative LSTM(mLSTM)이 제안 되었다.
  2.5 multiplicate RNN의 경우 입력 데이터가 차원이 높은 tensor의 경우 입력 tensor를 쪼개서, 각 array 별로 gate를 만들어서 hidden node를 통과시킬 gate를 학습시키는 방식으로 학습이 진행됨
  2.6 하지만 mRNN의 gate는 추가적인 sigmoid함수가 아닌 linear summation으로 gate가 학습되서 표현력이 sigmoid보다 좋음.
  2.7 mLSTM은 mRNN과 같은식으로 지난번에 출력된 hidden state의 data를 추가적인 multiplicate gate를 통해서 갱신해서 그 데이터를 vanila LSTM의 hidden state처럼 학습을 진행
  2.8 mLSTM은 다른 LSTM들과 달리 depth가 깊지 않아도, performance가 잘나오기 때문에 깊이가 깊어질 필요가 없고, 다른 요인때문에, 좋은 성능을 나타내는 것으로 보인다.
  2.9 요인에 대한 설명은 나와있지 않는것 같다.
  2.10 요즘 데이터들 중에는 입력데이터가 tensor가 아닐수는 없기 때문에, 다양한 곳에서 적용 할 수 있을 것 같다.
  2.11 https://arxiv.org/abs/1609.07959
