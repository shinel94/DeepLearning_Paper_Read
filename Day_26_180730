1. 신경망 module의 최전선? fractal
  1.1 image classification을 하기 위한 신경망 model이 계속 improve되어 왔고 AlexNet 에서 VGG로 그리고 GoogLeNet 그리고 마지막으로 ResNet이 되었다.
  1.2 Inception model에서 부터 Residual connection을 통해서 deep하고 sparse한 신경망을 학습 할 수 있게 되었다.
  1.3 regularizer로 drop-path, dropou이 제안되었고 그리고 stochastic depth나 student-teacher training method들이 제안되었다.
  1.4 추가적으로 data argumentation을 통해서도 성능이 향상되는 것을 확인 할 수 있었다.
  1.5 학습을 개선하기 위해서 Sigmoid나 tanh같은 gradient가 vanishing 되는것을 막기위해서 ReLU가 제안되었고 다양한 개량형 ReLU가 제안되었다.
  1.6 good initialization또한 이런 문제를 잘 개선시킬 수 있다.
  1.7 학습 속도를 개선시키기 위해 main loss가 아닌 auxiliary loss를 통해서 학습 속도를 개선 시킬 수 있지만 training과 실제 값 사이의 차이가 발생하게 된다.
  1.8 Fractal 도형처럼 같은 모양의 그림이 작아지면서 반복되게 등장하는 module
  1.9 각 module 간에는 pooling layer가 있고 하나의 module안에는 가장 바깥쪽에 가장 긴 것 과 평행하게 그것의 절반이되는 것 2개 또 절반 4개 인 식으로 module이 구성되어 있다.
  1.10 F_1 = conv(z), F_c+1 = [F_c dot F_c)] join conv(z) 인 식으로 되어 있다.
  1.11 모든 signal, input들이 conv layer를 거치기 때문에, 더 중요한 데이터가 있지 않다.
  1.12 drop path를 적용할때, local 방식과 global 방식 2가지로 적용한다.
    1.12.1 local은 각 각 join position에서 하나의 fractal 조각이 drop된다.
    1.12.2 global은 join 간 하나의 path가 사라지게 된다.
  1.13 이를 통해서 performance를 계속해서 확인해서 성능에 영향을 많이주는 path를 찾아서 model을 compression 시킬 수 있다.
  1.14 이 논문에서는 fractal의 기본 단위로 conv layer를 사용했다. 다른 fire module이나 inception moduel을 사용하면 성능이 개선 되는지 궁금하다.
  1.15 drop-path와 batch normalization을 같이 적용 할 수 있다는 것인지, 두가지를 따로따로 적용했는지 잘 모르겠다.
  1.16 어쨋든 drop-path 방식역시 fractalnet과 같이 여러 path로 나누어져 있는경우 깊은 신경망 학습에 이용하면 아주 좋다.
  1.17 https://arxiv.org/abs/1605.07648

2. 증식 LSTM
  2.1 sequence data를 입력으로 받아서 sequence data를 출력으로 내놓는 model에 대해서 RNN은 좋은 성능을 보여주고 있다.
  2.2 RNN의 한계점인 vanishing gradient문제는 LSTM과 같은 추가적인 model로 개선을 시킬 수 있다.
  2.3 하지만 이런 RNN은 입력 데이터가 잘 정제되지 않았을때, 실제 예측을 진행할 때, 잘못된 입력이 들어왔을때, hidden cell data가 고쳐지는데 시간이 오래 걸리게 되는 문제점이 발생 한다.
  2.4 이런 문제점을 해결 하기 위해서 Multiplicative LSTM(mLSTM)이 제안 되었다.
  2.5 multiplicate RNN의 경우 입력 데이터가 차원이 높은 tensor의 경우 입력 tensor를 쪼개서, 각 array 별로 gate를 만들어서 hidden node를 통과시킬 gate를 학습시키는 방식으로 학습이 진행됨
  2.6 하지만 mRNN의 gate는 추가적인 sigmoid함수가 아닌 linear summation으로 gate가 학습되서 표현력이 sigmoid보다 좋음.
  2.7 mLSTM은 mRNN과 같은식으로 지난번에 출력된 hidden state의 data를 추가적인 multiplicate gate를 통해서 갱신해서 그 데이터를 vanila LSTM의 hidden state처럼 학습을 진행
  2.8 mLSTM은 다른 LSTM들과 달리 depth가 깊지 않아도, performance가 잘나오기 때문에 깊이가 깊어질 필요가 없고, 다른 요인때문에, 좋은 성능을 나타내는 것으로 보인다.
  2.9 요인에 대한 설명은 나와있지 않는것 같다.
  2.10 요즘 데이터들 중에는 입력데이터가 tensor가 아닐수는 없기 때문에, 다양한 곳에서 적용 할 수 있을 것 같다.
  2.11 https://arxiv.org/abs/1609.07959

3. sequence 인풋을 처리하기 위해서 sequence 아웃풋을 내는 LSTM
  3.1 입력 sequence를 받고, 그를 통해서 학습을 진행 한다.
  3.2 그 뒤 배열이 끝나면 모델을 처음부터 다시 학습하는 것이 아닌, 배열 끝에 EOS라는 추가적인 변수를 넣어서 끊어진 여러 배열을 하나의 배열로 만듬
  3.3 이 연속된 배열을 통해서 Seuquence입력에 대해서 출력을 예측하는 RNN을 학습 할 수 있다.
  3.4 이때 RNN을 통해서 압축된 데이터로, 바로 prediction을 진행하는 것이아닌, 해당 벡터를 embedding이나 tokenization시켜서 그 압축된 벡터를 통해서 prediction을 진행하거나 recovoery를 진행하면 좋은 성능을 내는 것을 확인 할 수 있었다.
  3.5 이때, 입력 벡터를 반대로 넣게되면, 기계변역과 같은 문제에서 a,b,c to 1,2,3일때 입력이 c,b,a,EOS,1,2,3이 되면 a와 1사이의 유사성이 보다 커지게 모델을 학습 시킬 수 있을 것으로 생각됨.
  3.6 위의 방법을 좀더 일반화시켜서 적용한게 Biderectional RNN으로 생각된다. 본 논문에는 그 내용이 제안되지 않았음
  3.7 당연한 얘기지만 RNN보다 LSTM이 long sentence에 대해서 학습이 더 잘 진행되었다.
  3.8 결론적으로 짧게 잘린 여러개의 배열을 EOS를 추가함으로써 하나의 모델로 만들 수 있고, 깊은 신경망을 학습 시켜서 벡터를 압축하고 그를 통해서 복원하는 과정을 거치게 하면 좋은 성능을 얻을 수 있다.
  3.9 https://arxiv.org/abs/1409.3215
  
 4. Deep Bidirectional LSTM을 학습하는 방법
  4.1 LSTM에서 sequence를 통해서 target을 예측할 때 장기의존성 문제를 해결 할 수 있게 해주었다.
  4.2 이때 일반적으로 발음과 같은 시간에 따른 sequence는 과거의 값 뿐만아니라 미래의 상태에도 현재의 값이 영향을 받을 수 있기 때문에 Bidirectional RNN이 제안 되었다.
  4.3 그리고 현실적으로 하나의 배열을 한번에 처리하는것이 가장 좋지만, 물리적인 한계가 있기 때문에, sequence를 chunk로 잘라서 학습을 진행한다.
  4.4 이때 chunk를 시간순서대로 자르게 되면 chunk간 interdependecy를 capture하지 못하는 문제가 발생한다.
  4.5 이를 해결하기 위해서 Context Sensitive Chunk라는 방식이 제안되었다.
    4.5.1 우선 chunk를 frame 단위로 자르게 되는데 이때, data가 가지고 있는것이 자기 시간대의 데이터와 이전시간대의 데이터 이후 시간대의 데이터 3개지 블록으로 데이터를 가지게 된다.
    4.5.2 Bidirectional이기 때문에, 과거에 영향을 받는 LSTM은 현재시간대블록과 지난시간대 블록을 합쳐서 입력으로 받고, 미래에 영향을 받는 LSTM은 미래시간대의 블록과 현재 시간대 블록을 합쳐서 입력으로 받는 LSTM을 학습한다.
    4.5.3 그래서 총 현재 프레임 데이터의 갯수와 context data의 숫자 그리고 frame 숫자에 따라서 데이터를 얼마나 겹치게 할지, emperical하게 조절 할 수 있다.
  4.6 이를 입력으로 하는 LSTM을 학습하면 좋은 성능을 내는 것으로 확인 할 수 있었다.
  4.7 https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7428837
