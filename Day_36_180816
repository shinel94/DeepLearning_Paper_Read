1. Recursive Neural Network에서 input data를 잘 조절해서 context-sensitive-chunk BPTT 를 적용하기
  1.1 신경망이 다양한 곳에서 성공적인 성능을 내고 있고, Sequencial data에 RNN과 그로부터 파생된 모델들이 좋은 성능을 내고 있다.
  1.2 이때 time step에 대해서 즉 입력 sequence 길이를 무한대로 증가시킬 수 있지만, 이렇게 하면서 발생하는 gradient각 vanishing되거나 exploding되는 문제가 발생한다.
  1.3 그리고 이를 제어하기 위해서 입력의 길이를 조절하는 batch 방식의 학습방법이 제안되었다.
  1.4 하지만 chunk를 나눌때, sequence의 길이를 다 담지 못하게 chunk를 나누면 bidirectional한 model을 적용하지 못하는 문제가 발생한다.
  1.5 이를 막기위해서 가장긴 sequence의 길이를 기준으로 데이터를 나누고 chunk길이보다 짧은 데이터는 sequence의 뒷부분을 zero padding시킨다.
  1.6 그리고 Truncated BPTT라는 방법은 일정한 길이로 chunk를 나누는데 이때 하나의 입력데이터를 작은 chunk로 쪼게고, sequence끝에 zero padding을 추가해서 배열의 끝을 인식시키게 한다.
  1.7 그리고 chunk간 interdependecy를 capture하기 위해서 chunk의 양쪽 끝에 지난번 chunk의 값을 그대로 받아오는 region을 attach해서 interdependecy를 catch한다.
  1.8 이 chunk주변에 양쪽으로 지난번 출력과 미래의 출력을 attach함으로써 model을 bidiretional하게 구성하고 학습 시킬 수 있다.
  1.9 data를 chunk단위로 나누고 이번 입력에서 지난번 출력을 일부분 받음으로써 chunk간 interdependecy를 capture하는 model을 학습할 수 있다.
  1.10 이때 sequence간 interdependecy는 capture할 수 없다는 단점이 있지만 이것은 LSTM의 memory cell로 학습 할 수 있을 것으로 보인다.
  1.11 https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/paper_180.pdf
