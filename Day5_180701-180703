1. Extream Inception Network
  1.1 CNN은 filter를 통해서 feature를 extraction한다.
  1.2 filter나 다른 신경망의 가중치는 sparse한 형태가 성능이 좋다.
  1.3 이를 위해서 filter의 사이즈가 크면 좋지만 사이즈가 커지면 메모리 낭비가 커지고 계산이 힘들어진다.
  1.4 이를 해결하기위해 큰 size의 필터를 작은 size의 필터로 나누는 것이 기본적인 inception의 개념
  1.5 가중치를 설정하거나 신경망을 구성할 때, 신경망의 가중치들은 sparse하게 구성하는게 좋고 행렬계산은 dense하게 하는게 좋다.
  1.6 https://arxiv.org/abs/1610.02357
  
2. 가중치를 공유해서 network학습시에 메모리나 에너지관점에서 효율적으로 계산하자
  2.1 weight를 저장할 때 각각 layer별로 인풋과 가중치 그리고 바이어스 그리고 temporary값이 저장되는 장소까지 메모리가 낭비가 심하다.
  2.2 이를 줄이기 위해서 weight를 공유해서 메모리적인 측면에서 보다 효율적으로 관리 할 수있디.
  2.3 이렇게 구성하면 weight를 sparse하게 구성할 수도 있다.
  2.4 filter를 factorization해서 분해하는 것이 아닌 re-construct이나 decoupling 해서 다시 쌓는 느낌으로 가중치를 공유해서 메모리를 절약할 수 있다.
  2.5 특히 0이 곱해지는 경우에는 결과값과 가중치 모두 추가적인 저장공간 필요없이 저장 할 수 있다.
  2.6 https://arxiv.org/abs/1804.06508
  
3. 신기방이 R-CNN
  3.1 CNN을 통해서 feature를 extraction해서 latent variable들을 생성해낼 수 있다.
  3.2 이 latent variable로 feature를 classifaction 할 수 있다.
  3.3 그리고 이미지를 localization할때도 이 변수들을 사용해서 하면 더 잘된다.
  3.4 R-CNN은 이미지를 localization하는 부분 이부분은 bounding box-regression 방법으로 학습한다.
  3.5 image classification은 localization이 잘되는 모델의 출력값을 이용해서 해당 latent variable들을 이용해 SVM으로 분류기 모델을 만든다.
  3.6 softmax를 사용한 모델의 경우에는 성능이 저하되는 것을 확인할 수 있었다.
  3.7 bounding box-regression을 위한 latent variable은 마지막 layer의 값이 아닌 중간 layer에서 추출해서 진행했다.
  3.8 https://arxiv.org/abs/1311.2524
  
