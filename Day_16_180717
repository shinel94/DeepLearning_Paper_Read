1. Rectifier Unit 짱짱
  1.1 세포 신경이 자극을 받아 신호를 보내기 위해서는 역치 이상의 자극을 받아야지만 작동을 하게 된다.
  1.2 하지만 sigmoid나 tanh와 같은 함수들은 0을 이준으로 대칭하기 때문에 실제 세포의 작동과 조금 다르다.
    1.2.1 추가적으로 sigmoid 함수의 경우에는 레이어가 계속 쌓여 미분이 계속되게 되면 값이 특정부분으로 수렴하는데 이도 실제와는 다르다
  1.3 실제 세포는 symmerty 하지 않기 때문에 이를 맞추기 위해서 Rectifier unit을 쓰면 보다 실제와 가까운 데이터를 얻을 수 있다.
  1.4 ReLU를 사용하면 신경망 구성을 보다 sparse하게 구성할 수 있다. (ReLU(x) = max(0,x), 이기 때문에 많은 node값이 0이 된다)
    1.4.1 sparse하게 구성하면 입력데이터의 약간의 변화(noise)같은 것에 보다 robust하게 대처 할 수 있다.
    1.4.2 그리고 아주 큰 변화에서 역시 잘 표현 할 수 있다.
    1.4.3 sparse가 높을 수록 linear sparse가 높게 표현 할 수 있고.
    1.4.4 실제 세포 distribution역시 sparse하게 구성되어 있다.
  1.5 ReLU를 사용하게 되면 기울기를 계산할 때, exponential이나 복잡한 과정이 없기 때문에 계산 cost가 drastically 감소한다.
  1.6 0에서 미분이 가능해 지지 않는 문제가 있지만, 실제로 0이 되는 경우는 거의 없다.
  1.7 하지만 미분 가능하고 비슷하게 그래프가 그려지는 softplus를 제한했다
    1.7.1 1/alpha*log(1+exp(alpha*x))로 구성되어 있다.
    1.7.2 alpha가 0이면 softplus, alpha가 무한대이면 ReLU가 된다.
  1.8 이전의 tanh와 같은 활성화 함수를 사용하면 사전학습을 한 경우와, 하지 않은경우의 성능차이가 보였다.
  1.9 ReLU를 사용하면 사전학습을 한경우나 안한경우나 saturation되는 성능이 비슷하다.
  1.10 ReLU를 사용하면 사전학습도 필요없고, 전반적인 성능향상을 보이나, 실제로 사용해본 경험으로는 미분 기울기가 saturation하지 않기 때문에, solution이 쉽게 발산하려는 경향이 있다. 이를 잘 조절하면서 사용해야 하고, 초기화 문제 역시 있는 것으로 보인다.
  1.11 그래도 계산이 쉽고 미분이 쉽고 수렴이 빠르기 때문에 좋은건 맞다.
  1.12 http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf

2. Random Patch의 성능증가 - CNN이 왜 좋은 성능을 보여주는지 설명해주는 논문이라고 
  2.1 CNN의 convolutional product가 왜 좋은 성능을 나타내는지 보여주는 논문
  2.2 본 논문에서는 convolution필터의 구성은 ones 매트릭스로 데이터를 그대로 뽑아온뒤 일정 영역의 값을 sum pooling하게 되는 식으로 latent variable을 추출해낸다.
  2.3 이 추출해낸 성분을 이용해서, 간단한 algorithm들 k-means나 GMM과 같은 계산 코스트가 적은 것들을 사용하여도 성능이 좋아지는 것을 확인 할 수 있다.
  2.4 그래서 classification알고리즘이 단순한데도 좋은 성능을 보이는 것을 보아서, 더 복잡하고 성능이 좋은 classifier를 적용한다면 더 좋은 성능을 얻을 것이다.
  2.5 CNN이 먼저 나온것인지 이논문이 먼저 나온것인지는 모르겠지만, CNN의 이미지 classification에서 좋은 성능을 보여주는 간단한 예나 설명인것으로 이해할 수 있다.
  2.6 CNN은 이보다 복잡하게 latent variable을 만들때 사용하는 filter들 역시 학습을 통해서 조절해서 feature를 extraction하기 때문에 성능이 더 좋다.
  2.7 또 Inception과 같은 더 깊은 논의가 진행되었고, 어떻게 필터의 모양을 구성하면 좋은지 도 논의되고 있고
  2.8 stride역시 항상 같은 값으로 움직이는 것이 아닌 특정 비율만큼 감소 시킬수 있게끔 1과 2가 반복되는 숫자로 나오게 해서 어떤 부분은 겹치고 어떤부분은 겹치지 않게 필터가 움직이게 하는 것도 있다.
  2.9 결국 filter를 통해서 extraction 즉 이미지 분류에서는 이미지의 모든 변수를 사용하는 것 보다 일부분만 사용하는 것이 모델 성능에 좋다.
  2.10 https://www-cs.stanford.edu/people/ang/papers/nipsdlufl10-AnalysisSingleLayerUnsupervisedFeatureLearning.pdf
  
3. Long Term Memory를 저장하기 위한 RNN학파의 노력
  3.1 RNN은 예전부터 나왔고 RNN의 큰 단점인 long term memory를 기억하지 못한다는 단점이 있다.
    3.1.1 기억하지 못하는 이유는 gradient가 vanishing되기 때문에, 예전 인풋에대해서는 에러가 전파가 안됨
  3.2 그래서 이 long term메모리를 다루기 위해서 많은 unit들이 제안 되었지만 최고봉은 역시 LSTM
    3.2.1 최근 gate recurrent unit이란 GRU도 제안되긴 했다.
  3.3 Long term memory저장을 위해서 clockwork RNN을 제안했다.
  3.4 clockwork RNN (CW-RNN)의 구조는 직렬로 여려개 연결된 RNN 유닛을 여러개 병렬로 두어서 해당 병렬로 연결된 유닛 뭉치간 연결을 한쪽 방향으로만 진행되게 신경망이 구성되어 있다.
    3.4.1 논문에서는 right to left로 hidden unit의 정보가 전파 되게 되어 있고, 유닛 내부는 fully connected되어 있으며 표현이 fast에서 slow로 전파 된다고 되어 있다.
  3.5 output layer로 전파되는 값은 항상 모든 값을 이용하는 것이 아니라 (t mod Ti) 가 0이 되는 유닛만 출력됨
    3.5.1 t는 진행된 시간을 나타내고 Ti는 RNN뭉치가 가지고 있는 고유한 스피드(시간값)
  3.6 이를 통해서 수렴속도를 많이 증가 시킬 수 있고, 성능역시 개선되는 것을 확인 할 수 있었다.
  3.7 결국 이 모델에는 기본 유닛으로 Simple Recurent Neural Network를 사용했는데 기본 유닛으로 LSTM이나 GRU를 사용한다면 성능이 더 좋아 질것이라고 생각된다.
  3.8 https://arxiv.org/abs/1402.3511

4. NLP 처리까지 할 수 있는 NN
  4.1 NLP는 다양한 task 를가지고 있다
    4.1.1 Part Of Speech Tagging 단어가 가지는 특징 복수형(plural) 명사(noun)인지, 부사(adverb)인지 등
    4.1.2 Chunking 문장의 시작과 끝 그리고 내부를 구분하는 작업
    4.1.3 Named Entity Recognition 단어 자체의 category를 할당하는 작업
    4.1.4 Semantic Role Labeling 구문에서 작용하는 역할을 구분하는 작업
    4.1.5 Language model 다음에 나올 단어를 예측하는 작업
    4.1.6 Sementically Related Words 문맥적으로 비슷한 의미나 비슷한 용도 또는 비슷한 방식으로 사용되는 단어 관계를 찾는 작업
  4.2 이런 다양한 NLP task를 NN을 통해서 개선할 수 있다.
  4.3 본 논문에선는 Lookup table이라는 단어를 압축한 vector를 이용해서 NLP를 처리 하는것 같다.
  4.4 최근에는 word2vec이라는 단어 압축process가 있다.
    4.4.1 word2vec은 인풋으로 document가 가진 단어들을 one - hot encoding한 것이 들어가게 되는데 이때 인풋의 크기가 문장의 단어수 * 문장의 단어수 이다.
    4.4.2 가중치 벡터의 하나의 행의 값이 하나의 단어를 나타내게끔 오토엔코더를 학습한다
  4.5 이 lookup table의 값을 random patch를 이용해서 특정 사이즈로 추출해 내어서 새로운 feature 벡터를 얻어낸다.
  4.6 이 벡터를 NN의 인풋으로 넣고 다양한 task의 문제를 해결하기 위해 학습시킨다.
  4.7 NN이 다양한 task들을 같은 형상으로 잘 처리 할 수있다.
  4.8 NN이 NLP에 적용되기에 좋은 모델이다.
  4.9 https://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf
