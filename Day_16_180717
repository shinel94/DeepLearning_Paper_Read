1. Rectifier Unit 짱짱
  1.1 세포 신경이 자극을 받아 신호를 보내기 위해서는 역치 이상의 자극을 받아야지만 작동을 하게 된다.
  1.2 하지만 sigmoid나 tanh와 같은 함수들은 0을 이준으로 대칭하기 때문에 실제 세포의 작동과 조금 다르다.
    1.2.1 추가적으로 sigmoid 함수의 경우에는 레이어가 계속 쌓여 미분이 계속되게 되면 값이 특정부분으로 수렴하는데 이도 실제와는 다르다
  1.3 실제 세포는 symmerty 하지 않기 때문에 이를 맞추기 위해서 Rectifier unit을 쓰면 보다 실제와 가까운 데이터를 얻을 수 있다.
  1.4 ReLU를 사용하면 신경망 구성을 보다 sparse하게 구성할 수 있다. (ReLU(x) = max(0,x), 이기 때문에 많은 node값이 0이 된다)
    1.4.1 sparse하게 구성하면 입력데이터의 약간의 변화(noise)같은 것에 보다 robust하게 대처 할 수 있다.
    1.4.2 그리고 아주 큰 변화에서 역시 잘 표현 할 수 있다.
    1.4.3 sparse가 높을 수록 linear sparse가 높게 표현 할 수 있고.
    1.4.4 실제 세포 distribution역시 sparse하게 구성되어 있다.
  1.5 ReLU를 사용하게 되면 기울기를 계산할 때, exponential이나 복잡한 과정이 없기 때문에 계산 cost가 drastically 감소한다.
  1.6 0에서 미분이 가능해 지지 않는 문제가 있지만, 실제로 0이 되는 경우는 거의 없다.
  1.7 하지만 미분 가능하고 비슷하게 그래프가 그려지는 softplus를 제한했다
    1.7.1 1/alpha*log(1+exp(alpha*x))로 구성되어 있다.
    1.7.2 alpha가 0이면 softplus, alpha가 무한대이면 ReLU가 된다.
  1.8 이전의 tanh와 같은 활성화 함수를 사용하면 사전학습을 한 경우와, 하지 않은경우의 성능차이가 보였다.
  1.9 ReLU를 사용하면 사전학습을 한경우나 안한경우나 saturation되는 성능이 비슷하다.
  1.10 ReLU를 사용하면 사전학습도 필요없고, 전반적인 성능향상을 보이나, 실제로 사용해본 경험으로는 미분 기울기가 saturation하지 않기 때문에, solution이 쉽게 발산하려는 경향이 있다. 이를 잘 조절하면서 사용해야 하고, 초기화 문제 역시 있는 것으로 보인다.
  1.11 그래도 계산이 쉽고 미분이 쉽고 수렴이 빠르기 때문에 좋은건 맞다.
  1.12 http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf
