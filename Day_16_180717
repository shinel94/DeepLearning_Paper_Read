1. Rectifier Unit 짱짱
  1.1 세포 신경이 자극을 받아 신호를 보내기 위해서는 역치 이상의 자극을 받아야지만 작동을 하게 된다.
  1.2 하지만 sigmoid나 tanh와 같은 함수들은 0을 이준으로 대칭하기 때문에 실제 세포의 작동과 조금 다르다.
    1.2.1 추가적으로 sigmoid 함수의 경우에는 레이어가 계속 쌓여 미분이 계속되게 되면 값이 특정부분으로 수렴하는데 이도 실제와는 다르다
  1.3 실제 세포는 symmerty 하지 않기 때문에 이를 맞추기 위해서 Rectifier unit을 쓰면 보다 실제와 가까운 데이터를 얻을 수 있다.
  1.4 ReLU를 사용하면 신경망 구성을 보다 sparse하게 구성할 수 있다. (ReLU(x) = max(0,x), 이기 때문에 많은 node값이 0이 된다)
    1.4.1 sparse하게 구성하면 입력데이터의 약간의 변화(noise)같은 것에 보다 robust하게 대처 할 수 있다.
    1.4.2 그리고 아주 큰 변화에서 역시 잘 표현 할 수 있다.
    1.4.3 sparse가 높을 수록 linear sparse가 높게 표현 할 수 있고.
    1.4.4 실제 세포 distribution역시 sparse하게 구성되어 있다.
  1.5 ReLU를 사용하게 되면 기울기를 계산할 때, exponential이나 복잡한 과정이 없기 때문에 계산 cost가 drastically 감소한다.
  1.6 0에서 미분이 가능해 지지 않는 문제가 있지만, 실제로 0이 되는 경우는 거의 없다.
  1.7 하지만 미분 가능하고 비슷하게 그래프가 그려지는 softplus를 제한했다
    1.7.1 1/alpha*log(1+exp(alpha*x))로 구성되어 있다.
    1.7.2 alpha가 0이면 softplus, alpha가 무한대이면 ReLU가 된다.
  1.8 이전의 tanh와 같은 활성화 함수를 사용하면 사전학습을 한 경우와, 하지 않은경우의 성능차이가 보였다.
  1.9 ReLU를 사용하면 사전학습을 한경우나 안한경우나 saturation되는 성능이 비슷하다.
  1.10 ReLU를 사용하면 사전학습도 필요없고, 전반적인 성능향상을 보이나, 실제로 사용해본 경험으로는 미분 기울기가 saturation하지 않기 때문에, solution이 쉽게 발산하려는 경향이 있다. 이를 잘 조절하면서 사용해야 하고, 초기화 문제 역시 있는 것으로 보인다.
  1.11 그래도 계산이 쉽고 미분이 쉽고 수렴이 빠르기 때문에 좋은건 맞다.
  1.12 http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf

2. Random Patch의 성능증가 - CNN이 왜 좋은 성능을 보여주는지 설명해주는 논문이라고 
  2.1 CNN의 convolutional product가 왜 좋은 성능을 나타내는지 보여주는 논문
  2.2 본 논문에서는 convolution필터의 구성은 ones 매트릭스로 데이터를 그대로 뽑아온뒤 일정 영역의 값을 sum pooling하게 되는 식으로 latent variable을 추출해낸다.
  2.3 이 추출해낸 성분을 이용해서, 간단한 algorithm들 k-means나 GMM과 같은 계산 코스트가 적은 것들을 사용하여도 성능이 좋아지는 것을 확인 할 수 있다.
  2.4 그래서 classification알고리즘이 단순한데도 좋은 성능을 보이는 것을 보아서, 더 복잡하고 성능이 좋은 classifier를 적용한다면 더 좋은 성능을 얻을 것이다.
  2.5 CNN이 먼저 나온것인지 이논문이 먼저 나온것인지는 모르겠지만, CNN의 이미지 classification에서 좋은 성능을 보여주는 간단한 예나 설명인것으로 이해할 수 있다.
  2.6 CNN은 이보다 복잡하게 latent variable을 만들때 사용하는 filter들 역시 학습을 통해서 조절해서 feature를 extraction하기 때문에 성능이 더 좋다.
  2.7 또 Inception과 같은 더 깊은 논의가 진행되었고, 어떻게 필터의 모양을 구성하면 좋은지 도 논의되고 있고
  2.8 stride역시 항상 같은 값으로 움직이는 것이 아닌 특정 비율만큼 감소 시킬수 있게끔 1과 2가 반복되는 숫자로 나오게 해서 어떤 부분은 겹치고 어떤부분은 겹치지 않게 필터가 움직이게 하는 것도 있다.
  2.9 결국 filter를 통해서 extraction 즉 이미지 분류에서는 이미지의 모든 변수를 사용하는 것 보다 일부분만 사용하는 것이 모델 성능에 좋다.
  2.10 https://www-cs.stanford.edu/people/ang/papers/nipsdlufl10-AnalysisSingleLayerUnsupervisedFeatureLearning.pdf
