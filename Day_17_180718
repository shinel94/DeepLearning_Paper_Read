1. RNN에 Dropout을 적용시켜보자
  1.1 RNN은 시계열 데이터를 처리하기 위해 제안된 방식으로 지난번 출력을 이번 입력값과 합쳐서 이번 신경망의 입력으로 넣는 방식이다.
  1.2 RNN은 long term tendency를 처리하기 위해서 LSTM이라는 memory cell을 가진 RNN이 제안 되었고 성능이 좋다.
  1.3 하지만 지난번 출력을 이번 입력으로 받기 때문에, 예전 데이터가 가진 에러가 시간에 따라 큰 영향을 끼칠 수 있게 된다.
  1.4 이를 개선하기 위해서 dropout을 적용하려 했지만 번번히 실패하였다.
  1.5 하지만 기본적인 RNN유닛(LSTM,GRU)에 dropout을 적용하기 위해서는 수평방향이 아닌 수직방향에만 dropout을 적용시키면 성능개선을 찾을 수 있다.
  1.6 수평방향에 적용 하지않음으로써 시간에 대해서는 dropout이 적용되는 횟수가 독립적인 것을 확인 할 수 있었다.
  1.7 오로지 layer 깊이에 대해서만 dropout횟수가 정해지기 때문에 성능이 좋아지는 것으로 보인다.
  1.8 dropout을 적용하면 여러 모델을 학습하는 것과 같은 ensemble효과를 얻기 때문에 성능이 개선되는 것으로 보인다.
  1.9 결국 수평방향이 아닌 수직방향으로 dropout을 적용시키면 RNN에도 dropout을 적용 시킬 수 있다.
  1.10 https://arxiv.org/abs/1409.2329

2. 준지도 학습을 하는 Recursive Autoencoder
  2.1 manually제작된 단어 데이터셋을 이용하는 것이 아닌 단순히 단어를 벡터화 시킨 데이터로 데이터입력을 했어도 좋은 성능을 찾을 수 있다.
  2.2 단어벡터를 본 논문에서는 one-hot encoding이 아닌 N(0,sigma)를 따르게 분포시킨 것 같다.
  2.3 one-hot-encodoing도 나쁜 성능을 보여주진 않는데 이유를 모르겠음
  2.4 autoencoder로 단어를 압축을 하는데 phrase를 압축하기 위해서 지난번 단어의 압축된 hidden layer의 출력을 다음 단어 압축하는 데이터로 사용
  2.5 이 압축된 데이터를 이용해서 다시 복원시키는 과정을 거치면서 autoencoder를 압축함
  2.6 마지막으로 압축된 데이터를 이용해서 다음 단어를 예측하거나 sentiment와 같은 것을 예측하는 추가적인 신경망 classification을 학습해서 한번에 여러개의 작업을 진행 시킬 수 있다.
  2.7 autoencoder를 사용하면 n-gram의 polarity도 확인 할 수 있고, 단어 압축 역시 잘할 수 있다.
  2.8 이를 통해서 NLP task들을 처리 할 수 있다.
  2.9 하나의 단어를 압축하는 autoencoder를 recursive하게 사용함으로써 단어가 아닌 문장을 압축하는 autoencoder를 구성할 수 있다.
  2.10 http://www.aclweb.org/anthology/D11-1014
