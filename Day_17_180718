1. RNN에 Dropout을 적용시켜보자
  1.1 RNN은 시계열 데이터를 처리하기 위해 제안된 방식으로 지난번 출력을 이번 입력값과 합쳐서 이번 신경망의 입력으로 넣는 방식이다.
  1.2 RNN은 long term tendency를 처리하기 위해서 LSTM이라는 memory cell을 가진 RNN이 제안 되었고 성능이 좋다.
  1.3 하지만 지난번 출력을 이번 입력으로 받기 때문에, 예전 데이터가 가진 에러가 시간에 따라 큰 영향을 끼칠 수 있게 된다.
  1.4 이를 개선하기 위해서 dropout을 적용하려 했지만 번번히 실패하였다.
  1.5 하지만 기본적인 RNN유닛(LSTM,GRU)에 dropout을 적용하기 위해서는 수평방향이 아닌 수직방향에만 dropout을 적용시키면 성능개선을 찾을 수 있다.
  1.6 수평방향에 적용 하지않음으로써 시간에 대해서는 dropout이 적용되는 횟수가 독립적인 것을 확인 할 수 있었다.
  1.7 오로지 layer 깊이에 대해서만 dropout횟수가 정해지기 때문에 성능이 좋아지는 것으로 보인다.
  1.8 dropout을 적용하면 여러 모델을 학습하는 것과 같은 ensemble효과를 얻기 때문에 성능이 개선되는 것으로 보인다.
  1.9 결국 수평방향이 아닌 수직방향으로 dropout을 적용시키면 RNN에도 dropout을 적용 시킬 수 있다.
  1.10 https://arxiv.org/abs/1409.2329

2. 준지도 학습을 하는 Recursive Autoencoder
  2.1 manually제작된 단어 데이터셋을 이용하는 것이 아닌 단순히 단어를 벡터화 시킨 데이터로 데이터입력을 했어도 좋은 성능을 찾을 수 있다.
  2.2 단어벡터를 본 논문에서는 one-hot encoding이 아닌 N(0,sigma)를 따르게 분포시킨 것 같다.
  2.3 one-hot-encodoing도 나쁜 성능을 보여주진 않는데 이유를 모르겠음
  2.4 autoencoder로 단어를 압축을 하는데 phrase를 압축하기 위해서 지난번 단어의 압축된 hidden layer의 출력을 다음 단어 압축하는 데이터로 사용
  2.5 이 압축된 데이터를 이용해서 다시 복원시키는 과정을 거치면서 autoencoder를 압축함
  2.6 마지막으로 압축된 데이터를 이용해서 다음 단어를 예측하거나 sentiment와 같은 것을 예측하는 추가적인 신경망 classification을 학습해서 한번에 여러개의 작업을 진행 시킬 수 있다.
  2.7 autoencoder를 사용하면 n-gram의 polarity도 확인 할 수 있고, 단어 압축 역시 잘할 수 있다.
  2.8 이를 통해서 NLP task들을 처리 할 수 있다.
  2.9 하나의 단어를 압축하는 autoencoder를 recursive하게 사용함으로써 단어가 아닌 문장을 압축하는 autoencoder를 구성할 수 있다.
  2.10 http://www.aclweb.org/anthology/D11-1014

3. RNN에 비해서 파라미터수가 너무나 많은 GRU와 LSTM
  3.1 RNN에 비해서 LSTM은 4*(n*n+n*m+n) 개의 파라미터를 더 가진다.
    3.1.1 n은 hidden unit의 갯수 m은 input unit의 갯수
  3.2 GRU는 3*(n*n+n*m+n)개의 파라미터를 더 가진다.
  3.3 이들은 RNN과 달리 long term tendency를 위해서 추가적인 gate unit이 필요하기 때문이다.
    3.3.1 이때 이 gate unit역시 학습을 통해서 갱신이 되기 때문에 이를위한 파라미터가 필요하다.
    3.3.2 LSTM은 추가적인 gate가 3개이고 GRU는 2개의 추가적인 게이트가 필요하다.
  3.4 이때 gate를 계산하기 위한 parameter숫자를 줄임으로써 계산비용을 획기적으로 낮출수 있다.
  3.5 GRU의 경우만 parameter를 조절하고 있다.
  3.6 z와r gate를 계산할때 이전에는 이번입력 x와 지난번 출력h 그리고 bias 3개의 텀을 이용해서 게이트의 값을 계산하였다.
  3.7 본 논문에서는 이번 입력x를 사용하지 않는 gate1, bias와 이번입력x를 사용하지 않는 gate2, bias만 사용하는 gate3로 구분해서 학습을 진행했다.
  3.8 bias만 사용한 GRU의 경우에는 performance가 어느정도 감소하는 것을 확인 할 수는 있었지만 saturation이 늦게 일어나서 보다 오랜기간 학습을 진행하면 비슷한 성능을 보일것으로 생각된다.
  3.9 gate1과 gate2의 경우에는 모든 파라미터를 사용하는 경우와 비교했을때 눈에 띄는 성능차이는 확인할 수 없었다.
  3.10 하지만 본래 GRU유닛과 비교했을때 gate1과 gate2들의 성능은 거의 차이가 없었고, 비교적 간단한 문제의 경우에는 bias만 사용한 gate에서도 준수한 성능을 확인 할 수 있었다.
  3.11 GRU에서 parameter를 이렇게 줄였으니, LSTM에서도 줄일수 있겠지? 논문도 있겠지?
  3.12 https://arxiv.org/abs/1701.05923

4. MGU에도 parameter reduction을 적용시켜보자
  4.1 RNN이 나오고 long term tendency를 위해서 lSTM이 나오고, 너무 복잡해서 GRU가 나오고 또 너무 복잡해서 MGU가 나왔다.
  4.2 하지만 파라미터는 그래도 많다
  4.3 3번의 논문과 같은 방식으로 parameter의 수를 줄인다.
  4.4 왜 마지막에 바이어스만 들어있는 forget gate가 있는지는 Minimal Gated Unit for Recurrent Neural Networks논문을 참조하면 알 수 있다.
  4.5 성능은 전부다 비슷하고 심지어는 지난번 출력만 forget gate의 인풋으로 사용한 MGU2가 성능이 가장 좋더라.
  4.6 bias만 사용한 MGU는 파라미터가 거의 절반으로 줄어들어도 성능의 저하가 거의 없는것을 확인 할 수 있었다.
  4.7 https://arxiv.org/abs/1603.09420
