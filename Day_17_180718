1. RNN에 Dropout을 적용시켜보자
  1.1 RNN은 시계열 데이터를 처리하기 위해 제안된 방식으로 지난번 출력을 이번 입력값과 합쳐서 이번 신경망의 입력으로 넣는 방식이다.
  1.2 RNN은 long term tendency를 처리하기 위해서 LSTM이라는 memory cell을 가진 RNN이 제안 되었고 성능이 좋다.
  1.3 하지만 지난번 출력을 이번 입력으로 받기 때문에, 예전 데이터가 가진 에러가 시간에 따라 큰 영향을 끼칠 수 있게 된다.
  1.4 이를 개선하기 위해서 dropout을 적용하려 했지만 번번히 실패하였다.
  1.5 하지만 기본적인 RNN유닛(LSTM,GRU)에 dropout을 적용하기 위해서는 수평방향이 아닌 수직방향에만 dropout을 적용시키면 성능개선을 찾을 수 있다.
  1.6 수평방향에 적용 하지않음으로써 시간에 대해서는 dropout이 적용되는 횟수가 독립적인 것을 확인 할 수 있었다.
  1.7 오로지 layer 깊이에 대해서만 dropout횟수가 정해지기 때문에 성능이 좋아지는 것으로 보인다.
  1.8 dropout을 적용하면 여러 모델을 학습하는 것과 같은 ensemble효과를 얻기 때문에 성능이 개선되는 것으로 보인다.
  1.9 결국 수평방향이 아닌 수직방향으로 dropout을 적용시키면 RNN에도 dropout을 적용 시킬 수 있다.
  1.10 https://arxiv.org/abs/1409.2329
