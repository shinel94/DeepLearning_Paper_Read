1. 음성인식에서, 여러개의 마이크를 통해서 multi channel로 음성을 데이터화 시켜서 그 데이터로 신경망 입력으로 넣고 데이터를 처리하면 더 좋은 성능을 내는 것을 확인 할 수 있다.
  1.1 https://ieeexplore.ieee.org/abstract/document/6707744/

2. layer의 가중치를 학습할때, second order term을 쉽게 계산하기 위해서 mean shift데이터를 이용하면 모델을 더 쉽고 빠르게 학습할 수 있고, 과적합 역시 일부분 개선 되는 것을 확인 할 수 있다.
  2.1 https://ieeexplore.ieee.org/document/6853582/

3. 단어를 embedding을 진행하고 문장을 번역할때, 언어간 차이에서 오는 문장의 차이를 epsilon token을 추가해서 행렬을 정사각으로 만들어서 진행하면 모델의 학습이 더 잘 진행되고 또, LSTM을 정방향과 역방향 모델을 각각 학습을 진행해서 출력을 구하면 모델의 성능을 증가 시킬 수 있다. backword LSTM의 경우에는 단어 데이터만 이용해서 모델을 학습하게 되고 forward LSTM의 경우에는 지난번 출력값과 이번 단어들을 이용해서 모델을 학습한다. 그리고 출력값에 단어의 class term을 추가함으로써 모델의 성능을 증가 시킬 수 있다.
  3.1 https://www-i6.informatik.rwth-aachen.de/publications/download/936/SundermeyerMartinAlkhouliTamerWuebkerJoernNeyHermann--TranslationModelingwithBidirectionalRecurrentNeuralNetworks--2014.pdf

4. RNN에 Batch Normalization을 적용하는 방법
  4.1 RNN에서는 feed-forward network에서 적용한것 처럼 BN을 적용하면 학습이 잘 되지 않는 문제가 발생한다.
  4.2 그래서 보통 horizontal한 방향으로는 BN을 적용하지 않는다. 이때 horizontal 은 time direction한 방향을 의미
  4.3 이를 개선하기 위해서 time step wise로 BN을 적용해서 scale factor와 shift factor를 학습해서 모델의 학습속도를 개선 할 수 있다.
  4.4 하지만 과적합이 쉽게 발생하는 문제가 발생했다.
  4.5 결론적으로 RNN에 dropout을 적용하는 방식과 같은식으로 input to hidden이나 hidden to output이나 아니면 vertical한 방향으로만 BN을 적용시키면 모델의 성능을 조금 향상 시킬 수 있다.
  4.6 https://arxiv.org/abs/1510.01378

5. BN이 아닌 WN
  5.1 Batch Normalization 방법이 아닌 weight normalization방법을 사용하면 RNN이나 RL에서도 성능향상을 확인 할 수 있다.
  5.2 Weight를 방향과 크기의 성분으로 분해를 한뒤 방향과 크기를 각각 따로따로 학습을 진행해서 방향을 학습할때 길이에는 독립적으로 학습을 진행 할 수 있게 된다.
  5.3 BN의 batch의 크기에 따라서 unstable한 정도가 결정되는데 비해서, WN은 입력값과는 독립적으로 진행이 되기 때문에, 비교적 학습이 안정적이다.
  5.4 WN을 적용할때 LR이 비교적 커도 학습이 잘되는것을 확인 할 수 있었는데 이때 norm(v)가 커지는 것을 확인 할 수 있었는데, 학습이 진행되면서 커짐이 점점 수렴하는 것도 확인 할 수 있었다.
  5.5 WN이 가중치 matrix의 covariance matrix를 identity하게 만든는 역할을 하여서 gradient가 vanishing되는 것을 막을 수 있다.
  5.6 BN과 WN을 같이 적용시켜서, 입력값에대해서 안정적으로 그리고 가중치 학습에 안정적으로 학습되게 만듦으로써 성능이 향상되는 것을 확인 할 수 있다.
  5.7 https://arxiv.org/abs/1602.07868
  
6. ABCNN
  6.1 CNN모델에 attention을 적용해서 RNN과 같이 데이터간 연관성을 찾을 수 있다.
  6.2 입력데이터를 embedding을 진행하고 embedding된 데이터의 similarity를 다양한 방법으로 계산하여서, 그 similarity행렬로 가중치를 학습하거나 바로 attention을 진행해서 모델을 학습하면 단어간 연관성을 더 잘 찾을 수 있게 된다.
  6.3 stack을 하여서 ABCNN을 학습할때는, 하위 레이어만 가지고 먼저 학습을 진행한뒤 이후에는 하위 레이어는 fix시키고 상위레이어를 학습시키는 식으로 모델을 stack해 나간다.
  6.4 https://arxiv.org/abs/1512.05193

7. 기본적으로 성능이 검증된 CNN structure를 backbone으로 feature map을 얻어내고, 해당 feature map 데이터를 이용해서 position-senstitive pooling을 통해서 전체 이미지를 gird로 나누어서 해당 grid에대한 데이터를 하나의 채널에 데이터를 넣고 그리고 출력된 특징데이터를 이용해서 RoI pooling을 다시 진행해서 얻어낸 데이터로 classification을 진행한다.
  7.1 https://arxiv.org/abs/1605.06409
