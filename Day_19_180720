1. RNN에 대해서
  1.1 RNN의 가장큰 문제(신경망 자체의 주요한 문제)인 gradient가 vanishing되거나 exploding되는 문제가 있다.
    1.1.1 exploding되는 경우는 그라디엔트를 clipping시켜서 특정 threshold를 넘어가면 값을 제한하는 식으로 제어할 수 있다.
    1.1.2 하지만 vanishing되는 경우에는 gradient자체가 작은값을 나타내기 때문에 해결하기가 쉽지않다.
  1.2 이 vanishing gradient를 address하기 위해서 제안된것이 LSTM
  1.3 순환신경망은 이름에서 알수 잇듯이 자체적인 itterative한 nature를 가지고 있기 때문에 timestep에 따른 변화에 민감하게 반응하게 된다.
  1.4 그냥 RNN은 short term dependency에 대한 weight가 long term dependency보다 더 크게 평가하는 경향이 있다.
  1.5 그래서 LSTM은 long term dependecy의 gradient에 대한 정보를 저장하기 위한 memory cell과 short term deppendecy를 제어하는 hidden output이 있다.
  1.6 다양한 실험조건으로 모델을 mutation하고 hyperparameter를 초기화 해서 실험을 진행한 결과
    1.6.1 GRU가 LSTM보다 전반적으로 성능이 좋다. Language modeling을 제외하고.
    1.6.2 GRU중에서 논문에서 제시한 MUT1이라는 GRU가 성능이 가장 좋았다.
      1.6.2.1 update gate = sigm(W*x+b), reset gate = sigm(W*x+W*h+b), h = tanh(W*(r*h) + tanh(x) + b)*z + h*(1-z)
    1.6.3 PTM language modeling에서 dropout을 적용한 LSTM모델들이 전부 성능이 좋았다.
    1.6.4 forget gate의 bias를 크게한 LSTM은 다른 LSTM이나 GRU보다 성능이 좋았다.
    1.6.5 LSTM의 input gate와 output gate를 빼고 dropout을 적용한 LSTM의 성능이 좋았다.
      1.6.5.1 forget gate가 다른 gate들보다 중요하다.
  1.7 forget gate, input gate, output gate 순으로 LSTM의 성능에 영향을 끼침
  1.8 forget gate에 positive bias를 apply하면 좋은 성능을 나타냄
  1.9 논문의 결론은 GRU가 LSTM보다 좋은 성능을 보이는것 같지만, LSTM역시 사용하기에 부족하지 않은데 LSTM을 쓸때는 forget gate에 positive bias를 1 더해주는게 좋다.
  1.10 http://proceedings.mlr.press/v37/jozefowicz15.pdf

2. covariance matrix를 대신해서 쓸수있는 간단한 Spatial sign covariance matrix
  2.1 우선 machinel learning or pattern recognition에서 nonparametric model의 장점은 익히 알려진바 있다.
    2.1.1 과적합의 문제가 없다.
    2.1.2 학습이 간단하다.
    2.2.3 outlier에 비교적 강하다.
  2.2 데이터의 차원수가 높은 경우 PCA나 PLS를 적용해서 계산할수 없을 정도로 computational complexity가 높다.
  2.3 여기서 variance covariance maxtirx가 조금 높은 지분을 차지 하는것 같다.
  2.4 그래서 이 matrix를 대신해서 spartial sign covariance matrix를 사용하면 좋다.
  2.5 계산복잡도와 robustness 부분 에서 모두 개선되는 것이 나타난다.
  2.6 결국 이 multivariable의 전처리 방법을 통해서 계산복잡도도 개선할 수 있고, robustness역시 개선할 수 있다.
  2.7 https://www.ncbi.nlm.nih.gov/pubmed/16711760
  
3. 신경망의 계산 복잡도를 늘리지 않고 학습속도를 개선시키는 방법\
  3.1 이전에 봤던 residual을 이용한 성능개선이 아닌 filter를 fatorization시키는 것으로 계산복잡도를 늘리지 않고, 오히려 줄이면서 성능 개선을 시킬 수 있다.
    3.1.1 residual을 이용하면 개산복잡도가 줄어 들지는 않는다.
  3.2 fatorization은 크기가 큰 필터를 적용했을때와 같은 크기의 아웃풋이 나오지만 사이즈가 작은 filter를 여러개 붙여서 parameter수를 줄이면서 표현력은 유지 시킬 수 있다.
    3.2.1 5x5 filter를 3x3 두개를 cascade하게 연결시키면 같은크기의 입력이면 같은 크기의 출력이 나옴
    3.2.2 더 극단적인 경우는 nxn filter를 nx1 + 1xn필터2개로 fatorization시킬 수도 있다.
  3.3 신경망 작성할때 요령
    3.3.1 표현하는 데이터의 크기를 너무 급격하게 줄이거나 변형시키지 마라
    3.3.2 tile당 activation function을 더 적게 사용하라?
    3.3.3 필터의 크기와 필터의 갯수는 blance를 이루게 조절 하는 것이 좋다.
    3.3.4 spatial aggregating은 데이터를 잘 압축 시킬 수 있다.
  3.4 auxiliary classification을 적용하면 error를 더 잘전파해서 학습을 보다 빠르게 시킬 수 있다.
    3.4.1 마지막 head에 달린 classifier말고 추가적으로 중간에 classifier를 달아서 학습 시키는 방법
  3.5 output값을 하나의 분포가 아닌 2개의 분포를 적용해서 학습 속도를 개선 시킬 수 있다.
    3.5.1 이때 두개의 분포의 차이를 KL-divergence로 최대한이 대게 한다.
  3.6 pooling할때도 작은 사이즈를 가지게 pooling을 2개를 나눠서 한뒤 concat을 하면 parameter를 줄일 수 있다.
  3.7 신경망을 작성 할 때, filter의 사이즈를 작게 여러번 하는것이 sparse한 효과를 얻으면서 계산은 dense하게 할 수 있는 방법이다.
  3.8 https://arxiv.org/abs/1512.00567
