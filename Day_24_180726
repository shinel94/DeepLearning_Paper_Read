1. 다양한 방향으로 stacked 된 LSTM
  1.1 장기의존성을 가진 데이터를 분석하기 위해서 LSTMㅇ 제안되었다.
  1.2 1차원으로 stacked 된 LSTM은 Highway network와 유사한 모습을 나타낸다고 생각 할 수 있다.
  1.3 지난번 hidden node vector와 이번 input vector를 concatenation해서 이번 hidden node input을 만든다.
  1.4 concatenation함으로써 수식 표현이 보다 간단해 진다.
  1.5 기존의 stacked LSTM은 수직방향으로는 hidden node 데이터와 memory cell 데이터가 전파되지 않는다. 오직 output vector만 전파된다.
  1.6 Multidimensional LSTM의 경우는 입력받는 데이터가 벡터 1개가 아닌 N개의 벡터를 동시에 받아서 N개의 병렬로 구성된 LSTM을 이용하는 개념과 비슷하다.
    1.6.1 하나의 긴 벡터의 인풋을 받는것 보다 파라미터 수를 줄일 수 있는것 같다.
    1.6.2 이때 입력데이터의 크기가 커질수록 모델의 instability가 커진다.
    1.6.3 memory cell이 데이터를 한번에 저장하게 모델을 학습하기 때문
  1.7 GRID LSTM은 입력의 deimension에 상관없게 적용할 수 있고, depth도 설정할 수 있는 LSTM이다.
    1.7.1 본 모델에서는 입력은 모든 Hidden vector를 concate해서 들어가지만 출력은 각각 벡터에 따른 hidden node vector와 memory cell 벡터가 출력되게 N개의 모델을 학습한다.
    1.7.2 k번째 벡터를 계산하는 LSTM의 hidden node data는 이번 k번째 입력을 제외하고 나머지는 모두 지난번 출력을 설정한다.
    1.7.3 추가적인 data input이 없이 hidden node와 memory cell들이 전파된다.
    1.7.4 모든 hidden node들이 LSTM을 통과할 필요가 없기 때문에, 다음 hidden node는 하나의 추가적인 perceptron으로 N*d의 벡터를 d 사이즈의 벡터로 압축한다.
  1.8 다양한 dimension으로 가중치를 share해서 computation을 줄일 수 있다.
  1.9 가중치를 공유한 Tied 2 LSTM이 장기의존성에 대한 정보를 잘 기억하는것을 확인 할 수 있다.
  1.10 번역전 데이터를 입력으로 번역후 데이터를 출력으로 그리고 동시에 bidirection 데이터를 처리하게 2차원 data로 넣어서 3차원 LSTM을 적용하면 translation에서도 좋은 성능을 얻을 수 있다.
  1.11 https://arxiv.org/abs/1507.01526

2. Negative Sampling이란
  2.1 단어를 embbeding하는 벡터를 계산할때 Skip-gram 방법을 이용해서 단어들을 예측 하면 계산량이 너무나 많다.
  2.2 이를 줄이기 위해서 Negative Sampling이 제안되었다.
  2.3 임의로 2단어를 뽑아서 두 단어가 같은 문장에 들어 있다면 두 벡터 내적이 1에 가깝게 즉 비스하게 그리고 같은 문장에 없다면 두 벡터 내적이 0에 가깝게 학습하게 하는 파라미터 theta를 계산한다.
  2.4 이때 임의로 뽑을 확률을 noise distribution으로 단어가 출현한 빈도수의 3/4제곱을 적용해서 빈도수가 적은단어에 조금더 가중치를 준다.
  2.5 단어를 압축하는 방법은 one-hot encoding으로 입력벡터를 가지는 신경망을 이용해도 되고, 다양한 방법으로 단어를 벡터로 표현하게 한뒤, parameter들을 학습한다.
  2.6 A와 B 단어 둘 모두 C와 같이 등장했다면 A와 B는 같은 것으로 볼 수 있다는 가정에서 출발한다. A와 C는 비슷하게 그리고 B와 C가 비슷하게 학습된다.
  2.7 A와 B가 서로 같은 문장에서 등장하지 않는다면 A와 B는 서로 수직하게 (내적이 0이되게끔) 학습이 진행된다.
  2.8 이때 순서쌍을 뽑을 횟수가 hyperparameter가 된다.
  2.9 Skip-gram은 k번째 단어를 이용해서 주변 단어를 예측 할 수 있는 단어 벡터 k를 찾는게 목적 즉 k번째 벡터와 주변 k-1이나 k+1번째 단어를 나타내는 c 벡터가 서로 비슷하게 학습이 진행된다.
  2.10 https://arxiv.org/abs/1402.3722
