1. 다양한 방향으로 stacked 된 LSTM
  1.1 장기의존성을 가진 데이터를 분석하기 위해서 LSTMㅇ 제안되었다.
  1.2 1차원으로 stacked 된 LSTM은 Highway network와 유사한 모습을 나타낸다고 생각 할 수 있다.
  1.3 지난번 hidden node vector와 이번 input vector를 concatenation해서 이번 hidden node input을 만든다.
  1.4 concatenation함으로써 수식 표현이 보다 간단해 진다.
  1.5 기존의 stacked LSTM은 수직방향으로는 hidden node 데이터와 memory cell 데이터가 전파되지 않는다. 오직 output vector만 전파된다.
  1.6 Multidimensional LSTM의 경우는 입력받는 데이터가 벡터 1개가 아닌 N개의 벡터를 동시에 받아서 N개의 병렬로 구성된 LSTM을 이용하는 개념과 비슷하다.
    1.6.1 하나의 긴 벡터의 인풋을 받는것 보다 파라미터 수를 줄일 수 있는것 같다.
    1.6.2 이때 입력데이터의 크기가 커질수록 모델의 instability가 커진다.
    1.6.3 memory cell이 데이터를 한번에 저장하게 모델을 학습하기 때문
  1.7 GRID LSTM은 입력의 deimension에 상관없게 적용할 수 있고, depth도 설정할 수 있는 LSTM이다.
    1.7.1 본 모델에서는 입력은 모든 Hidden vector를 concate해서 들어가지만 출력은 각각 벡터에 따른 hidden node vector와 memory cell 벡터가 출력되게 N개의 모델을 학습한다.
    1.7.2 k번째 벡터를 계산하는 LSTM의 hidden node data는 이번 k번째 입력을 제외하고 나머지는 모두 지난번 출력을 설정한다.
    1.7.3 추가적인 data input이 없이 hidden node와 memory cell들이 전파된다.
    1.7.4 모든 hidden node들이 LSTM을 통과할 필요가 없기 때문에, 다음 hidden node는 하나의 추가적인 perceptron으로 N*d의 벡터를 d 사이즈의 벡터로 압축한다.
  1.8 다양한 dimension으로 가중치를 share해서 computation을 줄일 수 있다.
  1.9 가중치를 공유한 Tied 2 LSTM이 장기의존성에 대한 정보를 잘 기억하는것을 확인 할 수 있다.
  1.10 번역전 데이터를 입력으로 번역후 데이터를 출력으로 그리고 동시에 bidirection 데이터를 처리하게 2차원 data로 넣어서 3차원 LSTM을 적용하면 translation에서도 좋은 성능을 얻을 수 있다.
  1.11 https://arxiv.org/abs/1507.01526

2. Negative Sampling이란
  2.1 단어를 embbeding하는 벡터를 계산할때 Skip-gram 방법을 이용해서 단어들을 예측 하면 계산량이 너무나 많다.
  2.2 이를 줄이기 위해서 Negative Sampling이 제안되었다.
  2.3 임의로 2단어를 뽑아서 두 단어가 같은 문장에 들어 있다면 두 벡터 내적이 1에 가깝게 즉 비스하게 그리고 같은 문장에 없다면 두 벡터 내적이 0에 가깝게 학습하게 하는 파라미터 theta를 계산한다.
  2.4 이때 임의로 뽑을 확률을 noise distribution으로 단어가 출현한 빈도수의 3/4제곱을 적용해서 빈도수가 적은단어에 조금더 가중치를 준다.
  2.5 단어를 압축하는 방법은 one-hot encoding으로 입력벡터를 가지는 신경망을 이용해도 되고, 다양한 방법으로 단어를 벡터로 표현하게 한뒤, parameter들을 학습한다.
  2.6 A와 B 단어 둘 모두 C와 같이 등장했다면 A와 B는 같은 것으로 볼 수 있다는 가정에서 출발한다. A와 C는 비슷하게 그리고 B와 C가 비슷하게 학습된다.
  2.7 A와 B가 서로 같은 문장에서 등장하지 않는다면 A와 B는 서로 수직하게 (내적이 0이되게끔) 학습이 진행된다.
  2.8 이때 순서쌍을 뽑을 횟수가 hyperparameter가 된다.
  2.9 Skip-gram은 k번째 단어를 이용해서 주변 단어를 예측 할 수 있는 단어 벡터 k를 찾는게 목적 즉 k번째 벡터와 주변 k-1이나 k+1번째 단어를 나타내는 c 벡터가 서로 비슷하게 학습이 진행된다.
  2.10 https://arxiv.org/abs/1402.3722
  2.11 국문설명 http://dalpo0814.tistory.com/6

3. 모델의 사이즈를 줄이는 SqueezaeNet
  3.1 모델의 사이즈를 줄이면 3가지 이점이 있다.
    3.1.1 학습할때 서버와 더 적게 통신해도 된다. 즉 계산량 자체가 적다.
    3.1.2 새로운 모델 전송할때 적은 bandwidth로도 cloud에서 car와 같은 machine으로 전송이 가능하다.
    3.1.3 모델을 하드웨어에 설치할때 물리적인 resource가 적게 필요하다.
  3.2 파라미터를 줄이면 효과적으로 학습할 수 있고, 새로운 모델을 설치할때 부하고 적고, embedded deployment도 쉽다. 
  3.3 자율주행자동차와 같이 software를 다른 물리적인 device에 설치하기 위해서는 over to air로 update가 진행되야한다.
  3.4 Alexnet은 모델 크기만 240MB에 달해서 over to air update가 어렵기 때문에 이 성능을 내지만 parameter수가 적은 model을 squeeze net으로 학습할 수 있다.
  3.5 초기에서 부터 CNN은 3개의 차원을 가진 image 데이터를 입력으로 받았고, 이 3개의 차원을 한번에 계산해서 다음 출력으로 넘기는 식으로 convolutional filter가 설정되어 있다.
  3.6 최근에 제안된 많은 CNN 모델들은 parameter의 sparse를 올리기 위해서 filter를 factorization하거나 1x1 conv filter를 apply하는 등으로 성능을 개선 시킨다.
  3.7 깊이가 깊어질수록 성능이 좋아진다는 발견을 통해서 깊이가 깊은 신경망을 학습하기 위해서 Residual Network나 Highway Network가 제안 되었다.
    3.7.1 ResNet은 bypass connection을 통해서 성능 개선을 Highway Network는 gate를 통해서 성능개선을 얻어냈다.
  3.8 Fire module을 통해서 적은 파라미터로 좋은 성능을 낼 수 있다.
  3.9 CNN의 파라미터를 줄이는 전략
    3.9.1 replace 3x3 filters with 1x1 filters
    3.9.2 Decrease the number of input channels to 3x3 filters
    3.9.3 Downsample late in the network so that convolution layers have large activation maps
      3.9.3.1 이 큰 activation map이 모델의 performance를 개선 시킬것으로 보인다.
  3.10 적은수의 1x1 filter로 data를 압축한뒤 1x1 filter와 3x3 filter를 동시에 적은양을 사용해서 expand 시켜서 위의 전략을 만족하는 모델을 만들 수 있다.
  3.11 Fire module을 적용하면 parameter수를 줄이면서 성능은 유지되는 것을 확인 할 수 있다. Fire module은 각 사이즈별로 필터의 갯수를 hyperparameter로 갖는다.
  3.12 squeeze ratio에 따라서 다음 layer의 squeez part의 filter수가 조절된다.
    3.12.1 이번레이어에서 expand의 1x1 이 2개 3x3개 4개 라면 다음 레이어에서 squeez에서 1x1필터의 숫자가 6*squeeze ratio가 된다.
  3.13 이때 squeeze ratio가 커질수록 모델의 사이즈도 커지지만 성능도 증가되는것을 확인 할 수 있다.
  3.14 fire module의 expand part에서 3x3 filter의 비율이 높아진다고 해서 성능이 증가하지는 않는다.
  3.15 ResNet을 바탕으로 bypass connection을 attach하기 위해 filter숫자를 맞추기 위해 특정부분에는 1x1 conv가 추가 되었다.
  3.16 simple bypass를 적용한 squeeze net이 model의 size를 증가시키지 않고 성능을 증가시켜주는 것으로 보인다.
  3.17 이처럼 Squeeze Network에서 적용한 parameter를 줄이는 방법을 적용해서 보다 간단한 model을 develop할 것으로 생각된다.
  3.18 https://arxiv.org/abs/1602.07360
  
4. Highway LSTM으로 학습을보다 빠르게 진행 시킬 수 있다.
  4.1 layer의 memory cell간에 gate connection을 연결해서 LSTM의 신경망이 깊어질수록 발생하는 학습속도가 너무 느리거나 학습이 진행되지 않는 문제를 해결 할 수 있다.
  4.2 해당 Highway connection에 dropout을 적용시킴으로써 DSR에 성능개선을 얻을 수 있다.
  4.3 LSTM을 cascade하게 LSTM의 cell output이 다음layer LSTM의 input x vector가 되게 LSTM을 stack할 수 있다.
  4.4 Gate를 통해서 memory cell data를 전달
    4.4.1 지난번 레이어의 cell data와 이번 레이어의 지난번 시간 cell data그리고 이번 시간대 input x 들을 이용해서 sigmoid filter를 학습하고, 이 filter와 memory cell data를 element-wise multifly해서 다음 cell로 전달 시킨다.
  4.5 나머지 hidden layer나 forget input output gate는 동일하게 계산된다. 오로지 memory cell data가 갱신되는 방식만 지난번 memory cell 데이터를 추가로 참고해서 gate를 만들고 memory cell data를 이번 레이어로 전파 시킨다.
  4.6 bidirectional LSTM은 입력순서를 정반대로한 LSTM모델을 하나 추가하는것뿐
  4.7 sequence를 고정된 길이의 chunks로 split을 하고 왼쪽 청크와 오른쪽 청크를 합쳐서 하나의 입력으로 하는 모델을 학습한다.
  4.8 하나의 chunk를 계산할때 지난번 입력을 그냥 받으면 된다.
  4.9 LSTM의 memory cell 의 데이터를 layer 간 gate를 통해서 전달해주면 보다 깊은 구조를 가진 LSTM도 잘 학습할 수 있고 performance역시 좋다.
  4.10 https://arxiv.org/abs/1510.08983

5. Shake-Shake regularization
  5.1 최근 신경망의 overfit문제를 다루기위해 많은 방법들이 제안되고 있다.
  5.2 Batch Nomalization을 통한 computing statistic을 안정하게 하는방법이 제안되었다.
  5.3 Stochastic Gradient Descent을 통한 noisy gradient로 성능을 개선 시킬 수 있었다.
  5.4 가장 간단한 방법인 Dropout도 제안되었는데 Dropout과 Batch Nomalization을 동시에 사용하면 성능이 저하되는 문제가 발생한다.
  5.5 multi baranch를 가지는 NN에서 해당 branch를 standard summation of parallel branches with a stochastic affine combination으로 변형시키는 방법이 제안되었다.
  5.6 Gradient가 backpropagate 될떄 의도적으로 noise를 추가하면 더 generalization한 model을 얻을 수 있다.
  5.7 다른 filtter에서 CNN출력 벡터들의 covariance와 같은 상관계수를 비교해서 다른 특징들을 잘 학습하는지 확인 할 수 있다.
  5.8 multibranch model에서 2개의 branch를 가지는 모델들에 적요할때 하나의 가지의 출력에 의도적으로 dropout과 같은 효과를 내는 sparse matrix를 alpha를 곱하고 다른 가지에는 1-alpha를 곱한것을 출력으로 사용하고, 돌아올때는 노드 출력값도 같은 식으로 shake 해주면 그라디엔트에 noise를 추가하는 효과를 줌으르쏘 성능을 개선 시킬수 있다.
  5.9 https://arxiv.org/abs/1705.07485
  
  
