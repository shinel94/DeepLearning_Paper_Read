1. ResNet을 넘고 FractalNet을 넘은 DenseNet
  1.1 신경망이 특히 CNN이 이미지 인식에서 우월한 성능을 나타내고 있어서 이를 개선하기 위해서 많은 방법들이 제안되었다.
  1.2 3가지 관점에서 신경망이 개선되고 있다고 생각 할 수 있다.
    1.2.1 첫번째 관점은 데이터의 preporcessing과 관련된 부분이다.
      1.2.1.1 batch noramalization과 같이 데이터의 분산과 분포에 대한 영향을 줄이기 위해서 적용하는 방법이다.
      1.2.1.2 stochastic gradient discent 방식으로 특정데이터에 대한 중요도를 낮출 수 있고, 또 전반적인 학습이 개선된다.
      1.2.1.3 가중치를 초기화 하는 전략역시 다양한 방면에서 개선되고 있다.
      1.2.1.4 학습시에 dropout 이나 dropconnect 와 같이 regularization을 적용하여서 모델의 과적합을 막는다.
    1.2.2 두번째 관점은 얻어낸 feature map에 다른 형식의 classifier를 적용하는 것이다.
      1.2.2.1 이런 모델은 특정 성능이나, 특정 기능을 가진 모델을 학습하게 되는데 neural style과 yolo또는 sppnet과 같이 CNN 구조를 개선하는 것이 아닌 feature map의 데이터를 어떻게 관리해서 특정 기능을 할 수 있게 만드는 것을 목표로 하고 있다.
    1.2.3 세번째 관점은 CNN구조 block을 개선하는 방식이다.
      1.2.3.1 dropout에서 개선된 방식으로 residual 이나 skip connection이나 highway network나 stochastic depth와 같이 connection에 변형을 주어서 모델의 학습속도를 개선하고 과적합을 막는다.
      1.2.3.2 Network in network나 inception model과 같이 하나의 신경망으로 여러개의 ensemble효과를 줄 수 있는 모델을 한번에 학습한다.
      1.2.3.3 Filter를 fatorization하는 방식으로 적은 수의 파라미터로 성능을 얻을 수 있다.
      1.2.3.4 auxillary classifier를 추가해서 학습할때 error를 더 잘 전파 할 수 있게 할 수 있다.
  1.3 위의 다양한 방식으로 CNN이 개선되고 있고, 본 논문에서는 Residual connection을 더 개서 시키는 방식이 제안 되었다.
  1.4 Densely Connected Convolutional Network는 특정 block의 아웃풋간 연결을 다른 상위 layer로 direct connection을 추가하는 방식이다.
  1.5 수학적으로는 Residual connection과 차이가 없지만, 본 모델에서는 layer간 데이터를 받아올때 summation을 통해서 받는 것이 아닌 concatenation을 통해서 입력 데이터가 합쳐진다.
  1.6 적은 수의 필터수만 가지고도 좋은 성능을 내는 모델을 학습 할 수 있다.
  1.7 각 레이어가 모두 output layer에 연결되어 있기 때문에, 모든 feature map을 활용해서 model의 classification을 진행함으로써 과적합을 막을 수 있고, 적은 파라미터로 좋은 성능을 낼 수 있다.
  1.8 결국 파라미터가 적은수가 필요해지기 때문에 과적합 문제도 개선이 되고, 계산 비용역시 감소하지만 성능은 오히려 더 좋은 성능을 얻어내는 모델을 학습 시킬 수 있다.
  1.9 https://arxiv.org/abs/1608.06993

2. RNN에 dropout을 적용시키는 방법
  2.1 RNN이 language model에서 우수한 성능을 내는 것을 확인 할 수 있었다.
  2.2 하지만 RNN의 가장 큰 문제점이 gradient가 vanishing되거나 exploding되는것과 학습중에 과적합이 쉽게 발생한다는 것이다.
  2.3 여기서 과적합을 막기위해서 다른 형태의 신경망에서 제안된 dropout을 적용시키려 하지만 간단하게 적용하면 성능이 감소되는 현상이 나타난다.
  2.4 이를 개선하기위해서 첫째로 non-reccurent connection에만 dropout을 적용시키면, 성능이 개선되는 것을 확인 할 수 있었다.
  2.5 그리고 추가적으로 LSTM의 memory cell에 적용하면 또 역시 성능이 일부분 개선되는 것을 확인할 수 있었다.
  2.6 추가적으로 reccurent connection에 zoneout이나 highway connection으로 hidden state 값을 덜 갱신하거나, 추가적인 gate를 학습해서 모델을 개선 할 수 있다.
  2.7 그리고 본 논문에서 제안된 rnnDrop은 같은 입력에 대해서 같은 dropout mask를 적용하는 것을 말한다.
  2.8 이때 입력이 같으면 모든 time step에 대해서 동일한 dropout mask가 적용되기 때문에, 학습하는데는 오히려 더 쉬울것이라 예상된다.
  2.9 하나의 RNN을 rnnDrop으로 쪼개서 입력데이터 숫자만큼의 모델을 만들어서 ensemble 시키는것과 유사하다고 볼 수 있을것 같다.
  2.10 https://ieeexplore.ieee.org/document/7404775/
