2018.06.26 읽은 논문 정리
A. 딥러닝 학습 속도를 증진 시키기 위한 방법

1. information highway를 적용해서 학습속도를 증진 시킬 수 있다.
  1.1 LSTM에서 gate를 적용 하는 것에 착안해서 레이어의 입력값을 조절하는 파라미터를 추가적으로 학습한다.
  1.2 activation 입력이 H(x,W_h)*T(x,W_t) + x*(1-T(X,W_t)) 로 바뀌어서 입력된다.
  1.3 이를 통해서 학습에서 사용되지 않는 레이어들을 구할 수 있고 신경망 깊이도 조절 할 수 있다.
  1.4 https://arxiv.org/abs/1507.06228
  
2. Batch Normalization을 통해서 학습속도를 drastically 증진 시킬 수 있다.
  2.1 Batch Normalizaion(이하 BN)을 적용하면 얻을 수 있는 강점
    2.1.1 learning rate를 증가 시켜도 값이 잘 발산하지 않는다.
    2.1.2 weight initialization에 비교적 상관없이 학습이 진행 되는 것을 확인 할 수 있다.
    2.1.3 Regularizer로 역할을 하기 때문에 추가적인 Dropout을 적용하지 않아도 된다.
    2.1.4 결론적으로 학습 속도가 증가하게 된다.
  2.2 여기서도 activation input이 변경되어서 입력되는데 그 형태가 y = gamma*x_hat + beta 가 된다.
    2.2.1 x_hat은 mini_batch 내에서 각각 독립변수들의 z값을 계산한 결과가 된다 x_hat = (x-E(x))/sqrt(Var(x))
    2.2.2 그리고 추가적으로 학습을 진행할때 gamma와 beta를 추가적으로 학습하여야 한다.
  2.3 BN을 적용하고 Ensemble기법을 조합해서 State of the Art performace를 얻을 수 있다.
  2.4 https://arxiv.org/abs/1502.03167

B. CNN

1. CNN에서 pooling layer의 사이즈를 2가아닌 fractional number로 정하면 성능이 조금더 좋아 진다.
  1.1 CNN에서 pooling layer는 대부분 max-pooling size 2 (MP2)로 많이 쓰인다.
    1.1.1 MP2는 hidden layer의 size를 very quickly 줄여준다.
    1.1.2 MP2는 traslation과 clastic distortions 에 비교적 잘 저항한다.
  1.2 MP2는 모든 곳에서 overlap 되는 곳이 없고 pooling layer의 크기를 3으로 stride를 2로 하게 되면 모든 곳에서 overlap이 되어서 일반적이지 않다.
  1.3 Fractional Pooling을 사용하면 disjoint와 overlap이 하나의 레이어에서 동시에 존재 할 수 있다.
  1.4 filter의 사이즈는 1이나 2가 되고 이 숫자는 presudorandom number(유사난수)로 생성된다.
  1.5 https://arxiv.org/abs/1412.6071
  
2. Neural Style
  2.1 Classification을 위해 잘 학습된 CNN model은 이미지에 대한 latent variable을 내놓는다.
    2.1.1 이 값을들 이용하면 그림의 content와 style에 대한 정보를 계산할 수 있다.
  2.2 학습된 모델을 이용해서 content와 style을 계산하기 위해서 content를 따라갈 그림 p와 style을 따라갈 그림 a 가 필요하다
  2.3 그림 이미지가 압축되고 다시 생성되야 하기 때문에 max pooling대신에 average pooling이 적용되었다.
  2.4 content를 유사하게 계산하는 대는 입력 x와 content 그림 p를 CNN model에 넣고 출력되는 값들의 L2 norm을 비교한다.
    2.4.1 미분 할때는 ReLU와 유사하게 진행된다 절댓값이 취해지지만 내부가 음수가 될때는 0으로 둔다.
  2.5 style을 유사하게 계산하기 위해서는 x와 style 그림 a를 CNN model에 넣고 출력되는 값들의 gram matrix를 계산해서 gram maxtrix 간의 L2 norm을 계산한다.
  2.6 content loss와 style 로스를 하나의 로스로 합치면서 두가지 서로 다른 hyperparameter 가중치를 각각에 곱한다 보통 content/style = 1e-3 ~ 1e-4로 한다.
  2.7 content는 모든 CNN layer의 결과값에 대해서 계산해서 x를 구해낸다.
  2.8 style의 경우 가중치를 곱해서 특정 레이어의 값만 보통 얻어 낸다.
  2.9 layer가 깊어질수록 content/style값이 작아질수록 style을 더 따라가는 결과값을 얻을 수 있다.
  2.10 존나 싱기방기한 논문 실제로 실험해보고 싶은 논문 빠른 시일내로 해볼 예정 코드와 결과값역시 업로드 예정 아마도~
  2.11 https://arxiv.org/abs/1508.06576
 
