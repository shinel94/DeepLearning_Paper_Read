2018.06.26 읽은 논문 정리
딥러닝 학습 속도를 증진 시키기 위한 방법
1. information highway를 적용해서 학습속도를 증진 시킬 수 있다.
  1.1 LSTM에서 gate를 적용 하는 것에 착안해서 레이어의 입력값을 조절하는 파라미터를 추가적으로 학습한다.
  1.2 activation 입력이 H(x,W_h)*T(x,W_t) + x*(1-T(X,W_t)) 로 바뀌어서 입력된다.
  1.3 이를 통해서 학습에서 사용되지 않는 레이어들을 구할 수 있고 신경망 깊이도 조절 할 수 있다.
  1.4 https://arxiv.org/abs/1507.06228
2. Batch Normalization을 통해서 학습속도를 drastically 증진 시킬 수 있다.
  2.1 Batch Normalizaion(이하 BN)을 적용하면 얻을 수 있는 강점
    2.1.1 learning rate를 증가 시켜도 값이 잘 발산하지 않는다.
    2.1.2 weight initialization에 비교적 상관없이 학습이 진행 되는 것을 확인 할 수 있다.
    2.1.3 Regularizer로 역할을 하기 때문에 추가적인 Dropout을 적용하지 않아도 된다.
    2.1.4 결론적으로 학습 속도가 증가하게 된다.
  2.2 여기서도 activation input이 변경되어서 입력되는데 그 형태가 y = gamma*x_hat + beta 가 된다.
    2.2.1 x_hat은 mini_batch 내에서 각각 독립변수들의 z값을 계산한 결과가 된다 x_hat = (x-E(x))/sqrt(Var(x))
    2.2.2 그리고 추가적으로 학습을 진행할때 gamma와 beta를 추가적으로 학습하여야 한다.
  2.3 BN을 적용하고 Ensemble기법을 조합해서 State of the Art performace를 얻을 수 있다.
  2.4 https://arxiv.org/abs/1502.03167
