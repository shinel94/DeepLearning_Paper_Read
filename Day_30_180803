1. 단어를 embedding하는 다른 방법
  1.1 skip-gram이나 CBOW와 같은 방법으로 단어를 압축하는 방식이 있다.
  1.2 위의 방식은 단어를 기준으로 주변 몇 단어간의 유사도를 측정하는 방법이다.
  1.3 본논문에서 제안하는 skip-thought는 문장간의 관계에서 문장간의 유사도를 측정하는 방식이다.
  1.4 이번 문장의 출력값을 다음번 문장의 데이터를 압축하는데 사용하는 방식으로 적용된다.
  1.5 bidirection을 보통 진행된다. i번째 문장의 출력값을 이용해서 지난번 문장의 순서를 거꾸로 해석하고, 또 이번출력으로 다음번 문장을 압축하는데 사용한다.
  1.6 objective function은 encode로 압축한 데이터를 decode로 분해 햇을때, 문장을 제대로 예측하는지에 대한 probability를 error로 전파 한다.
  1.7 encode로 압축을 하고 압축한 데이터가 다음번이나 지난번 출력에 decode될때 사용되는 방식
  1.8 encode될때는 이번 문자의 데이터로만 encode가 적용된다.
  1.9 성능이 가장 좋지는 않지만, 하나의 모델로 다양한 task에 embedding을 진행 할 수 있다.
  1.10 얻어낸 vector가 문장과 단어의 특징을 general하게 extract되어서 extract된 벡터만 이용해서 추가적인 fine-tuning없이 linear classification을 진행 할 수 있다.
  1.11 https://arxiv.org/abs/1506.06726
