1. 단어를 embedding하는 다른 방법
  1.1 skip-gram이나 CBOW와 같은 방법으로 단어를 압축하는 방식이 있다.
  1.2 위의 방식은 단어를 기준으로 주변 몇 단어간의 유사도를 측정하는 방법이다.
  1.3 본논문에서 제안하는 skip-thought는 문장간의 관계에서 문장간의 유사도를 측정하는 방식이다.
  1.4 이번 문장의 출력값을 다음번 문장의 데이터를 압축하는데 사용하는 방식으로 적용된다.
  1.5 bidirection을 보통 진행된다. i번째 문장의 출력값을 이용해서 지난번 문장의 순서를 거꾸로 해석하고, 또 이번출력으로 다음번 문장을 압축하는데 사용한다.
  1.6 objective function은 encode로 압축한 데이터를 decode로 분해 햇을때, 문장을 제대로 예측하는지에 대한 probability를 error로 전파 한다.
  1.7 encode로 압축을 하고 압축한 데이터가 다음번이나 지난번 출력에 decode될때 사용되는 방식
  1.8 encode될때는 이번 문자의 데이터로만 encode가 적용된다.
  1.9 성능이 가장 좋지는 않지만, 하나의 모델로 다양한 task에 embedding을 진행 할 수 있다.
  1.10 얻어낸 vector가 문장과 단어의 특징을 general하게 extract되어서 extract된 벡터만 이용해서 추가적인 fine-tuning없이 linear classification을 진행 할 수 있다.
  1.11 https://arxiv.org/abs/1506.06726

2. Question과 Answer가 잘 pair되었는지 확인하는 model
  2.1 Question과 Answer가 잘 pair되었는지 확인하기 위한 model이 제안되었다.
    2.1.1 이 모델은 입력 question과 Answer sentence들을 WordeEmbedding으로 문장에 대한 matrix를 얻는다.
    2.1.2 이 문장에 대한 matrix를 이용해서 CNN이나 bidirectional LSTM을 적용해서 sentece를 압축한 feature map을 얻는다.
      2.1.2.1 Bidirectional LSTM은 uni LSTM이 과거에 대한 정보로 미래를 예측하는 것과 달리, 미래와 과거의 정보 즉 전체적인 문장의 내용을 이용해서 현재 위치를 압축하는 방식
    2.1.3 예전에 제안된 방법인 이 Question과 Answer에 대한 각각의 feature map에 단순히 max pooling을 진행해서 얻은 vector로 둘 사이의 similarity를 계산한다.
    2.1.4 이 논문에서는 cosine similairty가 이용됬다.
    2.1.5 correct pair의 similarity는 1에 가깝게 학습하고, incoreect pair의 similairty는 0에 가깝게 학습하는 loss function을 설정하고 SGD를 적용해서 모델을 학습한다.
  2.2 여기서 제안된 pooling이 Qestion과 Answer사이의 관계는 상관없이 모델을 출력하기 때문에, 이를 개선하기 위해서 attentive pooling을 적용했다.
  2.3 attentive pooling은 추가적인 gate를 학습하는 것과 같은 방식으로 학습된다.
  2.4 Qeustion에 대해서 얻은 feature map Q와 Answer에 대해서 얻은 feature map A를 이용해서 gate를 학습하는데 이 둘간의 관계를 찾아줄 선형 가중치 matrix U를 추가해서 학습한다.
  2.5 attentive pooling의 gate는 tanh(Q^TxUxA)로 학습이 진행된다.
    2.5.1 Q를 위해서 사용할 gate 변수들은 column-wise maxpooling된 값에 softmax를 적용시켜서 통과시킨다.
    2.5.2 A를 위해서 사용할 gate 변수들은 row-wise maxpooling된 값에 softmax를 적용시켜서 통과시킨다.
  2.6 얻은 gate vector를 Q와 A에 행렬곱을 시켜서 마지막 similarity를 계산할 두 vector를 얻는다.
  2.7 attentive pooling을 사용하면 더 적은 모델 parameter로 더 좋은 성능을 내는 것을 확인 할 수 있었다.
  2.8 attentive pooling을 사용하면 입력 sequence의 길이가 길어져도, 저하되는 성능이 큰 차이가 없는것을 확인 할 수 있어서, 모델의 robustness가 증가 되는것을 확인 할 수 있었다.
  2.9 embedding 시켜서 얻은 벡터를 CNN이나 bi-LSTM으로 feature map을 얻고, 얻은 feature map을 이용해서 gate를 하나더 학습해서 gate를 통과시켜 Q와 A간 similarity를 비교하는 벡터를 찾는데 사용할 수 있다.
  2.10 https://arxiv.org/abs/1602.03609

3. 욜로 구천
  3.1 제안된 YOLO가 우수한 성능을 보였지만 한계점이 있다.
    3.1.1 localization error가 너무 크다
    3.1.2 recall수치가 region proposal based method보다 너무 낮다.
  3.2 batch normalizartion을 이용해서 성능을 향상 시켰다.
  3.3 classifier network의 사이즈를 2배로 키우고 imagenet을 이용해서 fine tune을 적용해 성능을 개선했다.
  3.4 Fully connected layer를 통해서 bounding box를 예측하는 것이 아닌 anchor box를 통해서 bounding box를 예측한다.
    3.4.1 원래는 각 그리드의 크기를 예측하기 위한 FC가 필요했지만, 이제는 FC가아닌 anchor box를 통해서 feature를 extract할 수 있기 때문
    3.4.2 anchor box는 이미지가 있을 것으로 예상되는 window size 정도로 생각하면 될것 같다.
  3.5 입력이미지에 대한 가장 적절한 anchor box를 k-means clustering으로 찾아서 사용했다.
  3.6 하나의 이미지에서 특정사이즈의 이미지가 특정 position에 있는지를 찾을 수 있는게 anchor box인것 같다.
  3.7 anchor box position을 결정해서 해당 anchor box에 어떤 물체가 있는제 조건부 확률로 계산하는 방식
  3.8 feature map의 size는 줄이고 map의 channel수를 늘려서 성능을 향상 시켰다.
  3.9 마지막에 FC layer 가 사라짐으로써 입력이미지의 크기 제한이 없어져서 다양한 사이즈의 이미지에 대해서 학습을 진행 했다.
  3.10 계산을 빠르게 하기 위해서 feature map을 얻어내는 network를 VGG16 대신 Darkenet-19를 사용했다.
    3.10.1 3x3 filter의 전후에 1x1 filter를 attach해서 3x3의 계산양을 줄일 수 있었다.
  3.11 training할때 data에 random crop, rotation, hue, saturation, and exposure shift를 적용시켰다.
  3.12 Hierarchical Classification을 적용했다.
    3.12.1 label의 단어들을 WordNet에 있는 structure로 hierarchical 구조를 가지게 object를 구성한다
      3.12.1.1 가장 root object는 physical object이기 때문에 Pr(physical object) = 1이 되어야 한다.
  3.13 결국 이미지를 grid로 나눈고 거기에 anchor box데이터를 이용해서 모델을 학습하는 방식인것 같다.
  3.14 layer의 마지막에 FC를 빼고 conv layer를 추가함으로써 입력데이터의 사이즈에 제한이 없어졌다.
  3.15 classification을 hierarchical하게 진행함으로써 다양한 class를 분류 할 수 있게 되었다.
  3.16 마지막으로 압축된 feature map이 이미지의 그리드에대한 그리드 수 와 anchor box의 숫자와 class 숫자에 의해 결정된다
  3.17 마지막에 average pooling으로 원하는 크기로 데이터를 줄인다.
  3.18 줄인 데이터를 이용해서 classification을 진행할때, bounding box를 예측하는 것이 아닌 정해진 anchor box들 중에 탐색을 진행하고, 특정 anchor box에 특정 class 가 등장할 확률을 계산한다.
  3.19 https://arxiv.org/abs/1612.08242
  3.20 https://m.blog.naver.com/sogangori/221011203855

4. Tree Kernel을 이용한 NLP
  4.1 Tree Kernel을 이용하면 robust measure를 얻르 수 있다.
  4.2 flexiblity and interpetability, explicit syntatic feature 를 tree kernel로 정확하게 파악 할 수 있다.
  4.3 training, test time이 길지 않다.
  4.4 아니 논문에 tree kernel을 썼으면 적어도 tree kernel이 어떻게 구성되어 있는 kernel인지는 알려줘야, 이해를 할텐데 그냥 tree kernel만 썻다고 하면 할 수 있는게 없는데....
  4.5 http://www.aclweb.org/anthology/P13-2150
