1. Longer memory 를 learning하는 RNN
  1.1 sequentail data를 address하기 위해서는 RNN을 통해서 하는 것이 좋다.
  1.2 하지만 RNN은 long term dependecy에 대해서는 gradient가 vanishing되거나 exploding되는 문제가 발생한다.
  1.3 exploding의 경우는 gradient cliping으로 쉽고 효율적으로 처리가 가능하다.
  1.4 하지만 대부분 RNN에서는 gradient가 exponentially shrink하기 때문에 short term dependency에 더 focus하는 결과를 보여준다. practically ignoring longer term dependecy
  1.5 gradient가 줄어드는 두가지 이유가 있다.
    1.5.1 Standard nonlinearity function은 gradient가 대부분의 점에서 0과 가깝다. ReLU로 일부분 해결 가능하다.
    1.5.2 시간에 따라서 미분될때, 가중치 행렬의 고윳값이 작다면 시간에 따라서 exponentially 0으로 수렴하게 된다.
  1.6 일반적으로 5 ~ 10step 정도면 0으로 거의 수렴하게 된다. 그래서 long term dependency catch가 힘들다.
  1.7 long term dependecy를 catch할 추가적인 node(context layer)로 Simple Recurrent Neural Network를 개선할 수 있다.
  1.8 context layer는 x에서 값이 추가적인 nonlinear function없이 추가되고, 그 값이 다음 hidden layer와 output에 영향을 준다.
  1.9 시간에 따른 context layer에서 사용되는 가중치 alpha값을 학습할 수 있다면, time delay에 따라서 다르게 context를 capture할 수 있을 것이다.
  1.10 Hidden node의 수가 일정할때 context layer의 node숫자가 많을수록 좋은 성능을 나타낸다.
  1.11 LSTM과 같은수의 hidden node가 있을때 context layer를 가지고 있는 model이 성능이 전반적으로 좋다.
  1.12 LSTM보다 같은 파라미터를 가진다면 더 좋은 성능을 보여준다.
  1.13 https://arxiv.org/abs/1412.7753

2. RNN의 성능을 개선시키는 방법
  2.1 RNN의 출력값에 class vector를 추가해서 class vector와 그 벡터의 embbeding 값 2가지를 이용해서 classification을 진행
  2.2 Hierachical Softmax와 유사한 방법
    2.2.1 하나의 node에서 나온 child node나 class 내에서 특정 값을 나타낼 확률의 합이 1이되게 하는 방법
  2.3 input laye에서 hidden layer로 갈때 compression이 되면 좋고 이때 반드시 activation function은 nonlinear로 되는게 좋다.
  2.4 그리고 원래 output은 단어벡터가 그대로 나와서 값을 비교해서 classification을 진행했는데, class term까지 추가하여서 출력을 비교해서 계산한다.
  2.5 학습속도가 개선되는 것을 확인 할 수 있다.
  2.6 https://ieeexplore.ieee.org/document/5947611

3. 세지말고 예측하자
  3.1 단어를 sementic한 의미를 measure하는데 co-occurance matrix를 사용하면 비교적 sementic한 의미를 catch할 수 있다.
  3.2 DISSECT toolkit과 같은 것을 사용하면 count model을 구성할 수 있다.
  3.3 predict model은 word2vec toolkit으로 쉽게 변형 할 수 있다.
  3.4 skip-gram과 CBOW방법이 있는데 CBOW방법이 larger dataset에 더 적합하다.
  3.5 각 각 방법으로 embbeding된 벡터에서 유사한 단어들의 벡터의 simillarity를 measure해서 semantic한 의미를 추출하는 성능을 파악한다.
  3.6 압축된 단어 벡터들에 clustering을 적용해서 categorization도 measure할 수 있다.
  3.7 단어 벡터들을 통해서 선형적인 연산을 적용해서 sentatic한 의미를 나타내는지도 확인하고 유사한 의미를 지닌 단어들을 pair시켜서 성능을 비교한다.
  3.8 predict 모델이 count model보다 더 robust하고 그리고 초기 설정에 따라 count model은 성능 차이가 발생한다.
  3.9 http://www.aclweb.org/anthology/P14-1023

4. initialization을 잘해서 RNN을 잘 써보자
  4.1 Recurrent neural network는 input sequence 를 output sequence로 mapping시켜주는 model이다.
  4.2 초기 가중치와 momentum을 잘 조절하고 gradient를 clipping하면 좋은 성능을 내는 모델을 얻을 수 있다.
  4.3 LSTM은 longterm dependecy를 위해서 memory cell을 가지고 memory cell에 들어가서 더해지는 정보의 양을 조절하기 위해서 gate구조를 가지고 있고, memory cell에서 데이터를 꺼낼떄 떄 추가적인 gate를 통과해야 한다.
  4.4 초기 ReLU unit의 가중치를 초기화 할때, weigth matrix는 identity matrix로 bias는 zero 벡터로 설정한다.
    4.4.1 제일 첫번째 계산될 때, 지난번 레이어의 값이 그대로 전달되게 된다.
  4.5 recurrent connection에 사용되는 weight들만 identity로 initalize
  4.6 LSTM에서 forget gate의 bias를 크게하면 long term dependecy를 잘 catch함
  4.7 RNN의 recurrent weight를 identity matrix로 초기화 하면, randomly initailizaion한 것보다 long term dependency를 잘 읽어낸다.
  4.8 https://arxiv.org/abs/1504.00941
