1. Longer memory 를 learning하는 RNN
  1.1 sequentail data를 address하기 위해서는 RNN을 통해서 하는 것이 좋다.
  1.2 하지만 RNN은 long term dependecy에 대해서는 gradient가 vanishing되거나 exploding되는 문제가 발생한다.
  1.3 exploding의 경우는 gradient cliping으로 쉽고 효율적으로 처리가 가능하다.
  1.4 하지만 대부분 RNN에서는 gradient가 exponentially shrink하기 때문에 short term dependency에 더 focus하는 결과를 보여준다. practically ignoring longer term dependecy
  1.5 gradient가 줄어드는 두가지 이유가 있다.
    1.5.1 Standard nonlinearity function은 gradient가 대부분의 점에서 0과 가깝다. ReLU로 일부분 해결 가능하다.
    1.5.2 시간에 따라서 미분될때, 가중치 행렬의 고윳값이 작다면 시간에 따라서 exponentially 0으로 수렴하게 된다.
  1.6 일반적으로 5 ~ 10step 정도면 0으로 거의 수렴하게 된다. 그래서 long term dependency catch가 힘들다.
  1.7 long term dependecy를 catch할 추가적인 node(context layer)로 Simple Recurrent Neural Network를 개선할 수 있다.
  1.8 context layer는 x에서 값이 추가적인 nonlinear function없이 추가되고, 그 값이 다음 hidden layer와 output에 영향을 준다.
  1.9 시간에 따른 context layer에서 사용되는 가중치 alpha값을 학습할 수 있다면, time delay에 따라서 다르게 context를 capture할 수 있을 것이다.
  1.10 Hidden node의 수가 일정할때 context layer의 node숫자가 많을수록 좋은 성능을 나타낸다.
  1.11 LSTM과 같은수의 hidden node가 있을때 context layer를 가지고 있는 model이 성능이 전반적으로 좋다.
  1.12 LSTM보다 같은 파라미터를 가진다면 더 좋은 성능을 보여준다.
  1.13 https://arxiv.org/abs/1412.7753

2. RNN의 성능을 개선시키는 방법
  2.1 RNN의 출력값에 class vector를 추가해서 class vector와 그 벡터의 embbeding 값 2가지를 이용해서 classification을 진행
  2.2 Hierachical Softmax와 유사한 방법
    2.2.1 하나의 node에서 나온 child node나 class 내에서 특정 값을 나타낼 확률의 합이 1이되게 하는 방법
  2.3 input laye에서 hidden layer로 갈때 compression이 되면 좋고 이때 반드시 activation function은 nonlinear로 되는게 좋다.
  2.4 그리고 원래 output은 단어벡터가 그대로 나와서 값을 비교해서 classification을 진행했는데, class term까지 추가하여서 출력을 비교해서 계산한다.
  2.5 학습속도가 개선되는 것을 확인 할 수 있다.
  2.6 https://ieeexplore.ieee.org/document/5947611

3. 세지말고 예측하자
  3.1 단어를 sementic한 의미를 measure하는데 co-occurance matrix를 사용하면 비교적 sementic한 의미를 catch할 수 있다.
  3.2 DISSECT toolkit과 같은 것을 사용하면 count model을 구성할 수 있다.
  3.3 predict model은 word2vec toolkit으로 쉽게 변형 할 수 있다.
  3.4 skip-gram과 CBOW방법이 있는데 CBOW방법이 larger dataset에 더 적합하다.
  3.5 각 각 방법으로 embbeding된 벡터에서 유사한 단어들의 벡터의 simillarity를 measure해서 semantic한 의미를 추출하는 성능을 파악한다.
  3.6 압축된 단어 벡터들에 clustering을 적용해서 categorization도 measure할 수 있다.
  3.7 단어 벡터들을 통해서 선형적인 연산을 적용해서 sentatic한 의미를 나타내는지도 확인하고 유사한 의미를 지닌 단어들을 pair시켜서 성능을 비교한다.
  3.8 predict 모델이 count model보다 더 robust하고 그리고 초기 설정에 따라 count model은 성능 차이가 발생한다.
  3.9 http://www.aclweb.org/anthology/P14-1023

4. initialization을 잘해서 RNN을 잘 써보자
  4.1 Recurrent neural network는 input sequence 를 output sequence로 mapping시켜주는 model이다.
  4.2 초기 가중치와 momentum을 잘 조절하고 gradient를 clipping하면 좋은 성능을 내는 모델을 얻을 수 있다.
  4.3 LSTM은 longterm dependecy를 위해서 memory cell을 가지고 memory cell에 들어가서 더해지는 정보의 양을 조절하기 위해서 gate구조를 가지고 있고, memory cell에서 데이터를 꺼낼떄 떄 추가적인 gate를 통과해야 한다.
  4.4 초기 ReLU unit의 가중치를 초기화 할때, weigth matrix는 identity matrix로 bias는 zero 벡터로 설정한다.
    4.4.1 제일 첫번째 계산될 때, 지난번 레이어의 값이 그대로 전달되게 된다.
  4.5 recurrent connection에 사용되는 weight들만 identity로 initalize
  4.6 LSTM에서 forget gate의 bias를 크게하면 long term dependecy를 잘 catch함
  4.7 RNN의 recurrent weight를 identity matrix로 초기화 하면, randomly initailizaion한 것보다 long term dependency를 잘 읽어낸다.
  4.8 이떄 recurrent unit에 사용되는 non linear activation function은 Rectified Linear Unit 으로 설정해야 장기의존성을 잘 읽어냄
  4.9 https://arxiv.org/abs/1504.00941

5. trip2vec 자동차 운행기록을 보고 운행자 상태를 예상해보자 ARNet
  5.1 생물학적 gait와 같이 driver's driving behavior의 패턴을 인식하는게 중요하다.
  5.2 기존의 데이터를 가지고 결과물을 예측하는 방법으로는 새로운 입력에 대해서 오류가 너무나 많이 발생했다.
  5.3 그래서 본 논문에서는 차량의 GPS데이터를 입력으로 하는 Autoencoder Regularized Network를 제안했다.
  5.4 RNN을 통해서 time 에 따른 GPS data를 embadding한 벡터를 입력으로 받아서 데이터를 압축한다.
  5.5 GRU 2개를 쌓았고 한층에 128개의 recurrent 유닛이 있다.
  5.6 압축된 데이터를 바탕으로 다시 본래 입력값으로 복원시키는 FC와 압축된 데이터를 바탕으로 classification 시키는 FC 2개지를 각각 작성해서 두가지 출력을 얻어낸다.
  5.7 autoencoder부분을 통해서 압축된 데이터가 style을 잘 representation하게끔 학습된다.
  5.8 classification부분은 FC를 지나고 softmax를 통해서 classification을 하게된다.
  5.9 autoencoder가 있기 때문에 unseed driver에 대한 classification을 잘하게 된다. 없을때 seen driver에 overfit되는 것으로 볼 수 있다.
  5.10 clustering 방법을 활용해서 driver를 미리 분류한다.
  5.11 autoencoder가 결국 regularizer로 역할을 한다. 그리고 class 는 prior로써 unsupervised learning을 guide하는 역할을 한다.
  5.12 ARNet을 통해 압축된 데이터가 robust하고 clustering도 잘 된다.
  5.13 RNN을 통해서 압축한 데이터를 autoencoder로 복원시키는 모델과 classifiaction 모델을 두가지를 동시에 학습 시키면 unseen data에 대해서 더 잘 적용하는 모델을 만들 수 있다.
  5.14 https://arxiv.org/abs/1701.01272

6. 신경망 구조를 학습하는 신경망 모델
  6.1 강화학습을 통해서 신경망 모델을 학습하는 방법이 제시됬다.
  6.2 RNN과 CNN각각 다음 상황을 갈 수 있는 방법을 제한해서 정해두고, 해당 정책이 선택 되었을때 주어지는 보상을 계산해서 보상이 가장 큰 방향으로 모델을 확장시킨다.
  6.3 기존에 제안되었더 Neural Architecture Search 방법보다 약 1000배 정도 빠른데, 이는 parameter를 sharing 해서 그런것으로 생각된다.
  6.4 RNN의 경우
    6.4.1 해당 인풋이 있는 상황에서 다음 hidden layer로 넘어갈때, RNN모델의 출력 처럼 h1을 계산한다.
    6.4.2 h1을 입력으로 가지는 non linear activation function을 가진 추가적인 hidden node h2를 계산한다.
    6.4.3 h1과 h2를 바탕으로 각 각에 또 다른 activation function 들을 조합하는 h3와 h4를 계산한다.
    6.4.4 이때 h2에 사용된 activation function과 h2를 사용한 hidden node를 계산하는 activation function을 같게해 가중치를 공유한다.
    6.4.5 h1의 경우도 같은 방식으로 계산한다.
    6.4.6 마지막 출력을 h3와h4의 산술평균으로 설정한다.
    6.4.7 이때 변경하는 것은 activation function을 종류들을 변경해가며, 최적의 모델을 찾는다.
  6.5 CNN의 경우
    6.5.1 CNN 입력에서 RNN의 경우와 같이 추가적인 hidden node h2를 계산한다.
    6.5.2 각 각 hidden node에 identity, 3x3, 5x5 convolutional structure를 attach한다
    6.5.3 그리고 3x3 max나 average pooling을 진행한다.
    6.5.4 총 5가지 경우를 조합해서 최적의 모델을 계산한다.
    6.5.5 이때 모든 activation function을 ReLU나 tanh중에 골라서 사용했는데 임의로 identity나 sigmoid로 변경해도 성능이 개선된다.
  6.6 최고의 모델을 찾는다고 생각할 수는 없지만, 최선의 성능을 내는 모델을 학습하는 것으로 생각된다.
  6.7 계산속도역시 이전에 제안된 NAS보다 drastically 빨라서 ENAS라고 부르게 되었다.
  6.8 아직도 봐야할 논문은 산더미, 모르는 개념 산더미 시벙
  6.9 https://arxiv.org/abs/1802.03268
