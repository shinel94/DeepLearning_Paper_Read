1. New Variational inference based dropout technique
  1.1 sequence data를 처리하기위해서 RNN이 제안되었다.
  1.2 RNN은 time step data에 대해서 쉽게 overfit되는 문제가 발생한다.
  1.3 이를 막기위해서 다른 신경망처럼 dropout을 적용했지만, time step reccurent unit에 적용하면 성능이 저하되는문제가 발생된다.
    1.3.1 다양한 zonedout이나 dropconnect와 같은 방식이 제안되었다.
  1.4 적은양의 데이터로 학습을 해야하는 RNN은 regularisation의 부재로 early stopping이나 model자체를 작게하는 방법들이 쓰이고 있다.
  1.5 dropout은 신경망에 적용하는 popular regularisation technique이다.
    1.5.1 network unit을 randomly masked시키는 방법으로 임의로 unit을 dropped 시키는 방법이다.
  1.6 특정 time step 에서 dropout mask를 같게 적용시키는 방식으로 dropout을 적용
  1.7 예전 논문들에서는 RNN에 dropout을 적용시킬때, reccurent connection에 적용을 시키면 성능이 떨어지고, layer방향 non reccurent 방향으로 dropout을 적용시키면 성능이 향상되는 것을 확인 했다.
  1.8 Bayesian Neural Networks는 결국 입력데이터 들을 이용해서 가중치의 prior를 찾을수 있다는 것을 기반하고 있는것 같다.
    1.8.1 결국 weight의 prior distribution을 예측해야 하는데 이를 입력데이터들로(X,Y) 할 수 있고, 이게 dropout이랑 비슷하다.
  1.9 dropout mask를 임의로 0으로 정하는것이 아니라 특정 분포안의 값은 평균이 0이고 분산을 가지는 정규분포로 나머지는 그 값과 분산을 가진 정규분포로 값을 바꾸는 mask를 통해서 성능을 개선시킬 수 있다.
  1.10 LSTM이나 GRU의 입력 x와 hidden state에 dropout mask를 적용시킨다.
  1.11 모든 time step에 mask를 같은 것을 적용했다 고 적혀있다.
    1.11.1 이뜻이 각 batch별로 mask를 다르게 썼다는 뜻인거 같긴 하다.
    1.11.2 하지만 each time step별로 mask를 다르게 설정해서, 다른 batch에 대해서는 같은 time step이면 동일하게 dropout을 적용시킬 수 있을것 같다.
  1.12 tied weight가 gate를 학습할때 2K x 4K로 가중치를 성정한다는 것인것 같다.
  1.13 tied시키면 가중치를 학습할때 한번의 행렬곱이면 되서 single GPU로는 학습이 더욱 효율적이다.
  1.14 embadding connection에 dropout을 적용하지 않고, 너무큰 dropout probability는 overfitting을 유발한다.
  1.15 embedding connection과 recurrent connection에 같이 dropout을 적용하면 robustness가 증가하는 효과를 얻을 수 있다.
  1.16 LSTM이나 GRU에 dropout을 적용하기 위해서 gaussian distribution을 가진 분포의 값으로 all time step에  mask를 할당해서 x input과 hidden state에 dropout mask를 적용시켜서 dropout을 적용시킨다.
  1.17 https://arxiv.org/abs/1512.05287
  
2. RNN으 메모리가 넘모 작어
  2.1 stream of word를 읽고 next word를 predict하는 것과 같은 RNN모델이 제안되었다.
  2.2 이런 모델은 대답을 위해서 기억해야할 데이터를 저장하기 위한 memory cell의 크기가 너무 작아서 성능이 잘 나오지 않는것 같다.
  2.3 단순히 입력된 데이터를 그대로 출력하는 model에서 많은 어려움을 가지고 있다.
  2.4 이러한 memory의 부족을 해결하기 위해서 제안된 것이 memory network이다.
  2.5 input feature map은 가중치의 선형곱만이 아닌 입력데이터를 특정 feature map으로 mapping시킬 수 있는 방법을 사용하면 된다.
  2.6 입력데이터를 어떤 memory cell의 position에 저장할지도 학습을 한다.
  2.7 O는 hidden state에서 output출력을 내는데 R은 output 출력에서 classification이나 regression을 위해서 사용되는 mapping unit이다.
  2.8 입력 데이터들을 그대로 기억시킴으로써, 여러 문장을 읽어야 알 수 있는, 문맥상의 의미를 파악할 수 있는 model을 만들 수 있다.
  2.9 입력데이터들을 저장할때 그대로 저장하는 것은 memory의 한계가 있기때문에 hashing 방법을 통해서 저장할 데이터를 추린다.
    2.9.1 입력 데이터를 그대로 hashing하는 방법이 있는데, 이는 단어를 공유하고 있는지만 considered된다.
    2.9.2 입력 데이터를 clustering을 통해서 정확하게는 k-means clustering을 통해서 cluster point로 할당해서 휴사한 것들을 찾을 수 있게 했다.
  2.10 입력데이터를 그대로 넣던 embedding해서 넣던 데이털르 보관하는 memory를 추가함으로써 문맥을 파악해서 질문에 답하는 모델을 만들 수 있다.
  2.11 KNN이 모든 데이터를 memory에 저장해서 하는것처럼, 특정 기간내의 데이터를 저장하는 memory를 추가해서 성능을 개선했다.
  2.12 https://arxiv.org/pdf/1410.3916.pdf
  
3. Swapout 쓸어서 날려버렷 앙상블 효과를 주는 학습법
  3.1 Swapout은 dropout과 stochasitc depth 그리고 residual architecture를 혼합한 special case이다.
  3.2 Swapout이 강한 regularization을 가한다.
  3.3 dropout은 특정 output의 individual unit을 random하게 zero로 만드는것
  3.4 stochastic depth는 학습할때 데이터 flow를 임의로 skip 시키는 방식으로 모델을 학습시키는것
  3.5 Swapout방법을 통해서 같은 깊이의 ResNets보다 성능을 향상 시킬 수 있다.
  3.6 만약 학습이 가능하다면, 신경망의 깊이가 깊어질수록 모델의 성능이 증가된다.
  3.7 Swapout은 4가지의 조건들 중에 하나의 값으로 반환된다.
    3.7.1 첫번째 반환값은 0으로 dropout의 효과를 얻을 수 있다.
    3.7.2 두번째 반환값은 F(x)로 일반적인 feedforward unit의 반환으로 생각 할 수 있다.
    3.7.3 세번째 반환값은 입력 x벡터를 그대로 반환하는 것으로 skipped connection효과를 얻을 수 있다.
    3.7.4 네번째 반환값은 x + F(x)로 residual network unit의 반환으로 생각 할 수 있다.
  3.8 이 4개의 값들은 X와 F(x)의 값들의 조합으로 이루어 져있기 때문에 2개의 parameter를 임의의 확률로 추출해서 값을 얻을 수 있다.
    3.8.1 theta_1 * x + theta_2 * F(x) 에서 theta들이 임의추출될 parameter
  3.9 이렇게 출력을 정함으로써, model을 학습할때, dropout, stochastic depth, residual connection효과를 동시에 볼 수 있다.
  3.10 swapout을 적용하면 model의 performance도 증가하는데, optimizaion process에서 성능이 개선 된것으로 보인다.
  3.11 layer 간 connection을 학습 할 수 있게 하는것 같다.
  3.12 dropout과 stochastic depth, residual architecture의 효과를 동시에 적용시킬 수 있는 swapout모델을 쓰면 모델의 성능이 더 좋게 학습이 된다.
  3.13 https://arxiv.org/abs/1605.06465

4. RNN에 dropout적용하기
  4.1 feedforward connection에 dropout을 적용하는 것이 아닌 recurrent connection에 long term memory에 loss없이 dropout을 적용했다.
  4.2 단순히 feedforward architecture에 적용하는 것처럼 적용하면 time axis로는 dropout을 적용시키지 못한다.
  4.3 이전의 논문에는 LSTM의 memory cell에 dropout을 적용시켜서 성능을 개선했다
    4.3.1 Moon 의 논문에는 memory cell출력에 dropout이 적용됬다
    4.3.2 Gal의 논문에는 memory cell도 들어오는 input gate를 통과할 데이터에 dropout이 적용되었다.
  4.4 GRU에 단순하게 reccurent에 dropout을 적용하게 되면, dropout probability가 time에 따라 계속해서 곱해지게 되서 long-term dependecy를 capture하지 못하게 된다.
  4.5 하지만 gate에 들어가는 mask vector를 0과 1 두가지를 가진 discrete한 vector로 두면 학습이 잘된다.
  4.6 LSTM은 모든 gate의 hidden state input에 mask vector가 곱해지고, 그리고 memory cell에 위에설명한 두방식으로 각각 dropout이 적용된다.
  4.7 GRU의 경우에도 hidden state의 업데이트에서 지난번 hidden state가 입력될때 mask가 곱해진다.
  4.8 mask로 sampling을 진행하는게 direct로 dropout을 적용하는것보다 성능이 좋다.
  4.9 per step별로 time step 별로 mask를 할당해서 sampling을 하는게 성능이 가장 좋다.
  4.10 conventional한 feedfowrad방향으로 dropout을 적용시키고 mask sampling을 통해서 reccurent unit에 dropout을 적용시키면 모델을 과적합을 막으면서, 잘 학습 시킬 수 있다.
  4.11 https://arxiv.org/abs/1603.05118
  
드디어 백편째 논문, 싀벌 아는거 존나 많은줄 알았는데, 진짜 좆도 모른다.
아모른직다. 어렵다 십ㄹ

5. YOLOY 욜로 you only look once
  5.1 사람은 image를 볼때 glance만 해도 뭐가 어디에 있는지 어떻게 연관되어 있는지 알수 있다.
  5.2 빠르고 정확한 시스템은 자율주행자동차와 같은 곳에 추가적인 센서없이 사용되기에 중요한 모델이다.
  5.3 R-CNN의 경우 우선 후보 box들로부터 potential bounding box를 계산하고 그 box내에서 classifiaction이 진행된다.
  5.4 전체 입력이미지를 SxS grid로 나눈다.
  5.5 각 박스별로 class probability를 계산한다. 이때 box별로는 conditionaly indipendent가 성립한다.
  5.6 bounding boxes 와 confidence로 각 object가 차지하고 있는 bounding box를 계산해서 final detection을 찾아낸다.
  5.7 box 내에 class가 있을 신뢰도를 계산하고 특정 신뢰도 이상의 값만 취함으로써 원하는 box를 찾을 수 있다.
  5.8 개발된 CNN structure에 head 부분을 modify해서 YOLO를 만들었다.
  5.9 모델의 마지막 출력은 각 그리드별로 bounding box의 center의 값과 bounding box의 width와 height 그리고 confidence에 대한 정보 그리고 어떤 class로 할당될지 class의 확률을 가진 vector로 구성되어 있다.
    5.9.1 제안된 모델은 B개의 bounding box를 찾기 때문에 SxSx(5xB+C)로 output이 정해진다.
  5.10 precision이 reponsible한 grid에서만 error cost가 계산된다
  5.11 error cost는 bounding box 의 position error와 size error가 L2 Norm으로 더해지고, class error는 obj가 있을때와 없을때를 나눠서 error가 전파되고 마지막으로 precision역시 L2Norm으로 error가 전파된다.
  5.12 작은 물체가 반복되거나, 학습되지 않은 bounding box처럼 size가 일반적이지 않고 왜곡된 경우 detection이 잘 되지 않는다.
  5.13 몇 단계의 layer를 거쳐서 나온 feature map을 대상으로 bouding box를 예측하므로 object의 localization이 다소 부정확해지는 경우가 있다.
  5.14 작은 box의 error가 큰 box에서 발생한 error보다 더 가혹하게 평가된다.
  5.15 출력의 7x7x30의 값을 입력 이미지의 각 grid값과 비교해서 regression을 진행
  5.16 마지막 출력의 7x7x30에서 1,1,:의 값이 input image의 1,1그리드의 데이터와 같게 학습하는 방식 바뀐건 출력값 단 하나
  5.17 학습할때, Grid cell의 여러 bounding box들 중, ground-truth box와의 IOU가 가장높은 bounding box를 predictor로 설정한다.
    5.17.1 이때 각 grid의 데이터를 아래와 같은 3가지 기준으로 나눈다.
    5.17.2 Object가 존재하는 grid cell i의 predictor bounding box j 
    5.17.3 Object가 존재하지 않는 grid cell i의 bounding box j 
    5.17.4 Object가 존재하는 grid cell i 
  5.18 계산이 빠르다는 것은 model이 light하다고도 받아 들일수 있다, 그래서 YOLO는 CNN의 마지막 부분에 FC를 달아서 regression 모델로 바꿈으로써 test속도를 drastically 거의 in-time test가 가능하도록 video에 대해서 평가 할 수 있도록 제안된 신경망이다.
  5.19 구체적인 계산방법은 위의 설명과 논문에서 제안되어 있으나, 간단하게 말하면 CNN의 통과한 feature map에서 FC를 통과시켜서 regression 을 위한 predicted value를 B*5+C개 만들허서 학습을 진행하는 모델이다.
  5.20 https://arxiv.org/abs/1506.02640
  5.21 드디어 100편째, 여기까지 읽으려고 했는데 아직 모르는게 넘모 많아서 더 읽어야 할것 같다. 계속힘내자 

6. Deep Convolutional Nets이 Deep하고 Convolutional할 필요가 있을까?
  6.1 당연 deep하고 convolutional 해야지
  6.2 같은수의 parameter를 가졌지만 depth가 다른 신경망을 학습시켰을때, depth가 성능에 영향을 미치는지 확인 할 수 있다.
  6.3 신경망 모델을 압축하는 열쇠는 compact model을 another larger, moder complex model을 따라하게끔 학습하는 것이다.
  6.4 emperical한 실혐을 통해서 deep convolutional models 을 to shallow student model로 압축할때, loss of accuracy가 반드시 발생한다.
  6.5 압축한 student 모델의 경우 convolution 쪽으로는 넓어지면 성능이 증가 했지만 depth방향으로 증가할떄는 2~3까지 최고가 되었다가 4~5이 되면 감소하는 것을 확인 할 수 있었다.
  6.6 Conv가 없는 모델로 압축할 때는 더 성능이 좋지 않았다.
  6.7 teacher model의 결과로 soft targets을 학습하는 student 모델은 추가적인 regularizer가 들어가면 성능이 저하 되는것으로 보아서, 선생 model이 가르침녀서 regularization이 작동 하는 것으로 생각 할 수 있다.
  6.8 learnable parameter의 수를 control해서 학습하면 single non convolutional fully connected model이 deep convolutional model의 성능을 따라 잡을 수 없다.
  6.9 하지만 압축된 모델에서는 얇고 conv가 fewer한 모델이 더 좋은 성능을 내는것은 확인할 수 있었다.
  6.10 https://arxiv.org/abs/1603.05691

  
