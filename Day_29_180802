1. New Variational inference based dropout technique
  1.1 sequence data를 처리하기위해서 RNN이 제안되었다.
  1.2 RNN은 time step data에 대해서 쉽게 overfit되는 문제가 발생한다.
  1.3 이를 막기위해서 다른 신경망처럼 dropout을 적용했지만, time step reccurent unit에 적용하면 성능이 저하되는문제가 발생된다.
    1.3.1 다양한 zonedout이나 dropconnect와 같은 방식이 제안되었다.
  1.4 적은양의 데이터로 학습을 해야하는 RNN은 regularisation의 부재로 early stopping이나 model자체를 작게하는 방법들이 쓰이고 있다.
  1.5 dropout은 신경망에 적용하는 popular regularisation technique이다.
    1.5.1 network unit을 randomly masked시키는 방법으로 임의로 unit을 dropped 시키는 방법이다.
  1.6 특정 time step 에서 dropout mask를 같게 적용시키는 방식으로 dropout을 적용
  1.7 예전 논문들에서는 RNN에 dropout을 적용시킬때, reccurent connection에 적용을 시키면 성능이 떨어지고, layer방향 non reccurent 방향으로 dropout을 적용시키면 성능이 향상되는 것을 확인 했다.
  1.8 Bayesian Neural Networks는 결국 입력데이터 들을 이용해서 가중치의 prior를 찾을수 있다는 것을 기반하고 있는것 같다.
    1.8.1 결국 weight의 prior distribution을 예측해야 하는데 이를 입력데이터들로(X,Y) 할 수 있고, 이게 dropout이랑 비슷하다.
  1.9 dropout mask를 임의로 0으로 정하는것이 아니라 특정 분포안의 값은 평균이 0이고 분산을 가지는 정규분포로 나머지는 그 값과 분산을 가진 정규분포로 값을 바꾸는 mask를 통해서 성능을 개선시킬 수 있다.
  1.10 LSTM이나 GRU의 입력 x와 hidden state에 dropout mask를 적용시킨다.
  1.11 모든 time step에 mask를 같은 것을 적용했다 고 적혀있다.
    1.11.1 이뜻이 각 batch별로 mask를 다르게 썼다는 뜻인거 같긴 하다.
    1.11.2 하지만 each time step별로 mask를 다르게 설정해서, 다른 batch에 대해서는 같은 time step이면 동일하게 dropout을 적용시킬 수 있을것 같다.
  1.12 tied weight가 gate를 학습할때 2K x 4K로 가중치를 성정한다는 것인것 같다.
  1.13 tied시키면 가중치를 학습할때 한번의 행렬곱이면 되서 single GPU로는 학습이 더욱 효율적이다.
  1.14 embadding connection에 dropout을 적용하지 않고, 너무큰 dropout probability는 overfitting을 유발한다.
  1.15 embedding connection과 recurrent connection에 같이 dropout을 적용하면 robustness가 증가하는 효과를 얻을 수 있다.
  1.16 LSTM이나 GRU에 dropout을 적용하기 위해서 gaussian distribution을 가진 분포의 값으로 all time step에  mask를 할당해서 x input과 hidden state에 dropout mask를 적용시켜서 dropout을 적용시킨다.
  1.17 https://arxiv.org/abs/1512.05287
  
2. RNN으 메모리가 넘모 작어
  2.1 stream of word를 읽고 next word를 predict하는 것과 같은 RNN모델이 제안되었다.
  2.2 이런 모델은 대답을 위해서 기억해야할 데이터를 저장하기 위한 memory cell의 크기가 너무 작아서 성능이 잘 나오지 않는것 같다.
  2.3 단순히 입력된 데이터를 그대로 출력하는 model에서 많은 어려움을 가지고 있다.
  2.4 이러한 memory의 부족을 해결하기 위해서 제안된 것이 memory network이다.
  2.5 input feature map은 가중치의 선형곱만이 아닌 입력데이터를 특정 feature map으로 mapping시킬 수 있는 방법을 사용하면 된다.
  2.6 입력데이터를 어떤 memory cell의 position에 저장할지도 학습을 한다.
  2.7 O는 hidden state에서 output출력을 내는데 R은 output 출력에서 classification이나 regression을 위해서 사용되는 mapping unit이다.
  2.8 입력 데이터들을 그대로 기억시킴으로써, 여러 문장을 읽어야 알 수 있는, 문맥상의 의미를 파악할 수 있는 model을 만들 수 있다.
  2.9 입력데이터들을 저장할때 그대로 저장하는 것은 memory의 한계가 있기때문에 hashing 방법을 통해서 저장할 데이터를 추린다.
    2.9.1 입력 데이터를 그대로 hashing하는 방법이 있는데, 이는 단어를 공유하고 있는지만 considered된다.
    2.9.2 입력 데이터를 clustering을 통해서 정확하게는 k-means clustering을 통해서 cluster point로 할당해서 휴사한 것들을 찾을 수 있게 했다.
  2.10 입력데이터를 그대로 넣던 embedding해서 넣던 데이털르 보관하는 memory를 추가함으로써 문맥을 파악해서 질문에 답하는 모델을 만들 수 있다.
  2.11 KNN이 모든 데이터를 memory에 저장해서 하는것처럼, 특정 기간내의 데이터를 저장하는 memory를 추가해서 성능을 개선했다.
  2.12 https://arxiv.org/pdf/1410.3916.pdf
  
3. Swapout 쓸어서 날려버렷 앙상블 효과를 주는 학습법
  3.1 Swapout은 dropout과 stochasitc depth 그리고 residual architecture를 혼합한 special case이다.
  3.2 Swapout이 강한 regularization을 가한다.
  3.3 dropout은 특정 output의 individual unit을 random하게 zero로 만드는것
  3.4 stochastic depth는 학습할때 데이터 flow를 임의로 skip 시키는 방식으로 모델을 학습시키는것
  3.5 Swapout방법을 통해서 같은 깊이의 ResNets보다 성능을 향상 시킬 수 있다.
  3.6 만약 학습이 가능하다면, 신경망의 깊이가 깊어질수록 모델의 성능이 증가된다.
  3.7 Swapout은 4가지의 조건들 중에 하나의 값으로 반환된다.
    3.7.1 첫번째 반환값은 0으로 dropout의 효과를 얻을 수 있다.
    3.7.2 두번째 반환값은 F(x)로 일반적인 feedforward unit의 반환으로 생각 할 수 있다.
    3.7.3 세번째 반환값은 입력 x벡터를 그대로 반환하는 것으로 skipped connection효과를 얻을 수 있다.
    3.7.4 네번째 반환값은 x + F(x)로 residual network unit의 반환으로 생각 할 수 있다.
  3.8 이 4개의 값들은 X와 F(x)의 값들의 조합으로 이루어 져있기 때문에 2개의 parameter를 임의의 확률로 추출해서 값을 얻을 수 있다.
    3.8.1 theta_1 * x + theta_2 * F(x) 에서 theta들이 임의추출될 parameter
  3.9 이렇게 출력을 정함으로써, model을 학습할때, dropout, stochastic depth, residual connection효과를 동시에 볼 수 있다.
  3.10 swapout을 적용하면 model의 performance도 증가하는데, optimizaion process에서 성능이 개선 된것으로 보인다.
  3.11 layer 간 connection을 학습 할 수 있게 하는것 같다.
  3.12 dropout과 stochastic depth, residual architecture의 효과를 동시에 적용시킬 수 있는 swapout모델을 쓰면 모델의 성능이 더 좋게 학습이 된다.
  3.13 https://arxiv.org/abs/1605.06465
