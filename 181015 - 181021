1. ShuffleNet V2
  1.1 Group convolution을 진행할때 전체 입력을 여러개의 chunk로 나누어서 연산량을 줄이고, 줄여진 chunk간 데이터 교환 permutation을 진행해서 계산량을 줄이면서 표현력은 유지하고 performance를 좋게 얻을 수 있다.
  1.2 이때 입력과 출력의 channel width이 차이가 커지면 channel의 숫자 차이가 커지면 연산에서 비효울이 커진다.
  1.3 Group Convolution에서 Group의 수가 지나치게 커지면, 메모리 적인 부분에서 비효율 적으로 진행이 되면서 모델의 성능이 저하된다.
  1.4 NIN 구조에서 pallel하게 넓게 되어 있다면, 모델의 performance적인 부분에서 좋지 않은 영향을 준다.
  1.5 element wise연산역시 계산량 부분에서 많은 부분을  차지하고 있기 때문에, 지나치게 많은 element wise연산은 지양해야 한다.
  1.6 위사항을 지키면서 모델을 develop해 나가야 모델을 최적화 하기가 쉽다.
  1.7 https://arxiv.org/pdf/1807.11164.pdf
  
2. DSSD
  2.1 SSD는 배경에 비해서 object가 작을때 분류를 잘 하지 못한다는 단점이 있다.
  2.2 이 한계점을 SSD의 구조와 같은 구조 결과물에 symmetry하게 deconvolution을 진행해서 압축하는 과정의 데이터를 U-Net에서 처럼 같은 위상의 데이터를 element wise sum을 통해서 residual connection을 추가해서 upsampling을 진행한다.
  2.3 이때 feaure map의 크기가 큰것에서 얻은 데이터는 object가 큰 경우의 class를 분류하는 모델로서 적용 할 수 있다.
  2.4 각 prediction 모델마다 class와 bounding box를 YOLO와 같이 계산해서 error를 계산해서 모델을 학습한다.
  2.5 본래 SSD는 VGG를 사용했지만 Residual-101을 사용해서 feature map을 extraction해서 DSSD라는 prediction + classificatio model을 학습한다.
  2.6 SSD의 object에 따른 성능차이를 개선하는 모델을 만들 수 있다.
  2.7 https://arxiv.org/abs/1701.06659
