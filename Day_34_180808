인턴활동 드디어 끝났다 싀방 길고도 길었다
내 방학 어디갔니 허허허허허

1. output에 penalizing을 적용해서 모델의 성능을 개선하자
  1.1 최근 신경망이 다양한 outperformance를 내는 모델을 학습 할 수 있다는 것을 확인 할 수 있다.
  1.2 하지만 많은양의 데이터셋을 입력으로 해도, 모델이 쉽게 과적합 되는것역시 확인 할 수 있다.
  1.3 이를 막기 위해 다양한 방식의 Regularizer나 penalizer를 적용해서 과적합을 막기위한 방식이 제안되고 있다.
    1.3.1 Early Stopping을 이용해서, 모델의 복잡도를 유지하면서, 과적합이 되기전에 validation dataset을 이용해서 에러값이 증가하기 직전까지 학습을 진행하는 방식
    1.3.2 L1/L2 Regularization으로 가중치나 hidden state 출력의 값에 penalizing을 적용해서 가중치가 sparse하고 과적합을 의도적으로 막게 hidden state 출력을 조절 할 수 있다.
    1.3.3 Dropout방식을 적용해서 가중치중 일부를 의도적으로 Drop하여서, 하나의 큰 모델을 작은 여러개의 모델로 학습을 시키고 Ensemble효과를 나타내는 모델을 학습하는 방식
    1.3.4 Batch Normalization이라는 입력 데이터에 대한 정규화 방식으로, 입력데이터를 convariate shift를 적용해서 데이터의 분산에 의한 학습속도 저하를 막을 수 있다.
  1.4 위의 설명한 모든 방식은 output에 대해서는 연구되지 않았고, 오직 weight와 hidden state에만 적용하는 방식이다.
  1.5 그래서 본 논문에서 output에 penalizing을 적용해서 정규화를 적용한다.
  1.6 출력의 Distribution에 대해서 penalizing을 적용하는 방식으로 출력으로 특정 class의 값을 정하는게 아니라, class가 될 probability로 인식해서 모델을 학습한다.
  1.7 이때, 출력의 distribution을 잘 표현 할 수 있게끔학습하는데 그 term이 log(p(y\x))가 된다.
  1.8 그리고 information term, cross entropy를 이용해서 추가적으로 loss function을 구성한다.
    1.8.1 그리고 이때 information term에 특정 threshold 값을 설정해서 해당 값보다 크게 된다면, output의 distribution만을 이용해서 모델을 학습하게 된다.
  1.9 그리고 label smoothing방식의 loss function은 information term이 결국 cross entropy라는 hard classfication term인데 이를 변형해서 uniform distribution과 information 값 사이의 KL divergence를 최대화 하게 모델을 학습한다.
  1.10 그리고 KL divergence에서 일반적인 P\\u 가아닌 u\\P를 적용해서 confidence에 대한 정보를 반영할 수 있다.
  1.11 information equation과 probability distribution을 uniform distribution과 KL divergence를 계산해서 적절한 계수를 통해서 loss function을 정해서 학습하면, 학습속도와 과적합발생하는 문제를 일정부분 개선 할 수 있다.
  1.12 https://arxiv.org/abs/1701.06548
  
2. computing resource가 한계가 있는 device에 적용할 수 있는 Shuffle Net
  2.1 최근 신경망을 통해서 다양하고 복잡한 문제들이 해결되고 있다.
  2.2 그리고 CNN은 보통 Hundreds of layers와 Thousands of channels들로 구성되어 있다.
  2.3 이를 통해서 학습을 하기 위해서는 billions of FLOPs의 계산 속도가 필요해진다.
  2.4 GoogLeNet에서 볼 수 있듯이 신경망의 depth가 증가하면서 모델의 성능이 증가 되는 것을 확인 할 수 있다.
  2.5 SqueezeNet에서는 파라미터를 줄이므로써 모델의 성능은 저하시키지 않고 계산량은 줄일 수 있었다.
  2.6 ResNet에서 bottleneck structure를 통해서 성능을 개선 할 수 있었다.
  2.7 하지만 위의 모델들은 대부분 학습할때 많은양의 계산량을 필요로 한다.
  2.8 그래서 Suffle Net이라는 방식을 도입해서 1x1 필터를 모든 채널간 connection을 연결하는 것이 아니라 group화 시켜서 group내에서만 connection이 연결되게 학습을 진행해서 계산량과 성능을 얻을 수 있다.
  2.9 그리고 이 그룹간 layer가 여러층 쌓이게 되는데, group간 filter들을 shuffle을 통해서 성능을 향상 되는 것을 확인 할 수 있었다.
  2.10 다른 복잡한 모델을 같은 계산복잡도를 가지게 되면 shuffle net보다 낮은 성능을 나타내는 것을 확인 할 수 있다.
  2.11 1x1신경망의 connection을 줄여 계산량을 줄이면서 많은 수의 feature map을 얻을 수 있는 모델로 계산량이 한계가 있을때 모델의 성능이 가장 좋은 것을 확인 할 수 있다.
  2.12 https://arxiv.org/abs/1707.01083
