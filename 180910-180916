1. Deep Learning is Robust to Massive Label Noise
  1.1 테스트 샘플이 비교적 간단한 MNIST와 CIFAR-10으로 실험을 진행한 단점이 있긴하지만
  1.2 입력데이터의 라벨일 noise가 추가되어있는 즉, 진짜 정답이 아닌 이상한 답을 가진 label이 많은 데이터로 구성된 학습데이터로 학습을 진행해도 모델의 학습이 잘 되는 것을 확인 할 수 있다.
  1.3 이전에는 입력데이터에 준지도학습 semi supervised learning을 적용해서 noise 데이터를 보다 clean한 데이터로 만들려고 노력을 하는 방식이 제안되었다.
  1.4 그리고 다른 방식으로는 추가적인 알고리즘을 개발해서 noise데이터에대해서 학습을 더 잘 할수있는 학습알고리즘이나 loss function을 개발하려고 했다.
  1.5 하지만 본 논문에서는 noise를 가진 학습데이터를 이용해서 학습을 진행해도 without bell and whistle로도 학습이 잘 진행되는 것을 확인 할 수 있다.
  1.6 이때 noise데이터와 clean데이터의 비율과 관계없이 clean데이터의 절대적인 양이 일정수준이 이상되어야 학습이 잘 진행되는 것을 확인할 수 있다.
  1.7 학습을 할때 batch_size의 크기가 커지면 커질수록 noise데이터에 대한 영향이 작아지는 것을 확인 할 수 있다.
  1.8 하지만 noise데이터의 비율이 커질수록 batch_size에 의한 robustness가 감소하는 것을 확인 할 수 있다.
  1.9 추가적인 조치가 없어도 noise label을 가진 데이터셋에대해서 학습이 잘 진행 되는것을 확인 할 수 있다.

2. Squeeze and Excitation Network
  2.1 신경망에서 gate가 학습을 잘 시킨다는 것을 바탕으로 self gate mechanism 즉 self attention method로 신경망을 잘 학습 시킬 수 있다.
  2.2 이때 해당 레이어의 출력값들을 이용해서 출력값의 세기를 결정하는 Gate를 해당 출력값으로 학습을 하는 module을 만든다.
  2.3 다른 ResNet이나 Inception 에서 사용하는 module의 출력이 다음 module의 출력으로 들어가기전에 그 사이에 적용하는 moduel로 SE net이라고 명명했다.
  2.4 레이어의 출력으로 channel별 gate vector를 extract하고 extract된 데이터를 2개의 FC를 거쳐서 마지막 출력을 결정할 gate vector를 학습한다.
  2.5 이를 통해서 현재 제안된 다양한 state of art의 성능을 내는 신경망에 적용할 수 있으며, 계산복잡도및 계산비용이 크게 증가하지 않고, 데이터 처리 시간역시 크게 변화시키지 않고 모델의 성능을 증가 시킬 수 있다.
  2.6 결국에 self attention과 같은 방식으로 channel의 세기를 결정할 벡터를 channel값으로 또 같은 위상의 channel 들을 모두 이용해서 excitaion vector를 학습할 수 있고 성능 증가가 나타난다.
  
