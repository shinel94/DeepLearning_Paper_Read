드디어 8월에 들어 왔습니다.
인턴도 끝나가고, 논문도 백편까지 거의 다 도착 했습니다. Just a little more, hang in there

1. Gaussian Process와 Dropout 사이의 수상한 관계
  1.1 보통의 regression 이나 classification tools들은 model의 uncertatinty를 측정하지 못한다.
  1.2 그래서 softmax출력층을 통해서 분포를 전달하는것? 데이터의 분포확률을 전달하는 것? 이 더 모델의 uncertainty를 반영하기에 좋다.
  1.3 모델의 신뢰도를 파악할 수 있다면, 이를 통해서 model의 입력과 출력에 대해서 더 신경쓸 수 있게 된다.
  1.4 이 uncertainty는 reinforcement learning에 매우 중요하다.
  1.5 RL에서 사용하는 Q-value function은 agent가 취할 여러 행동들에 해당하는 quality를 estimate해서 그 차이를 계산해서 action을 취한다.
  1.6 모델의 unvertainty를 측정함으로써 멀리 버려진 정보에 대해서도 추출을 할 수 있게 된다.
  1.7 수학적으로는 Dropout을 적용한 심층신경망과 Deep Gaussian Process가 같다.
  1.8 Dropout의 목표는 approximate distribution과 posterior of deep gaussian process의 KL divergence를 최소화 시키는 것
  1.9 잘 이해는 안되지만 Dropout을 시킬 확률이나 sampling을 어떻게 하느냐에 따라서 process가 gaussian이 되기도 monte carlo가 되기도 하고 그러는듯
  1.10 일 sthocastic한 process를 잘 해석하면 uncertatinty를 계산할 수 있겠다.
  1.11 모델의 uncertainty를 같이 출력함으로써, 그 값을 통해서 추가적인 handle이 필요한 data를 extract할 수 있다.
  1.12 이해가 어렵습니다.
  1.13 https://arxiv.org/abs/1506.02142
