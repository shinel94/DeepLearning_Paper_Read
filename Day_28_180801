드디어 8월에 들어 왔습니다.
인턴도 끝나가고, 논문도 백편까지 거의 다 도착 했습니다. Just a little more, hang in there

1. Gaussian Process와 Dropout 사이의 수상한 관계
  1.1 보통의 regression 이나 classification tools들은 model의 uncertatinty를 측정하지 못한다.
  1.2 그래서 softmax출력층을 통해서 분포를 전달하는것? 데이터의 분포확률을 전달하는 것? 이 더 모델의 uncertainty를 반영하기에 좋다.
  1.3 모델의 신뢰도를 파악할 수 있다면, 이를 통해서 model의 입력과 출력에 대해서 더 신경쓸 수 있게 된다.
  1.4 이 uncertainty는 reinforcement learning에 매우 중요하다.
  1.5 RL에서 사용하는 Q-value function은 agent가 취할 여러 행동들에 해당하는 quality를 estimate해서 그 차이를 계산해서 action을 취한다.
  1.6 모델의 unvertainty를 측정함으로써 멀리 버려진 정보에 대해서도 추출을 할 수 있게 된다.
  1.7 수학적으로는 Dropout을 적용한 심층신경망과 Deep Gaussian Process가 같다.
  1.8 Dropout의 목표는 approximate distribution과 posterior of deep gaussian process의 KL divergence를 최소화 시키는 것
  1.9 잘 이해는 안되지만 Dropout을 시킬 확률이나 sampling을 어떻게 하느냐에 따라서 process가 gaussian이 되기도 monte carlo가 되기도 하고 그러는듯
  1.10 일 sthocastic한 process를 잘 해석하면 uncertatinty를 계산할 수 있겠다.
  1.11 모델의 uncertainty를 같이 출력함으로써, 그 값을 통해서 추가적인 handle이 필요한 data를 extract할 수 있다.
  1.12 이해가 어렵습니다.
  1.13 https://arxiv.org/abs/1506.02142

2. Filter의 channel방향으로 한번에 처리하는것이 아니라 쪼개서 64를 4개짜리 독립된 여러개의 path로 나누는 ResNeXt
  2.1 Filter를 channel방향으로 쪼개서 여러개의 독립된 path로 하지만 수학적으로 topology는 동일하게 structure를 구성해서 image classification을 진행
  2.2 합성곱신경망을 구성할때, 너무나 많은 hyperparameter들을 구성하야 하는 문제를 해결하기 위해 VGGnet에서 동일한 위상을 가진 블록을 반복해서 쌓는 식으로 모델을 작성했다.
  2.3 ResNet역시 이러한 특징을 inherite하고, VGG에 없던 residual, skip-connection을 attach했다.
  2.4 그리고 Inception model에서는 이러한 model의 topoloy적인 design을 개선해서 theoretical complexity를 낮추는 방법을 제안했다.
    2.4.1 split - transform- merge straetegy
    2.4.2 inception에서는 정확하게 큰 크기의 filter를 작은 사이즈의 필터로 쪼갠뒤에 concatenation이 진행되었다.
    2.4.3 이 방법을 통해서 inception model의 layer들을 dense하게 만듦으로써 표현력을 늘렸고, 동시에 오히려 계산량은 줄일 수 있었다.
  2.5 많은 양의 채널을 가진 하나의 path를 적은 수의 채널을 가진 여러개의 path로 쪼갬으로써 계산복잡도를 줄이면서 성능을 개선 시킬 수 있었다.
  2.6 VGG/ResNet의 경우 filter의 width, height, chaanel수를 통제해서 출력이 같으면 모델의 hyperparameter도 동일하게 output이 줄어들면 계산 flops는 일정하게 유지할 수 있게끔 hyperparameter를 setting한다.
  2.7 학습할때는, BatchNormalization과 initialization 전략이 따로 사용됬고, SGD momentum method를 이용해서 학습을 진행 시켰다.
  2.8 path의 숫자가 커지면 커질수록 신경망을 깊고 넓게 잘 학습 시킬 수 있고, 성능도 좋아진다.
  2.9 Inception을 통해서 filter를 쪼갰고 ResNeXt를 통해서 channel을 쪼개서 계산은 dense하게 하지만 model은 sparse하게 만듦으로써 계산 복잡도는 줄이면서, 성능을 향상 시킬 수 있다.
  2.10 https://arxiv.org/abs/1611.05431
  
  
  
  
