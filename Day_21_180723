1. Object classifier로서 적용할 수 있는 Networks on Convolutional feature maps(NoCs)
  1.1 object dector들은 대부분 2가지 main part로 구성되는데 하나는 feature extractor이고 나머지 하나는 classifier이다.
  1.2 feature extractor는 활발히 논의되고 있지만 classifier의 경우에는 그렇지 않다.
    1.2.1 SPPnet이나 Fast/Faster R-CNN의 경우가 featrue extractor의 예시
    1.2.2 대부분 모델에서 classifier는 그냥 simple multi-layer perceptrons으로 구성해서 진행하고 있다.
  1.3 많은 CNN 구조가 연구되고 있지만 모든 구조의 꼭대기에는 다층퍼셉트론이 붙어 있다.
    1.3.1 Fast R-CNN은 RoI pooling출력값을 이영해서 fine-tuned이 진행되고
    1.3.2 Faster R-CNN은 proposing regions 의 값들 역시 fine-tune으로 학습이 된다
    1.3.3 SPPnet의 경우에는 Conv net의 출력값을 특정 사이즈의 값을 가지게 pooling을 하게 되고 그 값을 다층 퍼셉트론으로 분류한다.
  1.4 최근의 많은 논문에서 제시된 방법들은 다층퍼셉트론이 다 붙어 있는데, 이 다층 퍼셉트론 대신에 linear SVM을 써도 성능에는 큰 차이가 없다.
  1.5 근데 이 논문에서는 특별한 구조를 제안 한 것이 아니라 그냥 CNN 모델의 출력값들중 FC가기 전의 값을 또다른 인풋으로 하는 다른 신경망을 제안한 것뿐
  1.6 특별한 추가적이고, 쌈박한 방법이 제시된것 같지는 않다.
  1.7 NoCs는 깊은수록 성능이 좋아지고 Conv layer가 MLP보다 좋은 성능을 낸다.
  1.8 출력으로 새로운 모델이 학습 되는 것이기 때문에, 성능이 개선 되는 것으로 보임
  1.9 https://arxiv.org/abs/1504.06066
  
2. NLP처리를 위핸 Recurrent Convolutional Neural Network RCNN
  2.1 document를 처리하는데 있어서 Recurrent, Recursive neural network가 좋은 성능을 내고 있다.
    2.1.1 Recursive는 단어수의 제곱에 비례하는 시간복잡도를 가지기 때문에 긴 문장에서는 비효율적이다.
    2.1.2 Recurrent는 long term dependency를 찾을 수 없어서, 최근 단어, 뒤쪽 단어를 중요하게 생각하는 경향이 있다.
  2.2 그래서 복잡성을 줄이면서 context의 의미를 잘 catch하기위해서 bidirectional RNN을 제안했음
  2.3 두개 RNN의 direction이 반대 방향으로 구성 되어 있다.
  2.4 left context를 비교하는 model은 지난번 출력과 이번 단어의 embedding vector를 이용해서 이번 출력값을 계산한다.
  2.5 right context의 경우에는 오른쪽 문장에서 계산된 값과 이번 단어의 embedding vector를 이용해서 왼쪽으로 출력값을 넘긴다.
  2.6 같은 position의 값 vector들을 concat 해서 x_i = [c_l(wi);e(wi);c_r(wi)]로 구성해서 activation function을 적용해서 출력 y 를 계산한다.
  2.7 이 y값들 중에서 필요한 값만 추출하는 정제과정을 거친다.
  2.8 각 단어에 대한 출력을 하나의 벡터로 압축 maxpooling한다.
  2.9 이 벡터를 이용해서 다시한번 classification을 위한 퍼셉트론을 적용한다.
  2.10 문서를 k class로 분류하는 softmax를 적용한다.
  2.11 word embedding은 Skip-gram model to pre-train을 적용해서 embedding한다.
  2.12 전통적인 방법보다 sparse한 data에도 강하고 symentic한 의미도 잘 잡아낸다.
  2.13 Recursive NN의 경우에는 tree의 structure가 성능에 큰 영향을주는 것 같다.
  2.14 long distance feature도 RCNN모델을 사용하면 잘 catch할 수있고, 추가적인 window filter를 사용하는 것이 아니라서 사람의 영향이 들어가는 hyper parameter가 적고 성능이 전반적으로 좋다.
  2.15 https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745
  
3. 오또엔코다
  3.1 오터엔코더를 학습할때는 layer별로 비지도학습 방식으로 데이터를 변형했다가 다시 복원하게 신경망을 구성하게 한다.
  3.2 이 오터언코더의 가중치 데이터를 신경망의 초기값으로 설정하면 local minima problem에 조금더 강한 모습을 보인다.
  3.3 denoising autoencoder의 경우에는 입력데이터에 일부 데이터값을 누락시키는 등, corruption을 주고 그 값을 입력으로 받고 출력을 왜곡되기전 데이터와 같아지게 학습을 진행하는 신경망
  3.4 이때 입력이 압축될때 사용하는 가중치와 압축된 값을 decode하는 가중치는 서로 symmetry관계인 가중치 텐서를 사용한다.
  3.5 변형된 X를 기준으로 원래 상태의 X를 예측 또는 복원하는 것이 Denoising Autoencoder
  3.6 corruption된 X를 autoencoder로 corruption되기 전으로 돌리게 학습시킴
  3.7 학습방법은 원래 X와 autoencoder의 최종 출력 Y간 distance나 loss 또는 difference가 최소화가 되게 W를 학습
  3.8 이 모델을 이용하면 data distribution들 중에 interesting structure를 잘 capture할수 있다.
  3.9 압축된 데이터를 이용해서 classification과 같은 supervised learning을 하면 잘된다.
  3.10 심층신뢰망이나 제약볼츠만머신들보다 좋은 성능을 가지는 것으로 보여진다.
  3.11 http://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf
