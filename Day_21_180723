1. Object classifier로서 적용할 수 있는 Networks on Convolutional feature maps(NoCs)
  1.1 object dector들은 대부분 2가지 main part로 구성되는데 하나는 feature extractor이고 나머지 하나는 classifier이다.
  1.2 feature extractor는 활발히 논의되고 있지만 classifier의 경우에는 그렇지 않다.
    1.2.1 SPPnet이나 Fast/Faster R-CNN의 경우가 featrue extractor의 예시
    1.2.2 대부분 모델에서 classifier는 그냥 simple multi-layer perceptrons으로 구성해서 진행하고 있다.
  1.3 많은 CNN 구조가 연구되고 있지만 모든 구조의 꼭대기에는 다층퍼셉트론이 붙어 있다.
    1.3.1 Fast R-CNN은 RoI pooling출력값을 이영해서 fine-tuned이 진행되고
    1.3.2 Faster R-CNN은 proposing regions 의 값들 역시 fine-tune으로 학습이 된다
    1.3.3 SPPnet의 경우에는 Conv net의 출력값을 특정 사이즈의 값을 가지게 pooling을 하게 되고 그 값을 다층 퍼셉트론으로 분류한다.
  1.4 최근의 많은 논문에서 제시된 방법들은 다층퍼셉트론이 다 붙어 있는데, 이 다층 퍼셉트론 대신에 linear SVM을 써도 성능에는 큰 차이가 없다.
  1.5 근데 이 논문에서는 특별한 구조를 제안 한 것이 아니라 그냥 CNN 모델의 출력값들중 FC가기 전의 값을 또다른 인풋으로 하는 다른 신경망을 제안한 것뿐
  1.6 특별한 추가적이고, 쌈박한 방법이 제시된것 같지는 않다.
  1.7 NoCs는 깊은수록 성능이 좋아지고 Conv layer가 MLP보다 좋은 성능을 낸다.
  1.8 출력으로 새로운 모델이 학습 되는 것이기 때문에, 성능이 개선 되는 것으로 보임
  1.9 https://arxiv.org/abs/1504.06066
  
2. NLP처리를 위핸 Recurrent Convolutional Neural Network RCNN
  2.1 document를 처리하는데 있어서 Recurrent, Recursive neural network가 좋은 성능을 내고 있다.
    2.1.1 Recursive는 단어수의 제곱에 비례하는 시간복잡도를 가지기 때문에 긴 문장에서는 비효율적이다.
    2.1.2 Recurrent는 long term dependency를 찾을 수 없어서, 최근 단어, 뒤쪽 단어를 중요하게 생각하는 경향이 있다.
  2.2 그래서 복잡성을 줄이면서 context의 의미를 잘 catch하기위해서 bidirectional RNN을 제안했음
  2.3 두개 RNN의 direction이 반대 방향으로 구성 되어 있다.
  2.4 left context를 비교하는 model은 지난번 출력과 이번 단어의 embedding vector를 이용해서 이번 출력값을 계산한다.
  2.5 right context의 경우에는 오른쪽 문장에서 계산된 값과 이번 단어의 embedding vector를 이용해서 왼쪽으로 출력값을 넘긴다.
  2.6 같은 position의 값 vector들을 concat 해서 x_i = [c_l(wi);e(wi);c_r(wi)]로 구성해서 activation function을 적용해서 출력 y 를 계산한다.
  2.7 이 y값들 중에서 필요한 값만 추출하는 정제과정을 거친다.
  2.8 각 단어에 대한 출력을 하나의 벡터로 압축 maxpooling한다.
  2.9 이 벡터를 이용해서 다시한번 classification을 위한 퍼셉트론을 적용한다.
  2.10 문서를 k class로 분류하는 softmax를 적용한다.
  2.11 word embedding은 Skip-gram model to pre-train을 적용해서 embedding한다.
  2.12 전통적인 방법보다 sparse한 data에도 강하고 symentic한 의미도 잘 잡아낸다.
  2.13 Recursive NN의 경우에는 tree의 structure가 성능에 큰 영향을주는 것 같다.
  2.14 long distance feature도 RCNN모델을 사용하면 잘 catch할 수있고, 추가적인 window filter를 사용하는 것이 아니라서 사람의 영향이 들어가는 hyper parameter가 적고 성능이 전반적으로 좋다.
  2.15 https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745
  
3. 오또엔코다
  3.1 오터엔코더를 학습할때는 layer별로 비지도학습 방식으로 데이터를 변형했다가 다시 복원하게 신경망을 구성하게 한다.
  3.2 이 오터언코더의 가중치 데이터를 신경망의 초기값으로 설정하면 local minima problem에 조금더 강한 모습을 보인다.
  3.3 denoising autoencoder의 경우에는 입력데이터에 일부 데이터값을 누락시키는 등, corruption을 주고 그 값을 입력으로 받고 출력을 왜곡되기전 데이터와 같아지게 학습을 진행하는 신경망
  3.4 이때 입력이 압축될때 사용하는 가중치와 압축된 값을 decode하는 가중치는 서로 symmetry관계인 가중치 텐서를 사용한다.
  3.5 변형된 X를 기준으로 원래 상태의 X를 예측 또는 복원하는 것이 Denoising Autoencoder
  3.6 corruption된 X를 autoencoder로 corruption되기 전으로 돌리게 학습시킴
  3.7 학습방법은 원래 X와 autoencoder의 최종 출력 Y간 distance나 loss 또는 difference가 최소화가 되게 W를 학습
  3.8 이 모델을 이용하면 data distribution들 중에 interesting structure를 잘 capture할수 있다.
  3.9 압축된 데이터를 이용해서 classification과 같은 supervised learning을 하면 잘된다.
  3.10 심층신뢰망이나 제약볼츠만머신들보다 좋은 성능을 가지는 것으로 보여진다.
  3.11 http://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf

4. RNN에 Residual을 합치자
  4.1 Residual neural network ResNet이 매우 깊은 layer를 가진 신경망에 학습을 잘되는 것을 확인 할 수 있었다.
    4.1.1 highway network처럼 바로 통과하는 입력이 있어서 gradient가 vanishing되는 문제를 control할수 있다.
  4.2 RNN에 residual을 적용 in vanila RNN의 경우
    4.2.1 hidden vector들이 ResNet의 x처럼 이번 인풋과 함께 출력으로 나오고 그와 동시에 계산에 들어가지 않고 바로 통과하는 residual이 있음
      4.2.1.1 ResNet의 x역할을 RNN에서 h가 맡음
    4.2.2 ResNet에도 모든 layer에서 적용하는 것이 아니라 특정 block 단위에서 적용했듯이 RNN에서도 RNN model이 K만큼 붙어 있는 것을 하나의 block으로 생각함
      4.2.2.3 가중치 벡터가 K개로 늘어남(RNN에서는 가중치를 공유하기 때문)과 동시에 저장해야될 벡터들도 K개로 들어남
  4.3 Long term dependecy를 저장하는 LSTM의 경우 그냥 Skip connection을 적용하면 잘 안되서 다른 modify된 모델을 제안
    4.3.1 alpha만큼의 기간 term이 지날때마다 alpha만큼 전에서의 hidden layer출력값을 이번 hidden layer출력값으로 add하게 structure를 구성
  4.4 Hybrid Residual LSTM
    4.4.1 LSTM과 vanila RNN에 residual을 적용한 RRN에 같은 입력을 넣어서 출력을 얻은뒤 두개의 hidden layer값을 산술평균하여서 다음 layer로 전달함
  4.5 ResRNN을 쓰면 같은 성능을 내는 LSTM에 비해서 파라미터수가 절반정도면 된다.
    4.5.1 이를 통해서 information flow를 잘 조절할 수 있고, Residual이 Sequence learning에 도움을 많이 주는것을 확인 할 수 있었다.
  4.6 Skip-Connect LSTM이 vanila LSTM보다 좋은 성능을 보인다.
    4.6.1 jump하는 distance L에 따라 very long term dependecy도 잘 catch할수 있다.
  4.7 https://www.aclweb.org/anthology/D16-1093

5. Residual 엘에스템
  5.1 Shortcut connection을 memory cell 에 적용하지 않고 output vector에 적용을 시킨다.
    5.1.1 memory cell에 short cut connection을 이용해서 reuse하면 학습을 오히려 더 어렵게 만든다.
  5.2 depth gate를 통해서 vanila LSTM에서 memory cell의 데이터를 layer방향 depth방향으로 전파 시키는 모델도 있다.
  5.3 Residual LSTM의 경우에는 input, forget, memory cell, output vector가 변형되지 않고 그대로 쓰인다.
  5.4 바뀌는 것은 hidden unit이 바뀌는데 hidden의 후보가 되는 벡터가 output vector와 곱해지기 전에 이번 입력을 한번더 더해주게 된뒤(residual) output vector와 곱해진다.
  5.5 ResNet처럼 residual은 입력 x vector이고 출력이 tanh(c)가 된다.
  5.6 이를 통해서 파라미터 숫자가 증가하지 않고 성능 개선을 할 수 있다. Highway LSTM은 파라미터숫자가 증가하고, memory cell data를 control하는 경향이 있다.
  5.7 Highway는 degradation문제가 나타나지만 Residual model은 degradation문제가 나타나지 않는다.
  5.8 plain은 degradation문제가 심각하게 발생하고 Residual이나 Highway를 적용한 model이 전반적으로 좋은 performance를 보여준다.
  5.9 https://arxiv.org/abs/1701.03360



