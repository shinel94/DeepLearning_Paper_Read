논문 읽기 2일차 힘내자
A. Neural Style
  1. Gram Matrix가 왜 style을 표현하는가
    1.1 기존의 non-parametric method로 질감(texture)를 추출하면 low-level freature들 밖에 추출 할 수 없었다.
    1.2 neural style transfer를 통해서 유의미한 결과를 얻을 수 있었다.
      1.2.1 Deep Convolutional Neural Network로 구성되어 있다.
      1.2.2 Gram Matrix로 각 층마다 artistic style of image를 계산해낸다.
      1.2.3 content와 함께 iteration을 돌면서 white noise image가 결과물로 변환된다.
      1.2.4 많은 추가적인 논문들이 나오고 있지만, 모든 논문에서 Why could the Gram matrices represent the artistic style?을 해결하지 않는다.
      1.2.5 Second order poly kernel이 MMD를 만족하는 것을 확인 할 것이다.
        1.2.5.1 Maximum Mean Discrepancy(MMD) : 임의의 두 데이터 셋이 같은 분산에서 얻은 것인지 확인 하는 방법
        1.2.5.2 Set.1 과 Set.2 의 평균의 차이의 제곱으로 곈하는데 이때 평균을 구하는 대신에 kernel을 이용해서 계산한다.
    1.3 Gram Matrix가 결국 Kernel trick으로 변형이 가능한데 Kernel function마다 다른 style capuring이 일어 날것이다.
    1.4 각각 층 마다 다른 특징을 뽑아낸다.
    1.5 상위층으로 갈수록 전체적인 특징을 추출해 낸다.
    1.6 커넬마다 다른 특징을 추출해 낸다.
    1.7 여러 Kernel Function을 Fusion해서 사용 할 수도있다.
    1.8 Gram Matrix가 MMD와 같은 결과를 나타낸다. 그래서 두 데이터간 분산을 일치 시키는 과정을 진행한다.
    1.9 https://arxiv.org/abs/1701.01036
    
B. Deep Neural Net Learning
  1. Hessian-Free optimization 을 사용하면 아주 깊은 레이어를 가진 신경망이나 순환신경망을 학습 할 수 있다.
    1.1 아주 깊은 레이어를 가진 DNN의 경우 학습이 이전 논문에서 발표된것 같이 전반적으로 잘 되지 않는다
    1.2 효과적인 initialization 전략과 momentum accerlation을 활용하면 학습이 잘 된다.
    1.3 두가지 momentum gradient discent algorithm이 있다.
      1.3.1 Classical Momentum(CM)
      1.3.2 Nesterov's Accelerated Gradient(NAG)
      1.3.3 CM보다 NAG가 조금더 안정적이다.
      1.3.4 실제로 실험해본 결과 좀더 큰 momentum constant(mu)에 대해서 CM은 oscillation 하는 반면 NAG는 oscillation하지 않는다.
    1.4 복잡한 HF 대신에 잘 random initialization하고 NAG 방법과 같은 momentum method를 사용하면 HF에 준하는 결과를 얻을 수 있다.
    1.5 NAG를 사용하면 momentum constant를 크게 정해도 oscillation에 비교적 잘 견디고, 큰 momentum을 사용하면 학습속도 및 성능역시 drastically 증가하는 것을 확인 할 수 있다.
    1.6 http://www.cs.toronto.edu/~fritz/absps/momentum.pdf
    
