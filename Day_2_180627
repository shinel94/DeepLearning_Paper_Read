논문 읽기 2일차 힘내자
A. Neural Style
  1. Gram Matrix가 왜 style을 표현하는가
    1.1 기존의 non-parametric method로 질감(texture)를 추출하면 low-level freature들 밖에 추출 할 수 없었다.
    1.2 neural style transfer를 통해서 유의미한 결과를 얻을 수 있었다.
      1.2.1 Deep Convolutional Neural Network로 구성되어 있다.
      1.2.2 Gram Matrix로 각 층마다 artistic style of image를 계산해낸다.
      1.2.3 content와 함께 iteration을 돌면서 white noise image가 결과물로 변환된다.
      1.2.4 많은 추가적인 논문들이 나오고 있지만, 모든 논문에서 Why could the Gram matrices represent the artistic style?을 해결하지 않는다.
      1.2.5 Second order poly kernel이 MMD를 만족하는 것을 확인 할 것이다.
        1.2.5.1 Maximum Mean Discrepancy(MMD) : 임의의 두 데이터 셋이 같은 분산에서 얻은 것인지 확인 하는 방법
        1.2.5.2 Set.1 과 Set.2 의 평균의 차이의 제곱으로 곈하는데 이때 평균을 구하는 대신에 kernel을 이용해서 계산한다.
    1.3 Gram Matrix가 결국 Kernel trick으로 변형이 가능한데 Kernel function마다 다른 style capuring이 일어 날것이다.
    1.4 각각 층 마다 다른 특징을 뽑아낸다.
    1.5 상위층으로 갈수록 전체적인 특징을 추출해 낸다.
    1.6 커넬마다 다른 특징을 추출해 낸다.
    1.7 여러 Kernel Function을 Fusion해서 사용 할 수도있다.
    1.8 Gram Matrix가 MMD와 같은 결과를 나타낸다. 그래서 두 데이터간 분산을 일치 시키는 과정을 진행한다.
    1.9 https://arxiv.org/abs/1701.01036
    
  2. Image의 content와 style을 image recognition을 위해 학습된 CNN을 이용해서 수치적으로 계산할 수 있다.
    2.1 주된 목적은 image가 가지고 있는 Sementic한 의미를 추출해 내는것
    2.2 많은 논문들이 non-parametric method로 image의 structure를 변경하지 않고 texture를 calculate한다.
      2.2.1 하지만 이런 방법들은 오직 low-level image feature들만 뽑아낼 수 있다는 fundamental limitation이 있다.
    2.3 이미지의 데이터 분포가 어떠한 형상인지는 상관없이 항상 texture를 추출 할 수 있어야 한다.
    2.4 SOTA의 성능을 가지는 CNN으로 하면 가능하다.
    2.5 max pooling 은 average pooling으로 변경
    2.6 층이 깊어 질수록 high-level content의 특징을 추출해낸다.
    2.7 층이 얕을 수록 재구성할때 실제 그림과 같게 재구성 된다.
    2.8 Gram Matrix로 feature를 수치적으로 계산 할 수 있다.
    2.9 Content는 하나의 Layer에서 loss를 계산
    2.10 Style은 여러 Layer에서 loss를 계산
    2.11 이 논문의 key는 CNN을 사용하면 content와 style을 따로따로 분리해서 계산할 수 있다는 점이다.
    2.12 https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf
    
B. Deep Neural Net Learning
  1. Hessian-Free optimization 을 사용하면 아주 깊은 레이어를 가진 신경망이나 순환신경망을 학습 할 수 있다.
    1.1 아주 깊은 레이어를 가진 DNN의 경우 학습이 이전 논문에서 발표된것 같이 전반적으로 잘 되지 않는다
    1.2 효과적인 initialization 전략과 momentum accerlation을 활용하면 학습이 잘 된다.
    1.3 두가지 momentum gradient discent algorithm이 있다.
      1.3.1 Classical Momentum(CM)
      1.3.2 Nesterov's Accelerated Gradient(NAG)
      1.3.3 CM보다 NAG가 조금더 안정적이다.
      1.3.4 실제로 실험해본 결과 좀더 큰 momentum constant(mu)에 대해서 CM은 oscillation 하는 반면 NAG는 oscillation하지 않는다.
    1.4 복잡한 HF 대신에 잘 random initialization하고 NAG 방법과 같은 momentum method를 사용하면 HF에 준하는 결과를 얻을 수 있다.
    1.5 NAG를 사용하면 momentum constant를 크게 정해도 oscillation에 비교적 잘 견디고, 큰 momentum을 사용하면 학습속도 및 성능역시 drastically 증가하는 것을 확인 할 수 있다.
    1.6 http://www.cs.toronto.edu/~fritz/absps/momentum.pdf
  
  2. Hidden layer의 directness와 transparency를 높이기 위한 Deeply-Supervised Nets(DSN)
    2.1 DSN을 사용하면 dropconnect나 maxout을 사용한 것처럼 classification error가 눈에 띄게 줄어들 것이다.
    2.2 솔직히 논문 잘 이해가 안된다.
    2.3 신경망 구조조차 이해가 잘 안된다. 그냥 CNN으로 featrue map을 추출해서 다른 classification을 적용 한다는 것인 것 같다.
    2.4 https://arxiv.org/abs/1409.5185
    
  3. Energy 기반 loss function
    3.1 많은 machine learning algorithm이 energy를 기반으로한 loss function을 계산해서 minimization을 진행한다.
    3.2 Latent Variable은 energy에 영향을 주는 extra variable로 but is not observed
      3.2.1 energy를 minimize하는 것으로 latent variable을 학습 할 수있다.
      3.2.2 latent variable은 숨겨진 특징을 나타낼 수 있다.
    3.3 에너지가 얼마나 카테고리를 잘 예측하는지를 나타내는 지표가 된다.
    3.4 에너지를 계산 할때는 모델을 통과한 Latent Variable로 계산된다.
    3.5 에너지 함수를 만족하기 위한 loss function의 조건이 나열 되어 있다.
    3.6 에너지 함수를 계산하기 위한 latent variable은 X와 Y 모두 model(such as 신경망)에 넣어서 얻어 내고 model의 파라미터들을 에너지 텀을 SGD와 같은 방법으로 학습한다.
    3.7 http://yann.lecun.com/exdb/publis/pdf/lecun-huang-05.pdf
