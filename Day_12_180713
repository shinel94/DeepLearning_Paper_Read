1. 두구두구 Inception Model
  1.1 신경망을 deeper하고 wide하게 구성하면 성능이 좋아지는 것은 널리 알려진 정설
  1.2 하지만 이렇게 구성하게 되면 finite한 컴퓨팅 리소스의 한계에 마주하면서 학습이 불가능한 지경에 이르게 됨
  1.3 또 파라미터수가 기하급수적으로 많아지면서 이에 필요한 데이터수 역시 기하급수적으로 증가하게됨
  1.4 그래서 이를 해결하기위한 방법중 하나가 Network in Network에서 제안하고 수정한 Inception model
  1.5 layer사이의 연결을 하나의 FC나 conv 레이어로 하는것이 아니라 ensemble효과를 줄 수 있는 다양한 모델을 추가함
  1.6 특히 1*1 conv 레이어는 여러개의 채널을 가진 데이터에 적용하는 FC와 비슷한 방법
    1.6.1 하나의 입력데이터의 채널간 변수를 이용해서 latent variable을 추출해냄
  1.7 이를 이용하면 가중치가 sparse한것같은 효과를 얻을 수 있어서 성능이 좋아짐
  1.8 계산은 dense하게 진행되서 학습은 빨라짐
  1.9 inception model은 이 구조를 많이 반복해서 학습을 진행, 이때 에러를 보다 잘 전파해주기 위해서 중간중간 에러를 전파해주는 추가적인 텀이 학습시에는 적용됨
  1.10 https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf
  1.11 인용수가 8000번 실화냐
