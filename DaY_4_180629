논문 읽기 4일차 벌써 금요일 입니다.

  1. Content 이미지의 Sihouettes를 보존하면서 transfer하는 loss fucntion을 통해서 logo generation model은 만들자
    1.1 본래 neural style은 content와 style 각 각의 이미지에서 특징들을 추출해서 둘을 mix해서 결과물을 만들어낸다.
    1.2 color를 preserve 하는 neural style도 제시 되었다.
      1.2.1 color를 preserve하기 위해서 style의 색깔을 affine변환을 통해서 content와 유사하게 만드는 방법이 있다.
      1.2.2 image에서 특징을 비교할때 luminance를 비교해서 학습하면 된다.
    1.3 image recogition을 위해 잘 학습된 CNN model을 사용하면 image의 latent variable을 extract하기 편리하다.
    1.4 바깥쪽 윤곽선을 기준으로 윤곽선에서 외부로 멀어 질수록 크게 그리고 윤곽선 내부로는 0이 되게 distance matrix를 작성해서 이 행렬을 비교한다.
      1.4.1 distance matrix는 생성된 이미지와 content의미지의 픽셀건 거리를 통해서 가장 짧은 선이 윤곽선이 되고 그 내부는 0으로 다시 계산된다.
    1.5 이를 통해서 silhouette 근처의 noise도 제거할 수 있다.
    1.6 총 loss function이 결국 L_content과 L_style과 L_distance의 적절한 비율의 합으로 결정된다.
    1.7 loss function에 dinstance term이 들어가게 되면, silhouette를 더 잘 유지하면서 transfer가 가능한 neural style을 만들 수 있다.
    1.8 https://arxiv.org/abs/1803.00686
  
  2. Large number of precise syntatic and semantic word relationship을 계산 할 수있는 Skip-Gram model
    2.1 subsampling을 통해서 leaning speed를 증진시킬 수있다.
    2.2 Skip-Gram을 통해서 가공된 단어를 나타내는 벡터들은 기본적인 논리연산을 적용할 수있다.
    2.3 자주 등장하는 단어에 낮은 가중치를 곱함으로써 학습속도를 증진 시킬수 있다.
    2.4 단어를 나타내는 vector들간의 cosine distnace를 계산함으로써 semantic한 meaning을 계산할 수 있다.
    2.5 Skip - Gram을 통해서 단어들의 sementic한 meaning을 빠르고 정화가게 학습할 수 있고, Negative Sampling을 통해서 빈도수가 적은 단어도 잘 학습할 수 있다.
    2.6 https://arxiv.org/abs/1310.4546
  
  3. RNN을 학습을 잘하자
    3.1 RNN은 시간에 따라서 지난번 출력값이 이번 레이어로 input이 되어서 output에 영향을 주는 신경망이다.
    3.2 이때 아주 오래전 layer부터 입력을 받기 때문에 gradient가 vanishing되거나 exploding되는 경우가 빈번하다.
    3.3 이를 억제하기 위해 exploded gradient에 normalization을 하거나
      3.3.1 미분값이 exploding할 때, cuvature를 포함한 그보다 고차원 order의 derivative들 역시 exploding한다 가정을 가지고 찾아낸다.
      3.3.2 threshold를 정하고 그 값보다 클때 graident를 threshold/abs(g)*g 로 변경한다.
    3.4 vanished gradient에 regularization을 적용한다.
      3.4.1 jacobian matrix로 regularization을 적용한다.
    3.5 https://arxiv.org/abs/1211.5063
