180626
  1. Training Very Deep Networks - arXiv:1507.06228
  2. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift - arXiv:1502.03167
  3. Fractional Max-Pooling - arXiv:1412.6071
  4. A Neural Algorithm of Artistic Style - arXiv:1508.06576
  
180627
  5. Demystifying Neural Style Transfer - arXiv:1701.01036
  6. Image Style Transfer Using Convolutional Neural Networks
  7. On the importance of initialization and momentum in deep learning
  8. Deeply-Supervised Nets - arXiv:1409.5185
  9. Loss Functions for Discriminative Training of Energy-Based Models.
  
180628
  10. Preserving Color in Neural Artistic Style Transfer - arXiv:1606.05897
  11. FitNets: Hints for Thin Deep Nets - arXiv:1412.6550
  12. Deep learning via Hessian-free optimization
  13. k-Sparse Autoencoders - arXiv:1312.5663
  14. Speeding up Convolutional Neural Networks with Low Rank Expansions - arXiv:1405.3866

180629
  15. Constrained Neural Style Transfer for Decorated Logo Generation - https://arxiv.org/abs/1803.00686
  16. Distributed Representations of Words and Phrases and their Compositionality - https://arxiv.org/abs/1310.4546
  17. On the difficulty of training Recurrent Neural Networks - https://arxiv.org/abs/1211.5063
  
180701 -180703
  18. Xception: Deep Learning with Depthwise Separable Convolutions - https://arxiv.org/abs/1610.02357
  19. UCNN: Exploiting Computational Reuse in Deep Neural Networks via Weight Repetition - https://arxiv.org/abs/1804.06508
  20. Rich feature hierarchies for accurate object detection and semantic segmentation - https://arxiv.org/abs/1311.2524
  
180705
  21. Network In Network - https://arxiv.org/abs/1312.4400
  
180706 
  22. Deep filter banks for texture recognition and segmentation
  23. ImageNet Classification with Deep Convolutional Neural Networks
  
180707
  24. Efficient Estimation of Word Representations in Vector Space - https://arxiv.org/abs/1301.3781
  25. Distilling the Knowledge in a Neural Network - https://arxiv.org/abs/1503.02531?context=cs
  
180708
  26. Diagnosing Error in Object Detectors
  27. Recognition using regions
  28. Fast R-CNN
  
180710
  29. Improving the Fisher Kernel for Large-Scale Image Classification
  
180711
  30. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation - https://arxiv.org/abs/1406.1078
  31. Maxout Networks - https://arxiv.org/abs/1302.4389
  
180713  
  32. Going Deeper with Convolutions
  33. Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition - Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition

180714
  34. Recurrent Continuous Translation Models

180715
  35. U-Net: Convolutional Networks for Biomedical Image Segmentation - https://arxiv.org/abs/1505.04597
 
180716
  36. Exploiting generative models In discriminative classifiers
  37. Gated Feedback Recurrent Neural Networks - https://arxiv.org/abs/1502.02367
  38. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling - https://arxiv.org/abs/1412.3555
  39. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection
  
180717
  40. Deep Sparse Rectifier Neural Networks
  41. An Analysis of Single-Layer Networks in Unsupervised Feature Learning
  42. A Clockwork RNN - https://arxiv.org/abs/1402.3511
  43. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning
  
180718
  44. Recurrent Neural Network Regularization - https://arxiv.org/abs/1409.2329
  45. Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions
  46. Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks - https://arxiv.org/abs/1701.05923
  47. Minimal Gated Unit for Recurrent Neural Networks - https://arxiv.org/abs/1603.09420
  48. Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning - https://arxiv.org/abs/1602.07261
  
180719
  49. SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient - https://arxiv.org/abs/1609.05473
  50. Dropout improves Recurrent Neural Networks for Handwriting Recognition - https://arxiv.org/abs/1312.4569
  51. Deep Residual Learning for Image Recognition - https://arxiv.org/abs/1512.03385
  
180720
  52. An Empirical Exploration of Recurrent Network Architectures
  53. Spatial sign preprocessing: a simple way to impart moderate robustness to multivariate estimators.
  54. Rethinking the Inception Architecture for Computer Vision - https://arxiv.org/abs/1512.00567
  
180721
  55. BLEU: a Method for Automatic Evaluation of Machine Translation

180722
  56. Object Detection Networks on Convolutional Feature Maps - https://arxiv.org/abs/1504.06066
  57. Recurrent Convolutional Neural Networks for Text Classification
  58. Extracting and Composing Robust Features with Denoising Autoencoders
  59. Recurrent Residual Learning for Sequence Classification
  60. Residual LSTM: Design of a Deep Recurrent Architecture for Distant Speech Recognition - https://arxiv.org/abs/1701.03360
  61. How to Construct Deep Recurrent Neural Networks - https://arxiv.org/abs/1312.6026

180724
  62. Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems -1 https://arxiv.org/abs/1508.01745
  63. Depth-Gated LSTM - https://arxiv.org/abs/1508.03790
  64. Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks - https://arxiv.org/abs/1506.03099
  65. Recurrent Convolutional Neural Networks for Discourse Compositionality - https://arxiv.org/abs/1306.3584

180725
  66. Learning Longer Memory in Recurrent Neural Networks - https://arxiv.org/abs/1412.7753
  67. Extensions of recurrent neural network language model
  68. Donâ€™t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors
  69. A Simple Way to Initialize Recurrent Networks of Rectified Linear Units - https://arxiv.org/abs/1504.00941
  70. Autoencoder Regularized Network For Driving Style Representation Learning - https://arxiv.org/abs/1701.01272
  71. Efficient Neural Architecture Search via Parameter Sharing - https://arxiv.org/abs/1802.03268

180726
  72. Grid Long Short-Term Memory - https://arxiv.org/abs/1507.01526
  73. word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method - https://arxiv.org/abs/1402.3722
  74. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size - https://arxiv.org/abs/1602.07360
  75. Highway Long Short-Term Memory RNNs for Distant Speech Recognition - https://arxiv.org/abs/1510.08983
  76. Shake-Shake regularization - https://arxiv.org/abs/1705.07485
  77. Dynamic Evaluation of Neural Sequence Models - https://arxiv.org/abs/1709.07432
  
180727  
  78. Attention Is All You Need - https://arxiv.org/abs/1706.03762
  79. Recurrent neural network based language model
  80. Tracking Emerges by Colorizing Videos - https://arxiv.org/abs/1806.09594
  81. Fast-Slow Recurrent Neural Networks - https://arxiv.org/abs/1705.08639

180730
  82. FractalNet: Ultra-Deep Neural Networks without Residuals - https://arxiv.org/abs/1605.07648
  83. Multiplicative LSTM for sequence modelling - https://arxiv.org/abs/1609.07959
  84. Sequence to Sequence Learning with Neural Networks - https://arxiv.org/abs/1409.3215
  85. Training Deep Bidirectional LSTM Acoustic Model for LVCSR by a Context-Sensitive-Chunk BPTT Approach
  86. Using the Output Embedding to Improve Language Models - https://arxiv.org/abs/1608.05859

180731
  87. Factorization tricks for LSTM networks - https://arxiv.org/abs/1703.10722
  88. Layer Normalization - https://arxiv.org/abs/1607.06450
  89. Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations - https://arxiv.org/abs/1606.01305
  90. A Structured Self-attentive Sentence Embedding - https://arxiv.org/abs/1703.03130
  91. Recurrent Highway Networks - https://arxiv.org/abs/1607.03474

180801
  92. Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning - https://arxiv.org/abs/1506.02142
  93. Aggregated Residual Transformations for Deep Neural Networks - https://arxiv.org/abs/1611.05431
  94. Not All Contexts Are Created Equal: Better Word Representations with Variable Attention
  95. Regularizing and Optimizing LSTM Language Models - https://arxiv.org/abs/1708.02182

180802
  96. A Theoretically Grounded Application of Dropout in Recurrent Neural Networks - https://arxiv.org/abs/1512.05287
  97. MEMORY NETWORKS - https://arxiv.org/abs/1410.3916
  98. Swapout: Learning an ensemble of deep architectures - https://arxiv.org/abs/1605.06465
  99. Recurrent Dropout without Memory Loss - https://arxiv.org/abs/1603.05118
  100. You Only Look Once: Unified, Real-Time Object Detection - https://arxiv.org/abs/1506.02640


