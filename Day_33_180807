1. dropout을 이제는 node가 아닌 block에 쓰이는 시대
  1.1 이제는 하나의 node를 drop시키는 것이 아니라 blcok을 drop시켜서 regularization을 시키는것 같다.
  1.2 이 논문의 CNN model 의 structure를 imporve시켜서 feature map을 더 잘 extract하는 것이 목표인 논문읻.
  1.3 fractal network와 거의 유사한 형태를 나타내는 모델
    1.3.1 유사한 모델이 fractal의 형태로 쪼개져서 자기가 복제되는 형태와 같이 나타나는 structure
  1.4 이 논문에서는 deep한 CNN보다 wide한 CNN을 선호하는데, depp하면 성능이 좋아지기는 하지만 일정깊이 이상되면 모델의 깊이보다 filter의 수가 더 성능에 직접적인 영향을 주는 논문들이 많아 졌기 떄문으로 생각된다.
  1.5 ResNet이나, ResNeXt나 fractal net에서 볼수 있듯이 모델의 깊이보다 parallel한 방향의 폭이 더 성능에 큰 영향을 주는 것을 확인 할 수 있다.
  1.6 multi-Residual Net은 Residual connection이 계속 연결된 블록을 Fractal 처럼 연결하되 함수를 다르게 구성하는 것 '하지만 본 논문에서는 함수가 모두 같은 블록을 사용한것 같긴함' 에 fractal net과 차이가 있다.
  1.7 그리고 각 함수 블록을 stochastic depth의 방식을 적용시켜서 drop시킴으로써 정규화 시킬 수 있는데, 이 방법이 적용되었는지는 명확하게 명시 되지 않은것 같다.
  1.8 아마 안되있는것 같은데, 적용시켜서 학습을 하면 모델의 robustness가 더욱 향상 될것으로 생각된다.
  1.9 이 논문은 결국 하나의 CNN module block을 개선시켜서 더 계산복잡도와 계산비용을 줄이고, multi core process에서 더 잘 작동하는 module block을 제안하는 논문으로, YOLO나 neural style이나 object detection과는 다른 목적을 가지고 있는 논문이다.
  1.10 그래서 결국 parallel하게 연결된 모듈들과 stochastic하게 drop되어 있는 connection에 의해서 하나의 모델로 다양한 ensemble효과를 내는 모델을 학습 할 수 있게된다.
  1.11 https://arxiv.org/abs/1609.05672
