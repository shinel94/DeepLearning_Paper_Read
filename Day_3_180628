논문 읽기 3일차 벌써 목요일이다 빠생
A. Neural Style
  1. Neural Style Transfer를 이용했을때 color를 preseve하는 방법
    1.1 기존의 Neural Style algorithm을 사용하면 생성된 이미지의 color가 content를 따르는 것이 아닌 style을 따르게 된다.
    1.2 color histogram matching 이나 luminance-only transfer를 통해서 color를 보존하는 transfer를 진행할 수있다.
    1.3 Color Histogram Matching - Style Input을 변환
      1.3.1 Style의 color분포를 content의 color와 match를 시킴으로써 새로운 Style인풋으로 Neural Style을 학습한다.
        1.3.1.1 이때 style의 컬러 분포와 content의 컬러 분포를 일치시킬때는 각각의 평균과 분산을 비교한다.
      1.3.2 style의 컬러를 변환 시킬때는 affine변환을 통해서 변화시킨다.
        1.3.2.1 S' = A*S+b
      1.3.3 이미지가 RGB 3차원 데이터를 가지므로 평균은 3차원 벡터 분산은 3X3 행렬이 되고 그리고 affine변환을 위한 A와 b를 계산한다.
        1.3.3.1 b = mean(content) - A*mean(style)
        1.3.3.2 A = std(content)*std(style)^-1
      1.3.4 이미지 변환에서 이미지 channel의 order가 result에 영향을 미친다.
    1.4 Color Historgram Matching - 생성된 이미지를 추가적으로 변환
      1.4.1 생성된 이미지에 추가적으로 위의 방식과 같게 color를 Content 인풋과 matching을 시킨다.
      1.4.2 좀더 좋은 성능을 보인다.
      1.4.3 동시에(simultaneously) matching을 할 필요가 없어져서 경쟁이 줄어 들었다.
    1.5 Luminance-only Transfer
      1.5.1 Style Transfer에 사용하는 Style인풋에는 오직 Luminance channel만 사용한다.
        1.5.1.1 이는 visual perception에서 luminance만 이용한것이 color보다 성능을 좋은 것에서 기반했다.
      1.5.2 Style과 Content 둘 모두에서 Luminance data만 가진 data를 extract한뒤 luminance output image를 얻어 낸다.
      1.5.3 YIQ color space를 사용하면 luminance를 color space로 변경할 수 있다.
      1.5.4 혹시 이미지가 잘 매칭이 되지 않는다면 output luminance image를 std(Lc)/std(Ls)*(Ls-mean(Ls))+mean(Lc)를 적용해 새로운 output을 생성하면 좀 잘 맞다.
    1.6 결국에 스타일 transfer전에 style input에 color transfer를 적용하는 방법이나
    1.7 오직 luminance channel로만 학습을 진행하는 방법 2가지 방법을 이용하면 color preserve가 가능하다.
    1.8 전자의 방법에는 어떻게 content의 이미지를 style로 전가 시키는데 어려움이 있다.
    1.9 후자의 방법에는 luminance 데이터를 이용하기 때문에 그림의 데이터 손실이 일부분 발생한다.
    1.10 https://arxiv.org/abs/1606.05897
    
B. Deep Neural Network
  1. 얇은 deep neural network
    1.1 신경망은 깊이가 깊어질수록 성능이 증가 하지만, gradient-based training은 non-linearlity가 커지면서 학습이 잘 되지 않는다.
    1.2 FitNets이라는 방법을 이용해서 thin and depp network를 학습 할 수있다.Distilling the Knowledge in a Neural Network
    1.3 Teacher Network와 Student Network로 구성되어 있다.
    1.4 loss function이 실제랑 비교하는 term 과 teacher로 부터 파생된 penalized term 2가지가 있다.
    1.5 Teacher 가 Student에 주는 hint가 결국 regularizer로서 역할하게 된다.
    1.6 실제로 classification을 진행하는 student network를 학습하고 그리고 이를 regularize시키는 teacher network를 학습시킨다.
    1.7 teacher network를 학습할때는 guide와 hint 두가지 를 통해 학습이 진행된다.
    1.8 잘구성된 structure를 이용해 teacher를 학습 시킨다.
    1.9 학습된 teacher structure를 이용해서 그것보다 얇은 student structure를 학습 시킨다.
    1.10 적은 파라미터로도 State Of The Art의 performance가 나온다
    1.11 teacher structure를 이용해서 teacher structure 보다 얇아 parameter수가 획기적으로 줄어든 student structure를 학습 시킨다.
    1.12 이때 teacher structure 가 student structrue로 압축 되는 것으로 볼 수있고, 이것이 regularizer로 작용한다.
    1.13 결국에 teacher structure에서 studnet struucture가 학습되고 성능도 좋은 청출어람을 볼 수있다.
    1.14 https://arxiv.org/abs/1412.6550
  2. pre-training없이 효과적인 학습하기
    2.1 Hessian Free optimization을 사용하면 사전학습과 pine 튜닝이 진행되는 모델보다 성능이 좋다.
    2.2 pre-training은 under fitting을 막을 수는 없다.
    2.3 curvature를 계산 (2nd order derivative를 계산)해서 gradient의 방향과 크기를 계산할 수 있고 이를 통해서 학습이 좀더 빨리 진행된다.
    2.4 2nd order를 적용하는 방법은 CG, NCG, or truncated-Newton, Gauss-Newton approximation등으로 진행할 수 있다.
    2.5 2nd order optimization은 1st order optimization과 달리 사전학습이 없어도, fine tuning이 없어도 학습이 잘된다. 성능역시 보다 좋다는 것을 확인 할 수 있다.
    2.6 http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf
  3. k - sparse autoencoder
    3.1 hidden layer에서 k개의 가장 activities가 높은 가중치만 남겨놓고 나머지는 무시한다
    3.2 성능이 다른 state of the art와 비교해서 아주 우수하고 학습또한 쉽다.
    3.3 기존의 방법들은 계산비용이 너무나 크다.
    3.4 selection k 빼고는 전부다 linear 가되게 auto encoder를 구성
    3.5 selection step이 regularizer로 작동 (drop connect랑 비슷한듯)
    3.6 weight값들을 0으로 만드는 것이 아닌 hidden layer의 값들 z = W*x + b 에서 z값들이 k 개의 좋은거 빼고 나머지가 0 으로 바뀜
    3.7 느낌이 max pooling이랑 비슷한듯.
    3.8 활성화된 곳으로만 역전파가 이루어짐
    3.9 활성화된 뉴런은 Iteratuve Thresholding with Inversion(ITI)로 계산됨
    3.10 localized gabor filter랑 비슷하게 작동함
    3.11 다른 신경망에서 initialize 전략으로 사용 할수 있다.
    3.12 그래서 다른 autoencoder뿐만 아니라 다른 다양한 신경망에 k-sparse 전략을 사용 할 수 있을 것이다.
    3.14 https://arxiv.org/abs/1312.5663
  4. low rank cross channel decomposition으로 CNN의 학습속도를 drastically증진시킬 수 있다.
    4.1 CNN은 본질적으로 다양한 필터를 이용해서 다양한 특징들을 추출해내기 때문에 어쩔수 없이 parameter의 숫자가 너무나 많다.
    4.2 CNN의 parameter숫자를 줄임으로써 계산의 complexity와 cost를 줄일 수 있다.
    4.3 하나의 2차원 필터를 2개의 1차원 필터의 곱으로 표현해서 1차원 필터 2개로 decompose해서 계산량을 줄일수 있다.
    4.4 full rank original convolutional filter bank들은 can be decomposed in to a linear combination of a set of separable basis filter
    4.5 각 레이어별로 decompose를 다르게 적용해서 속도의 증가량과 손실되는 accuracy들을 조절할 수 있다.
    4.6 https://arxiv.org/abs/1405.3866
