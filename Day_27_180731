1. Factorization tricks for LSTM network
  1.1 LSTM모델의 하나의 큰 weight matrix를 2개의 작은 matrix의 product로 만드는 fatorization이나
  1.2 LSTM모델을 partitioning 하는 방법을 통해서 parameter수를 drastically 줄이고 성능은 유지하는 LSTM을 만들 수 있다.
  1.3 RNN과 더불어서 LSTM이 많은 language model들에 적용되고 있지만, 대부분 모델들의 파라미터 숫자가 너무 많아 학습에 오랜 시간 걸린다는 문제점이 있다.
  1.4 RNN의 한계를 개선하기 위해서 LSTM이 제안 되었는데 RNN에 비해서 LSTM의 parameter가 너무 많아 이를 줄이는데 focus를 맞추었다.
  1.5 Factorization은 cascade한 2개의 가중치 matrix로 하나의 큰 matrix를 쪼개는 것
    1.5.1 inception model에서 제안된 5x5를 3x3 2개로 나누거나 3x3 을 1x3과 3x1로 쪼개는 것과 같은 방식으로 진행됨
    1.5.2 LSTM은 입력벡터2개 2*p와 4개의 gate에 대한 가중치 4*n 으로 구성 된 것을 변형시킨다.
    1.5.3 (2*p x 4*n) -> (2*p x W) * (W x 4*n) 으로 바뀌면서 가중치를 줄인다
  1.6 Group LSTM은 입력 sequence 자체를 chunk로 쪼개서 LSTM을 넣어서 학습을 진행시키는 방식
    1.6.1 다른 논문에서 제시되었듯이 chunk간 interdependecy가 무시되는 경향이 나타 날 수도 있겠지만 해당 내용은 논문에서 제시되지 않았음
    1.6.2 parallel하게 입력 데이터에 대한 하나의 모델을 여러개의 작은 모델로 나누어서 학습하는 것으로 볼 수 있다. 
  1.7 같은 iteration에서는 성능이 parameter가 많은게 좋은것을 확인 할 수 있지만, 같은 시간대에 학습한 성능을 비교하면 parameter reduction을 시킨 model의 성능이 더욱 좋다.
  1.8 Group LSTM의 경우 2층 stack이 되어있을때 1층에는 4개의 그룹으로 2층에는 8개의 그룹으로 와 같은 식으로 hierarchical하게 만들 수도 있다.
  1.9 LSTM을 factorization 시키거나 작은 LSTM으로 나누어서 학습 할 수 있게 데이터를 group화 시켜서 parameter수를 줄이면서 성능을 개선 시킬 수 있다.
  1.10 http://arxiv.org/abs/1703.10722
