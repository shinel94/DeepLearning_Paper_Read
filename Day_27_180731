1. Factorization tricks for LSTM network
  1.1 LSTM모델의 하나의 큰 weight matrix를 2개의 작은 matrix의 product로 만드는 fatorization이나
  1.2 LSTM모델을 partitioning 하는 방법을 통해서 parameter수를 drastically 줄이고 성능은 유지하는 LSTM을 만들 수 있다.
  1.3 RNN과 더불어서 LSTM이 많은 language model들에 적용되고 있지만, 대부분 모델들의 파라미터 숫자가 너무 많아 학습에 오랜 시간 걸린다는 문제점이 있다.
  1.4 RNN의 한계를 개선하기 위해서 LSTM이 제안 되었는데 RNN에 비해서 LSTM의 parameter가 너무 많아 이를 줄이는데 focus를 맞추었다.
  1.5 Factorization은 cascade한 2개의 가중치 matrix로 하나의 큰 matrix를 쪼개는 것
    1.5.1 inception model에서 제안된 5x5를 3x3 2개로 나누거나 3x3 을 1x3과 3x1로 쪼개는 것과 같은 방식으로 진행됨
    1.5.2 LSTM은 입력벡터2개 2*p와 4개의 gate에 대한 가중치 4*n 으로 구성 된 것을 변형시킨다.
    1.5.3 (2*p x 4*n) -> (2*p x W) * (W x 4*n) 으로 바뀌면서 가중치를 줄인다
  1.6 Group LSTM은 입력 sequence 자체를 chunk로 쪼개서 LSTM을 넣어서 학습을 진행시키는 방식
    1.6.1 다른 논문에서 제시되었듯이 chunk간 interdependecy가 무시되는 경향이 나타 날 수도 있겠지만 해당 내용은 논문에서 제시되지 않았음
    1.6.2 parallel하게 입력 데이터에 대한 하나의 모델을 여러개의 작은 모델로 나누어서 학습하는 것으로 볼 수 있다. 
  1.7 같은 iteration에서는 성능이 parameter가 많은게 좋은것을 확인 할 수 있지만, 같은 시간대에 학습한 성능을 비교하면 parameter reduction을 시킨 model의 성능이 더욱 좋다.
  1.8 Group LSTM의 경우 2층 stack이 되어있을때 1층에는 4개의 그룹으로 2층에는 8개의 그룹으로 와 같은 식으로 hierarchical하게 만들 수도 있다.
  1.9 LSTM을 factorization 시키거나 작은 LSTM으로 나누어서 학습 할 수 있게 데이터를 group화 시켜서 parameter수를 줄이면서 성능을 개선 시킬 수 있다.
  1.10 http://arxiv.org/abs/1703.10722

2. Batch Normalization 을 RNN에도 써보자
  2.1 학습 속도를 증진시키기 위해서 Batch Normalization이라는 방식이 제안 되었다.
    2.1.1 BN은 각 activation function으로 들어가기 전 feature map에 적용되는 방식으로, cell의 값을 -1과 1사이의 값을 가지는 z값으로 정규 분포화 시킨뒤 그 값에 scale fator와 shift factor를 곱해주어서 값을 보정해서 일반적인 whitening 보다 강한 표현력을 가지고 있다.
      2.1.1.1 BN의 변수는 scale factor와 shift factor로 batch마다 한쌍의 factors들이 할당 된다.
    2.1.2 CNN에 BN을 적용시키는 방법은 BN이 activation function을 통과하기 전에 적용시키는 방식이기 때문에, CNN의 합성곱이 일어나고 난 feature map의 각 cell에 할당되는 scale factor와 shift factor가 정해진다.
      2.1.2.1 쉽게 말해서 filter 와 곱해지는 입력 data part를 하나의 batch로 보고 normalization이 일어나느 것으로 볼 수 있다.
  2.2 RNN은 batch size가 fixed 되어 있지 않기 떄문에, BN을 적용시키기가 힘들다.
  2.3 이를 위해서 layer normalization이 제안 되었다.
  2.4 covariate shift문제를 해결하기 위해서 제안 되었다
    2.4.1 non linear activation에 들어가기 전에 summation되기 전에 적용되어서 covariate shift문제를 완화 시킬 수 있다.
  2.5 layer nomalizaton은 batch가 아닌 하나의 hidden layer에 적용해서 normalization을 적용시킨다.
  2.6 scale factor와 shift factor를 모든 시간대의 입력에대해서 공유한다.
  2.7 vanila RNN의 경우 hidden state가 결정되기 직전에 normalization이 진행된다.
  2.8 weight normalization에는 하나의 벡터만 normalization될 때는 영향을 받지만 entire하게 matrix를 normalization할때는 영향을 받지 않는다 그리고 batch normalization에는 영향을 받는것으로 볼 수 있다.
  2.9 Layer별로 Normalization을 진행하면 FC나 CNN에 적용된 Batch Normalization이 가지고 있는 큰 장점인 학습속도가 증진되는 효과, 학습률을 증가시켜도 안정된 gradient를 보인다는 효과를 얻을 수 있다.
  2.10 https://arxiv.org/abs/1607.06450
  
3. 정규화 하자 정규화 RNN에 ZoneOut
  3.1 많은 regularizaing technique가 사용되고 발전되고 있다.
    3.1.1 early stopping이나 reccurent를 제외한 곳에 dropout을 적용하거나 또 batch normalizaion이나 layer normalizaion들이 제안되었고 성능을 개선 시켰다.
  3.2 zoneout이라는 novel regularizaing method 가 제안되었다.
  3.3 dropout은 해당 node의 값을 drop시키는 것인데 간단하게 말하면 특정 node의 값을 previous timestep의 값으로 대체하는 것을 의미한다.
  3.4 이 방법을 통해서 dropout과 달리 vanishing gradient 문제에 보다 잘 대처 할 수 있다.
  3.5 최근까지 제안된 RNN은 recurrent connection이 아닌 feed-forward connection에만 dropout이 적용되었다.
  3.6 또 더 최근 논문에는 memory cell이나 GRU state에 dropout이 적용되는 식으로 dropout이 개선되고 있다.
    3.6.1 input output gate에 dropout이 적용되고 있다.
  3.7 LSTM과 GRU와 같이 gate를 가지고 있는 곳에서만 time step으로 dropout을 적용시킬 수 있다.
  3.8 zoneout은 어떤 RNN에도 상관 없이 적용시킬 수 있고, 지난번 값을 이용하는 것이기 때문에, 학습할때도 gradient가 vanishing 되는것을 막을 수 있다.
  3.9 zoneout을 per-unit version of stochastic depth로 볼 수 있다. 
  3.10 zoneout을 적용하면 왜곡된 데이터에 보다 강한 robustness가 보다 강해지는 것을 확인 할 수 있었다.
  3.11 zoneout은 다른 batch normalization전략과 feed-forward connection에 dropout을 적용하는 것과 동시에 적용할 수 있공 성능역시 향상되는 것을 볼 수 있다.
  3.12 hidden state의 변화를 입력에 대해 조금더 robust하게 network를 학습 시킬 수 있고, flow of information도 imporve 시킬 수 있다.
  3.13 https://arxiv.org/abs/1606.01305

4. Self attention sentence embedding
  4.1 짧은 text data를 가지고 classification을 하기위해서 CNN이나 LSTM과 같은 방식이 제안 되었다.
  4.2 그리고 attention mechanism 역시 제안 되었다
    4.2.1 데이터에 추가적인 attention에 대한 정보를 가지고 있어서, 집중해야할 정보에 대한 데이터를 input에 가지고 있는 model
  4.3 하지만 sentence를 나타내는 data들은 이런 attention information을 가지고 있기가 쉽지 않다.
  4.4 이를 개선하기 위해서 self attention이 제안 되었다.
  4.5 우선 모델은 LSTM을 통해서 전체 문장을 embedding vector로 표현을 하고, 하나의 vector에서 값을 추려내서 사용하는 것이 아니라 전체 embedding vector를 matrix로 표현해서 해당 matrix를 한번에 처리한다.
  4.6 이때 H matrix와 gate matrix와 같은 structure를 가진 A weight metrix를 H를 기반으로 학습해서 self attention이 진행된다.
  4.7 그리고 이 A와 H의 곱을 통해서 얻은 feature vector를 이용해서 사용자 classification이나 star point prediction에 사용한다면 좋은 성능을 보여준다.
  4.8 그리고 이 A matrix에 추가적인 penalty가 추가 되어서 regularizer로서 효과를 나타낼 수 있다.
    4.8.1 A*A^T가 identity matrix에 가깝게 학습하게 되면, A가 같은 것끼리는 같게 다른것들끼리는 다르게 embedding하는 효과를 얻을 수 있다.
  4.9 hidden state 값을 pooling해서 embedding하는 것이 아닌 hidden state map을 self attention으로 더 많은 정보를 효율적으로 embedding시킴으로써 단순한 pooling보다 성능이 좋다.
  4.10 모든 hidden state 값을 한번에 사용하기 때문에, long term dependecy나 단어간 interdependecy를 더 잘 catch할 수 있다.
  4.11 그리고 출력 벡터의 width뿐 아니라 height가 증가되면 데이터의 표현력도 커지지만 일정크기 이상으로는 성능이 좋아지지 않는다.
  4.12 LSTM이나 RNN 또 GRU 그리고 bidirectional LSTM 들의 특정 timestep에 대한 값에 따로따로 pooling을 진행하는 것 보다 전체 hidden state map을 구하고 이를 통해서 self-attention을 적용하게 되면 보다 embedding vector를 잘 표현 할 수 있다.
  4.13 https://arxiv.org/abs/1703.03130

5. Recurrent Highway Network
  5.1 신경망 model은 layer가 깊어질수록 좋은 성능을 내는 것으로 알려져 있다.
  5.2 RNN은 time axis로는 deep하게 연결되어 있는것으로 생각 할 수 있다.
  5.3 하지만 신경망의 depth가 커질수록 error가 backpropagation이 잘 되지 않는 문제를 확인 할 수 있다.
  5.4 그래서 RNN에서도 모든 시간입력에 대해서 error를 계산해서 전파하는 것이 아닌 특정 시간대별로 주기별로 잘라서 error를 전파하는 방식으로 depth를 조절한다.
  5.5 그리고 추가적으로 다양한 method들이 제안 되었다.
    5.5.1 간단하게 memory cell을 통해서 그리고 self attention을 할 수 있는 gate를 통해서 error가 보다 잘 흐르게 모델을 만들거나
    5.5.2 skip connection을 줄여서 error가 돌아올 path의 길이를 줄이는 방법도 제안되었다.
    5.5.3 그리고 bidirection 으로 에러를 양방향으로 학습을 진행 하거나
    5.5.4 또는 RNN이 아닌 model에서는 auxiliary error를 전파하는 식으로 error를 더 잘 전파 시킬 수 있다.
    5.5.5 Fast-Slow RNN이나 clockwork-RNN 또는 Depth-gate LSTM과 같은 다양한 model이 제안되고 있다.
  5.6 gradient의 문제를 해결하기 위해서 Highway layer를 가지고 있는 Recurrent Highway Network가 제안되었다.
  5.7 간단한 Stacked RNN의 경우 model의 구조가 결국 Time에 의한 depth T와 feed-forward depth D가 곱해진 T*D개의 cell을 가지게 된다.
  5.8 시간축으로 전파될때 사용되는 가중치 R에따라서 gradient가 vanishing될지 exploding할지 정해지는데 이때 R의 eigenvalue의 크기로 이를 예측할 수 있다.
  5.9 Highway Network와 같은 방식으로 학습이 진행된다
    5.9.1 이때 이번 입력으로 들어오는 값은 highway network와 같은 방식으로 gate mechanism으로 학습되고 해당시간대 출력이 결정된다. 이때 gate들에서는 이번 cell의 모든 입력을 이용해서 gate를 학습한다.
  5.10 GRU와 차이를 잘 모르겠음, 지난번 출력에 대해서 사용할 데이터를 고르기 위한 gate와 이번 입력에서 고르기 위한 gate 2개를 학습하는데 결국 GRU와 같은 형식이 아닌가
  5.11 결국 Gate를 학습함으로써 깊은 recurrent neural network를 학습 시킬 수 있고, 효율도 좋아진다.
  5.12 https://arxiv.org/abs/1607.03474
  
  
  
    
    


