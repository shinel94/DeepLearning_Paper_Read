1. LSTM을 이용한 Natural Language Generator
  1.1 LSTM모델에 utterance와 같은 추가적인 정보를 저장하고 있는 새로운 structure를 가지고 있다.
  1.2 처음에는 onehot encoding data를 이용해서 d vector를 설정하고 reading gate를 통해서 필요한 데이터만 읽어서 LSTM의 memory cell로 보낸다.
  1.3 이것 제외하면 보통의 LSTM과 같은 구조를 가지고 있다.
  1.4 LSTM을 수평되게 한층 더 쌓아서 다음 시간 t+1 로 넘어가기 전에 한번더 LSTM을 거치고 여기에서 Skip connection을 사용해서 gradient vanishing문제를 조금 해소 했다.
  1.5 reccurent connection이 아닌 connection에서는 dropout을 적용해서 regularizer로서 역할 하게 했다.
  1.6 이 forward 모델을 통해서 semantic한 의미를 얻어 낸다.
  1.7 압축한 데이터를 기준으로 새로운 rerangker 모델을 하나더 학습한다.
  1.8 들어가는 word vector들은 다른 모델을 통해서 embbeding된 벡터를 사용한다.
  1.9 sementic controlled LSTM을 사용하면 문장을 generating하는 모델을 잘 만들 수 있다
  1.10 학습은 처음 주어진 조건에 대한 one hot ecoding data와 문장에 대한 embbeding vector 2가지가 들어가서 학습이 진행됨
  1.11 structure는 기본적인 LSTM에 추가적인 sentence planning data를 담고있는 one hot encoding 벡터를 처리하는 부분이 있고 여기서 reading gate를 통과해서 일반적인 LSTM과 달리 memory cell에 추가적인 데이터가 들어간다.
  1.12 forward를 학습을 하고 난뒤 그 모델을 이용해서 backward를 학습해서 generator를 학습시킨다.
  1.13 https://arxiv.org/abs/1508.01745
  
2. Depth Gated LSTM
  2.1 main idea는 memory나 node의 데이터간 connection을 만들고, flow를 control하는 gate도 학습해서 데이터 flow를 잘 조절하게 하자
  2.2 Highway network와 기본적으로 개념은 같다.
  2.3 Depth Gated는 LSTM의 memory cell의 데이터를 layer방향으로 전파시켜 보자는거에서 출발
  2.4 이 gate를 통해서 memory cell에도 layer방향으로 error가 잘 전파되는 것을 확인 할 수 있었다.
  2.5 GRU에는 memory cell이 없이 좋은 성능을 내는 것도 memory cell이 time 방향으로만 connection이 있기 때문일 수도 있겠다.
  2.6 LSTM의 구조는 크게 바뀌지 않고 memory cell간 연결을 layer방향 depth방향으로 connection gate를 가지는 structure를 제안
  2.7 성능이 비교적 잘나옴 하지만 Degradation문제가 나타나는 것을 확인 할 수 있었다.
  2.8 https://arxiv.org/abs/1508.03790

3. 데이터를 누락시켜서 학습을 진행하자
  3.1 RNN에서 데이터를 학습할때, mini batch를 통해서 학습이 진행된다면
  3.2 입력된 데이터들이 통과해서 나온 hidden node의 값을 모두 다음레이어로 전파하는 것이 아니라 sampling을 통해서 일부분만 전파를 시킴
  3.3 scheduled sampling을 사용하면 다른 방법들과 함께 사용해서 성능을 개선하 수 있다.
  3.4 모든 데이터가 중요한것은 실제와 조금 다르기 때문에, sampling을 통해서 중요한 데이터만 다음 레이어로 넘기는 식으로 성능을 개선한다.
  3.5 https://arxiv.org/abs/1506.03099
