1. LSTM을 이용한 Natural Language Generator
  1.1 LSTM모델에 utterance와 같은 추가적인 정보를 저장하고 있는 새로운 structure를 가지고 있다.
  1.2 처음에는 onehot encoding data를 이용해서 d vector를 설정하고 reading gate를 통해서 필요한 데이터만 읽어서 LSTM의 memory cell로 보낸다.
  1.3 이것 제외하면 보통의 LSTM과 같은 구조를 가지고 있다.
  1.4 LSTM을 수평되게 한층 더 쌓아서 다음 시간 t+1 로 넘어가기 전에 한번더 LSTM을 거치고 여기에서 Skip connection을 사용해서 gradient vanishing문제를 조금 해소 했다.
  1.5 reccurent connection이 아닌 connection에서는 dropout을 적용해서 regularizer로서 역할 하게 했다.
  1.6 이 forward 모델을 통해서 semantic한 의미를 얻어 낸다.
  1.7 압축한 데이터를 기준으로 새로운 rerangker 모델을 하나더 학습한다.
  1.8 들어가는 word vector들은 다른 모델을 통해서 embbeding된 벡터를 사용한다.
  1.9 sementic controlled LSTM을 사용하면 문장을 generating하는 모델을 잘 만들 수 있다
  1.10 학습은 처음 주어진 조건에 대한 one hot ecoding data와 문장에 대한 embbeding vector 2가지가 들어가서 학습이 진행됨
  1.11 structure는 기본적인 LSTM에 추가적인 sentence planning data를 담고있는 one hot encoding 벡터를 처리하는 부분이 있고 여기서 reading gate를 통과해서 일반적인 LSTM과 달리 memory cell에 추가적인 데이터가 들어간다.
  1.12 forward를 학습을 하고 난뒤 그 모델을 이용해서 backward를 학습해서 generator를 학습시킨다.
  1.13 https://arxiv.org/abs/1508.01745
  
