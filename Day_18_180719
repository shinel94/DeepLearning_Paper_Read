1. GAM에 RL 학습방법을 접목시킨 SeqGAN
  1.1 GAN은 두가지 문제점을 가지고 있다.
    1.1.1 처음 시작할때 완전히 임의의 인풋이 들어가기때문에, 인풋 데이터 간에 완전히 discrete되기 힘들다.
    1.1.2 Score나 Loss를 계산할때 모든 sequence가 generate되고 난뒤에 계산이 된다. -> 시계열 데이터에 적용 시키기 힘들다.
  1.2 이 두가지 문제를 해결하기 위해 강화학습 방법을 접목시킨다.
    1.2.1 Generative Model is treated as an agent of reinforcement learning
    1.2.2 The general token is treated as state
    1.2.3 task-specific sequence score, such as BLEU is treated as Reward
    1.2.4 Employe a Discriminator to evaluate the sequence and feedback the evaluation to guide the learning of the generative model.
  1.2 Policy Gradient로 학습을 진행했고
  1.3 입력값을 지나번 출력값과 concat하고 oracle evaluation mechanism을 사용 했다.
    1.3.1 oracle evaluation mechanism이 뭔지?
  1.4 이를 사용하면 text generation, music generation과 같은 sequence 데이터 생성에 잘 사용 할 수 있다.
  1.5 학습은 RL의 방식으로 recurrent를 적용해서 입력데이터를 얻어내는데 이를 oracle방식으로 생성해서 학습하면 짱짱
  1.6 https://arxiv.org/abs/1609.05473

2. GRU를 넘어선 MGU
  2.1 RNN신경망이 고안되었고, long term dependecy를 반영하기 위한 순환신경망으로 LSTM이 제안되었다.
  2.2 하지만 LSTM은 3개의 gate와 2개의 giddene state를 가지고 있기 떄문에 구조가 복잡하다.
  2.3 그래서 이를 gate 1개 hidden sate 1개로 줄인 GRU가 제안되었다.
  2.4 RNN의 성능은 gate수에 비례해서 나타나지 않다고 알려졌고, 심지어 gate가 더욱 많은 LSTM보다 GRU가 보다 좋은 성능을 보여주고 있다.
    2.4.1 하지만 이때 gate숫자에 비례해서 parameter도 증가해야하지만 model간 전체 parameter수를 통제해서 성능을 비교했다.
  2.5 그래서 이보다 gate와 state수를 줄인 MGU가 제안되었다.
  2.6 gate와 hidden state 수가 줄었기 때문에, 자연스레 파라미터 숫자역시 줄고 이로인해서 model complexity가 drastically 줄어든다.
  2.7 GRU 이전에 LSTM with coupled gates가 있었다.
    2.7.1 LSTM의 input gate(memory cell로 들어가는 input을 제어)가 1-forget gate로 대체된 신경망
  2.8 RNN의 gate unit은 성능에 큰 영향을 끼친다.
    2.8.1 이때 forget gate가 가장 중요하고 그다음이 input gate 그리고 output gate이다
      2.8.1.1 GRU와 LSTM을 보면 유추할 수 있다.
  2.9 forget gate is unanimously considered the most important one
  2.10 그래서 MRU는 GRU의 reset gate를 forget gate로 대신한다.
  2.11 파라미터가 더적은 MGU가 GRu보다 좋은 성능을 낸다.
  2.12 peephole connection이 없이 좋은 성능을 내는 MGU
  2.13 https://arxiv.org/abs/1603.09420
  
3. Dropout을 쓴 RNN으로 handwriting recognition하기
  3.1 RNN에는 아직 dropout이 적용되지 않았는데 recurrent connection에 영향을 주지않게 dropout을 적용했다.
  3.2 handwriting에는 텍스트의 선과 이미지가 sequence 입력으로 들어와서 recognization 된다.
  3.3 이때 최근에는 Hidden Matkov Model이 많이 사용됬다.
    3.3.1 하지만 이 모델은 first order Markov chain만 쓰기 때문에 long term dependency를 잡을수 없다.
    3.3.2 그리고 HMMs 들은 하나의 hidden state가 하나의 데이터만 가질수 있기때문에 log(n)bit의 데이터만 carry할 수 있다.
  3.4 하지만 RNN은 그런거 없다.
    3.4.1 RNN이 가지는 Recurrent Connection이 그것을 잘 처리해 준다.
  3.5 recurrent에 dropout을 적용하지 않고 feedforward방향으로만 dropout을 적용시켜서 RNN에도 recurrent connection에 영향을 주지않고 dropout을 적용시킬 수 있다.
  3.6 dropout이 classification model의 가중치에 L1 L2 weight decay regularization의 효과를 나타내는것으로 보여진다.
  3.7 LSTM의 가중치는 dropout과 함께 크기가 커지는 것으로 보여진다.
  3.8 overfitting이 눈에띄게 줄어드는것을 확인 할 수 있다.
  3.9 https://arxiv.org/abs/1312.4569
