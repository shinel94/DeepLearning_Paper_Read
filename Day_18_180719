1. GAM에 RL 학습방법을 접목시킨 SeqGAN
  1.1 GAN은 두가지 문제점을 가지고 있다.
    1.1.1 처음 시작할때 완전히 임의의 인풋이 들어가기때문에, 인풋 데이터 간에 완전히 discrete되기 힘들다.
    1.1.2 Score나 Loss를 계산할때 모든 sequence가 generate되고 난뒤에 계산이 된다. -> 시계열 데이터에 적용 시키기 힘들다.
  1.2 이 두가지 문제를 해결하기 위해 강화학습 방법을 접목시킨다.
    1.2.1 Generative Model is treated as an agent of reinforcement learning
    1.2.2 The general token is treated as state
    1.2.3 task-specific sequence score, such as BLEU is treated as Reward
    1.2.4 Employe a Discriminator to evaluate the sequence and feedback the evaluation to guide the learning of the generative model.
  1.2 Policy Gradient로 학습을 진행했고
  1.3 입력값을 지나번 출력값과 concat하고 oracle evaluation mechanism을 사용 했다.
    1.3.1 oracle evaluation mechanism이 뭔지?
  1.4 이를 사용하면 text generation, music generation과 같은 sequence 데이터 생성에 잘 사용 할 수 있다.
  1.5 학습은 RL의 방식으로 recurrent를 적용해서 입력데이터를 얻어내는데 이를 oracle방식으로 생성해서 학습하면 짱짱
  1.6 https://arxiv.org/abs/1609.05473

2. GRU를 넘어선 MGU
  2.1 RNN신경망이 고안되었고, long term dependecy를 반영하기 위한 순환신경망으로 LSTM이 제안되었다.
  2.2 하지만 LSTM은 3개의 gate와 2개의 giddene state를 가지고 있기 떄문에 구조가 복잡하다.
  2.3 그래서 이를 gate 1개 hidden sate 1개로 줄인 GRU가 제안되었다.
  2.4 RNN의 성능은 gate수에 비례해서 나타나지 않다고 알려졌고, 심지어 gate가 더욱 많은 LSTM보다 GRU가 보다 좋은 성능을 보여주고 있다.
    2.4.1 하지만 이때 gate숫자에 비례해서 parameter도 증가해야하지만 model간 전체 parameter수를 통제해서 성능을 비교했다.
  2.5 그래서 이보다 gate와 state수를 줄인 MGU가 제안되었다.
  2.6 gate와 hidden state 수가 줄었기 때문에, 자연스레 파라미터 숫자역시 줄고 이로인해서 model complexity가 drastically 줄어든다.
  2.7 GRU 이전에 LSTM with coupled gates가 있었다.
    2.7.1 LSTM의 input gate(memory cell로 들어가는 input을 제어)가 1-forget gate로 대체된 신경망
  2.8 RNN의 gate unit은 성능에 큰 영향을 끼친다.
    2.8.1 이때 forget gate가 가장 중요하고 그다음이 input gate 그리고 output gate이다
      2.8.1.1 GRU와 LSTM을 보면 유추할 수 있다.
  2.9 forget gate is unanimously considered the most important one
  2.10 그래서 MRU는 GRU의 reset gate를 forget gate로 대신한다.
  2.11 파라미터가 더적은 MGU가 GRu보다 좋은 성능을 낸다.
  2.12 peephole connection이 없이 좋은 성능을 내는 MGU
  2.13 https://arxiv.org/abs/1603.09420
  
3. Dropout을 쓴 RNN으로 handwriting recognition하기
  3.1 RNN에는 아직 dropout이 적용되지 않았는데 recurrent connection에 영향을 주지않게 dropout을 적용했다.
  3.2 handwriting에는 텍스트의 선과 이미지가 sequence 입력으로 들어와서 recognization 된다.
  3.3 이때 최근에는 Hidden Matkov Model이 많이 사용됬다.
    3.3.1 하지만 이 모델은 first order Markov chain만 쓰기 때문에 long term dependency를 잡을수 없다.
    3.3.2 그리고 HMMs 들은 하나의 hidden state가 하나의 데이터만 가질수 있기때문에 log(n)bit의 데이터만 carry할 수 있다.
  3.4 하지만 RNN은 그런거 없다.
    3.4.1 RNN이 가지는 Recurrent Connection이 그것을 잘 처리해 준다.
  3.5 recurrent에 dropout을 적용하지 않고 feedforward방향으로만 dropout을 적용시켜서 RNN에도 recurrent connection에 영향을 주지않고 dropout을 적용시킬 수 있다.
  3.6 dropout이 classification model의 가중치에 L1 L2 weight decay regularization의 효과를 나타내는것으로 보여진다.
  3.7 LSTM의 가중치는 dropout과 함께 크기가 커지는 것으로 보여진다.
  3.8 overfitting이 눈에띄게 줄어드는것을 확인 할 수 있다.
  3.9 https://arxiv.org/abs/1312.4569

4. Degradation Issue를 address하기 위한 residual nets
  4.1 신경망은 깊어질수록 표현하는 정보량이 커지면서 성능이 좋아진다.
  4.2 하지만 신경망이 깊어지면 깊어질수록 gradient가 vanishing되거나 exploding되는 문제가 나타나면서 학습이 진행되지 않는 문제가 발생하고
  4.3 error가 잘 전파 되지 않아서 신경망의 깊이가 깊어질수록 에러가 커지는 degradation문제가 발생한다.
  4.4 이를 address하기 위해서 제안된것이 residual framework이다.
  4.5 기본적으로 hidden layer의 출력이 residual이 되게하는 F(x)-x꼴로 되게 H(x)를 학습하게 한다.
  4.6 결과적으로 출력은 H(x)+x라는 highway structure가 나타나게 된다.
  4.7 이를 conv에 적용하기 위해서 zero padding을 할수도 있고 1x1 conv도 할 수 있는데, 이런 방법들과 상관없이 degradation 문제를 처리할 수 있다.
  4.8 결과적으로 ResNet을 사용하면 몹시 깊은 신경망(about 1000 layer)도 학습을 할 수 있게 되는것을 확인 할 수 있다.
  4.9 또 ResNet을 적용하면 학습속도역시 개선되는것을 확인 할 수 있다.
  4.10 학습속도도 빨라지고 Degradation문제를 해결 할 수 있는 ResNet
  4.11 https://arxiv.org/abs/1512.03385
  4.12 인용수가 만번을 


